[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "XXX\n\n\n\nselection bias\n\n\nmediation\n\n\nregression\n\n\nIPW\n\n\ndoubly-robust\n\n\nTMLE\n\n\n\nXXn\n\n\n\n\n\n\nAug 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecovering causal effects from post-treatment selection induced by missing outcome data\n\n\n\nselection bias\n\n\nmediation\n\n\nregression\n\n\nIPW\n\n\ndoubly-robust\n\n\nTMLE\n\n\n\nSolutions to the problem of effect recoverability from selection/missingness via regression adjustment and doubly-robust estimation\n\n\n\n\n\n\nAug 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecovering from selection bias with IPW methods\n\n\n\nIPW\n\n\nselection bias\n\n\n\nOn the consistency of IPW methods to recover causal effects from selection bias\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html",
    "href": "posts/2023-03-01_ipw/index.html",
    "title": "Recovering from selection bias with IPW methods",
    "section": "",
    "text": "Confounding bias and selection bias are together the most prevalent hurdles to the validity and generalizability of causal inference results. In general, both arise from uncontrolled extraneous flows of statistical information between treatment and outcome in the analysis. Their precise characterization in the Structural Causal Models framework (SCM), and potential corrections, differ in nature:"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#simple-simulation-setting",
    "href": "posts/2023-03-01_ipw/index.html#simple-simulation-setting",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Simple simulation setting",
    "text": "Simple simulation setting\nLet us consider the SCM given by the following DAG and set of structural equations:"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#the-dag",
    "href": "posts/2023-03-01_ipw/index.html#the-dag",
    "title": "Recovering from selection bias with IPW methods",
    "section": "The DAG:",
    "text": "The DAG:\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  M ~ A,\n  S ~ A + L,\n  Y ~ A + M + L\n) %&gt;% tidy_dagitty(layout = \"nicely\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#structural-equations",
    "href": "posts/2023-03-01_ipw/index.html#structural-equations",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Structural equations:",
    "text": "Structural equations:\n\\[\n\\begin{aligned}[c]\nL &\\sim\\text{Nor}(0,\\sigma^2_L) & &\\\\\nA &\\sim\\text{Ber}(q) & &\\\\\nM &= \\alpha_0 + \\alpha_1A + u_M & u_M &\\sim\\text{Nor}(0,\\sigma^2_M) \\\\\nS &= \\mathbb{I}[\\gamma_0 + \\gamma_1A + \\gamma_2M + \\gamma_3L + u_S &gt; 0] & u_S &\\sim\\text{Nor}(0,\\sigma^2_S) \\\\\nY &= \\beta_0 + \\beta_1A + \\beta_2M + \\beta_3L + u_Y & u_Y &\\sim\\text{Nor}(0,\\sigma^2_Y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#selection-mechanism",
    "href": "posts/2023-03-01_ipw/index.html#selection-mechanism",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Selection mechanism:",
    "text": "Selection mechanism:\nWhen \\(S=1\\) for a particular unit, we get to observe their whole data \\((L,A,M,Y)\\). When \\(S=1\\), only the final outcome \\(Y\\) is missing, so we get to observe \\((L,A,M)\\).\nLet us put this SCM as a data-generating process (DGP) in R code:\n\n\nCode\n# Libraries needed\nlibrary(DescTools)\nlibrary(LaplacesDemon)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(zeallot)\nlibrary(reshape2)\n\n# Data generating process\ndgp = function(param){\n  \n  #### Parameters\n  c(n,      # Number of samples\n    sdl,    # Stdr. dev. of selection predictor\n    p,      # Treatment assigment probability\n    a0, a1, # Parameters of A-&gt;M relation\n    sdm,    # Stdr. dev. of mediator noise\n    g0, g1, # Parameters of A-&gt;S relation\n    g2,     # Parameter of M-&gt;S relation\n    g3,     # Parameter of L-&gt;S relation\n    sds,    # Stdr. dev. of selection noise\n    b0, b1, # Parameters of A-&gt;Y relation\n    b2,     # Parameter of M-&gt;Y relation\n    b3,     # Parameter of L-&gt;Y relation\n    sdy # Stdr. dev. of selection noise\n    ) %&lt;-% param     \n  \n  # Exogenous selection predictor\n  L = rnorm(n=n, mean=0, sd=sdl)\n  \n  # Treatment assigment\n  A = rbinom(n=n, size=1, prob=p)\n  \n  # Mediator\n  noise.M = rnorm(n=n, mean=0, sd=sdm)\n  M = a0 + a1*A + noise.M\n  \n  # Selection mechanism\n  noise.S = rnorm(n=n, mean=0, sd=sds)\n  S = ifelse(g0 + g1*A + g2*M + g3*L + noise.S &gt; 0, 1, 0)\n  \n  # Outcome\n  noise.Y = rnorm(n=n, mean=0, sd=sdy)\n  Y = b0 + b1*A + b2*M + b3*L + noise.Y\n  \n  # true ATE\n  true.ATE = b1 + b2*a1\n  \n  # Data\n  dat = data.frame(L,A,M,S,Y)\n  \n  # Return\n  return(list(dat,true.ATE))\n}"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#results-under-no-exclusion-complete-data",
    "href": "posts/2023-03-01_ipw/index.html#results-under-no-exclusion-complete-data",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Results under no exclusion (complete data)",
    "text": "Results under no exclusion (complete data)\nFor a moment let us pretend we observe all variables for everyone. Let us examine the most interesting models that we can fit with the simulated data, to check their ability to recover population-level parameters under noisy samples.\n\n\nCode\n# Model for M, with complete data\nmod.M = lm(M ~ A, data=dat.1)\n\n# Model for S, with complete data\nmod.S = glm(S ~ A + L, family=binomial('probit'), data=dat.1)\n\n# Model for Y, with complete data\nmod.Y = lm(Y ~ A + M + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 1: Controlling for predictor of Y\nmod.ate.1 = lm(Y ~ A + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 2: Treatment is randomized, no controls needed\nmod.ate.2 = lm(Y ~ A, data=dat.1)\n\n# All models put together\nmodels.full = list(\"M\" = mod.M, \"S\" = mod.S, \"Y\" = mod.Y, \"ATE (O-set)\" = mod.ate.1, \"ATE (no adj)\" = mod.ate.2)\n\n# Summary of models\nmodelsummary(models.full, statistic = \"[{conf.low}, {conf.high}]\",\n             estimate  = \"{estimate}{stars}\") \n\n\n\n\n\n\nM\nS\nY\nATE (O-set)\nATE (no adj)\n\n\n\n\n(Intercept)\n0.104***\n−0.107+\n−1.579***\n−1.498***\n−1.524***\n\n\n\n[0.069, 0.139]\n[−0.219, 0.005]\n[−1.816, −1.342]\n[−1.732, −1.263]\n[−1.803, −1.245]\n\n\nA\n0.506***\n0.296***\n0.281\n0.677***\n0.742***\n\n\n\n[0.456, 0.556]\n[0.135, 0.457]\n[−0.114, 0.675]\n[0.340, 1.013]\n[0.342, 1.141]\n\n\nL\n\n0.427***\n1.755***\n1.768***\n\n\n\n\n\n[0.341, 0.514]\n[1.584, 1.925]\n[1.597, 1.940]\n\n\n\nM\n\n\n0.784***\n\n\n\n\n\n\n\n[0.369, 1.200]\n\n\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n1000\n\n\nR2\n0.283\n\n0.311\n0.301\n0.013\n\n\nR2 Adj.\n0.282\n\n0.308\n0.300\n0.012\n\n\nAIC\n1022.7\n1278.2\n4824.1\n4835.8\n5178.8\n\n\nBIC\n1037.5\n1292.9\n4848.7\n4855.5\n5193.5\n\n\nLog.Lik.\n−508.364\n−636.108\n−2407.072\n−2413.913\n−2586.403\n\n\nRMSE\n0.40\n0.47\n2.69\n2.70\n3.21\n\n\n\n\n\n\n\nSince the treatment is randomized, no control variables are required in the regression of \\(Y\\) against \\(A\\) to remove confounding bias. However, \\(L\\) is in the \\(O\\)-set of the effect; this is, controlling for \\(L\\) produces an asymptotically more efficient estimator. Such property is not seen in finite samples [see last two models].\nIt is no surprise that, with complete data, the point-estimate for the coefficient of \\(A\\) in the last two models lie close to the true ATE (0.75), even under a moderately noisy DGP (\\(R^2\\approx 0.3\\))."
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#results-under-exclusion",
    "href": "posts/2023-03-01_ipw/index.html#results-under-exclusion",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Results under exclusion",
    "text": "Results under exclusion\nIgnoring samples for which \\(S=0\\) reduces sample size from 1000 to 514. This is, the probability of exclusion is about 0.486.\n\n\nCode\n# Number of observation per selection\n# 1 = selected / observed\n# 0 = excluded\ntable(dat.1$S) %&gt;% kbl(col.names = c('S','N')) %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nS\nN\n\n\n\n\n0\n486\n\n\n1\n514\n\n\n\n\n\n\n\nIn contrast with the complete data case, a regression-based approach for estimating the ATE with only the selected samples does require controlling for \\(L\\), since conditioning on \\(S\\) opens the non-causal path:\n\n\\(A\\longrightarrow S\\longleftarrow L\\longrightarrow Y\\)\n\nUnfortunately, controlling for \\(L\\) alone does not remove selection bias, despite blocking such path. This is because the distribution of \\(L\\) in the sample does not necessarily match the distribution in the population.\nLet us inspect it.\n\nRegression-based approach:\nWe fit a model for the outcome \\(Y\\) against \\(A\\), controlling for \\(L\\), using only the selected samples:\n\n\nCode\n# Data for the selected sample\ndat.s = dat.1 %&gt;% filter(S==1)\nN.s = nrow(dat.s)\n\n# Model for causal effect, with selected\nmod.ate.s = lm(Y ~ A + L, data=dat.s)\nmodelsummary(list(\"ATE(S=1)\" = mod.ate.s), \n             statistic = NULL,\n             estimate  = \"{estimate}{stars} [{conf.low}, {conf.high}]\") \n\n\n\n\n\n\nATE(S=1)\n\n\n\n\n(Intercept)\n−1.341*** [−1.688, −0.995]\n\n\nA\n0.516* [0.058, 0.974]\n\n\nL\n1.843*** [1.601, 2.085]\n\n\nNum.Obs.\n514\n\n\nR2\n0.307\n\n\nR2 Adj.\n0.304\n\n\nAIC\n2458.1\n\n\nBIC\n2475.0\n\n\nLog.Lik.\n−1225.026\n\n\nRMSE\n2.62\n\n\n\n\n\n\n\nThe resulted 95% confidence interval for the coefficient of \\(A\\), using only samples for which \\(S=1\\), still covers the true ATE (0.75). Yet, a downwards bias shrinks the point-estimate and its lower bound noticeably. Using this, we would conclude the ATE is smaller than what truly is.\nLet us implement an IPW-based approach to compare against:\n\n\nIPW-based approach:\nGiven the SCM, and the assumption of \\(\\gamma_2=0\\), we can express:\n\n\\(w_i=\\mathbb{P}(S=1)/\\mathbb{P}(S=1\\mid A=a_i,L=l_i)\\)\n\\(v_i(a)=1/\\mathbb{P}(A=a) = 1/q\\) because treatment is randomized\n\nThe derived finite-sample estimator of the mean counterfactuals / potential outcomes is: \\[\n\\hat{Y}^a = N^{-1}\\sum_{i=1}^N\\frac{\\mathbb{I}(A_i=a)\\cdot\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\n\\]\nPutting all ingredients together we get point-estimates \\(\\hat{Y}^1=\\hat{\\mathbb{E}}[Y\\mid do(A=1)]\\) and \\(\\hat{Y}^0=\\hat{\\mathbb{E}}[Y\\mid do(A=0)]\\):\n\n\nCode\n# Compute the probability of selecton for all the units selected\n# We leverage the model mod.S, trained on complete data since S, A and L are\n# always observed (Y is the only one missing) and predict only on the selected units\nprob.s.unit = predict(mod.S, newdata=dat.s, type='response')\n\n# The unconditional probability of selection can be consistently estimated \n#... with counts from the total sample size and selected sample size\nprob.s = nrow(dat.s) / nrow(dat.1)\n\n# Since the treatment is randomized, the propensity score in the population is known at 0.5. \n# It can also be estimated from counts in the complete dataset\n# prop.score = sum(dat.1$A==1)/nrow(dat.1)\nprop.score = 0.5\n\n# Estimated counterfactuals/potential outcomes via IPPW\nest.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.Y = weights * Y) %&gt;% group_by(A) %&gt;% \n  summarise('PO'=sum(weighted.Y)/N.s)\n\n# Print resulted estimates\nhead(est.PO) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nA\nPO\n\n\n\n\n0\n-1.4318076\n\n\n1\n-0.7537995\n\n\n\n\n\n\n\nWhich, can be used to compute a point-estimate of the ATE: \\(\\hat{ATE}=\\hat{Y}^1-\\hat{Y}^0\\):\n\n\nCode\n# Print estimated ATE\nest.ATE = as.numeric(est.PO[2,2]-est.PO[1,2])\ndata.frame('IPW ATE'=est.ATE) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\n\n\n\n\n0.678008\n\n\n\n\n\n\n\nWe can see that the IPW-based approach produces a point-estimate closer to the true ATE. To get valid confidence intervals, however, a bootstrapping or asymptotic analysis need to be invoked. We can compute naïve confidence interval without resampling, using the fact:\n\\[\n\\hat{\\text{var}}(\\hat{\\text{ATE}}) \\geq \\hat{\\text{var}}(\\hat{Y}^1)+\\hat{\\text{var}}(\\hat{Y}^0)=(N-2)^{-2}\\sum_{a\\in\\{0,1\\}}\\sum_{i=1}^N \\mathbb{I}(A_i=a)\\cdot \\hat{v}_i(a)^2\\hat{w}_i^2(y_i-\\hat{Y}^a)^2\n\\]Such variance is naïve in the sense that it is overconfident due to ignoring the covariance between the potential outcomes, and the higher-order contributions of the weighting factors in the total variance. Anyway, using it will get us:\n\n\nCode\n# Estimated counterfactuals/potential outcomes via IPPW\nsd.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.var = weights^2 * ( A* (Y-est.PO[2,2])^2 + (1-A)* (Y-est.PO[1,2])^2)) %&gt;% \n  group_by(A) %&gt;% \n  summarise('sd'=sqrt(sum(weighted.var))/(N.s-2))\n\n# Naïve confidence interval\nnaivebounds = qnorm(0.975)*sum(sd.PO[,2])\ndata.frame('IPW ATE'=as.numeric(est.ATE),\n           'naïve LB'=as.numeric(est.ATE)-naivebounds,\n           'naïve UB'=as.numeric(est.ATE)+naivebounds) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\nnaïve.LB\nnaïve.UB\n\n\n\n\n0.678008\n0.5741569\n0.7818592\n\n\n\n\n\n\n\nAs noted, the naïve confidence interval is tighter than those produced by inference on the complete data. This means such confidence interval is not statistically valid, but it can still help us visualize the convergence of IPW estimator.\nNow, if we simulate and repeat the DGP and same analysis for different sample sizes, consistency would be visually perceived if we see:\n\nPoint-estimate converging to the true ATE\nConfidence intervals shrinking at a fast (**) rate\n\nLet us test it. We run 20 simulations with different complete sample sizes, from \\(N=500\\) to \\(N=23\\,000\\) [number of complete samples from \\(N_s=250\\) to \\(N_s=12\\,000\\)]. We repeat the procedure three times and average the results on those three repetitions. Such averages are presented in the following table:\n\n\nCode\n# Data frame to save results from iterations\nrounds.IPPW = data.frame(N=NA, Ns=NA, EST=NA, LB=NA, UB=NA)\n\n# All sample sizes to consider\nsamplesizes = round(500*exp(0.2*(0:19)))\n\n# Number of repetitions\nM = 3\n\n# Paramaters are the same as before\nparam.iter = param.1\n\n# Seed\nset.seed(66)\n\n# Loop\nfor(m in 1:M){\n  for(n in samplesizes){\n    \n    # Change sample size\n    param.iter[1] = n\n    \n    # Generate the data (complete and selected)\n    dat.iter = dgp(param.iter)[[1]]\n    dat.s.iter = dat.iter %&gt;% filter(S==1)\n    \n    # Estimated probability of selection\n    N.s.iter = nrow(dat.s.iter)\n    prob.s.iter = N.s.iter / n\n    \n    # Model for S, with complete data\n    mod.S.iter = glm(S ~ A + L, family=binomial('probit'), data=dat.iter)\n    prob.s.unit.iter = predict(mod.S.iter, newdata=dat.s.iter, type='response')\n  \n    # Propensity score model\n    prop.score.iter = sum(dat.iter$A==1)/nrow(dat.iter)\n    \n    # Put everything together\n    row.iter = dat.s.iter %&gt;% mutate(\n      prob.s.unit = as.numeric(prob.s.unit.iter),\n      prob.s = as.numeric(prob.s.iter),\n      prop.score = as.numeric(prop.score.iter),\n      pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n      weights = pro.weight * prob.s * (1/prob.s.unit),\n      weighted.Y = weights * Y) \n    \n    # Estimated counterfactuals/potential outcomes via IPPW\n    po.iter = row.iter %&gt;% group_by(A) %&gt;% \n      summarise('Y(A)'=sum(weighted.Y)/N.s.iter)\n    \n    # Estimated ATE\n    ATE.iter = as.numeric(po.iter[2,2]-po.iter[1,2])\n    \n    # Naïve variances\n    sd.PO.iter = row.iter %&gt;% mutate(\n      weighted.var = weights^2 * ( A* (Y-po.iter[2,2])^2 + (1-A)* (Y-po.iter[1,2])^2)) %&gt;% \n    group_by(A) %&gt;% \n    summarise('sd'=sqrt(sum(weighted.var))/(N.s.iter-1))\n    \n    # Naïve confidence interval\n    naivebounds = qnorm(0.975)*sum(sd.PO.iter[,2])\n    \n    rounds.IPPW = rbind.data.frame(rounds.IPPW,\n                                   data.frame(N=n,\n                                              Ns=N.s.iter,\n                                              EST=ATE.iter,\n                                              LB=ATE.iter-naivebounds,\n                                              UB=ATE.iter+naivebounds))\n  }\n}\n# Print resulted estimates\nrounds.IPPW = rounds.IPPW[-1,] %&gt;%\n  group_by(N) %&gt;%\n  summarise_all(mean)\n\nhead(rounds.IPPW) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nN\nNs\nEST\nLB\nUB\n\n\n\n\n500\n249.6667\n0.5919352\n-0.1461034\n1.3299737\n\n\n611\n309.0000\n0.7701710\n0.1782066\n1.3621354\n\n\n746\n379.6667\n0.8962619\n0.2991885\n1.4933353\n\n\n911\n453.6667\n0.7438565\n0.5811669\n0.9065460\n\n\n1113\n556.0000\n0.7817429\n0.3579693\n1.2055165\n\n\n1359\n679.0000\n0.6411555\n0.3223225\n0.9599884\n\n\n\n\n\n\n\nAn the following plot:\n\n\nCode\n### Plot results from rounds\nrounds.IPPW[,-1] %&gt;% melt(id.vars = 'Ns') %&gt;% \n  mutate(type = ifelse(variable=='EST','EST','N. C.I.') ) %&gt;% \n  ggplot(aes(x=Ns,y=value,group=variable)) + \n  geom_point(aes(shape=type,color=type)) + \n  geom_smooth(method = lm, formula = y ~ x + I(sqrt(x)), se = FALSE) + ## O(N^0.5) convergence\n  labs(y='Value of estimate / bound', x='number of complete samples') +\n  geom_hline(yintercept=T.ATE, col = 'red') +\n  theme_bw()\n\n\n\n\n\nWe can appreciate that the estimator converges to the true ATE [in red], and its uncertainty reduce at a sustained rate; there is convergence in probability. In other words, the IPW-based estimator is consistent.\n\nMathematical justification of consistency\nWe can prove consistency mathematically for this SCM. However, to avoid measure-theoretic conundrums, let us consider the case for which all variables are discrete. Results are generalizable for mixed discrete-continuous cases with positive distributions (no zero-measure events).\nLet us assume \\(Y\\) has support on \\(\\{y_{(c)}\\}_{c=1}^C\\), and \\(L\\) has support on \\(\\{l_{(k)}\\}_{k=1}^K\\), then:\n\\[\n\\begin{aligned}\n\\hat{Y}^a &= N^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\\cdot \\mathbb{I}(A_i=a) \\\\\n&=\nN^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a\\mid L=l_i)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a,L=l_i)}\\cdot y_i\\cdot\\mathbb{I}(A_i=a,S_i=1)\\\\\n&=\n\\sum_{i=1}^N\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_i)\n}\\cdot\n\\frac{\ny_i\\cdot\\mathbb{I}(A_i=a,S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_i,S=1)\n}\\\\\n&= \\sum_{i=1}^N\\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{i}\\cdot\\mathbb{I}(A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\sum_{i=1}^N\\mathbb{I}(Y_i=y_{(c)},A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\hat{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\n\\end{aligned}\n\\]\nAssuming the propensity scores and the probability of selection are both correctly specified, all finite-sample approximations \\(\\hat{\\mathbb{P}}\\) converge in the limit to the true distributions \\(\\mathbb{P}\\). Then:\n\\[\n\\begin{aligned}\n\\text{plim}_{N\\rightarrow\\infty}\n\\hat{Y}^a\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(S=1)}{\n{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(L=l_{(k)})}{\n{\\mathbb{P}}(L=l_{(k)}\\mid S=1)\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n{\\mathbb{P}}(L=l_{(k)})\n\\cdot\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)} S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\sum_{c}\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)}, S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\mathbb{E}(Y\\mid A=a,L, S=1)\\\\\n&=\n\\mathbb{E}_L\n\\mathbb{E}(Y\\mid A=a,L, S=1) =\\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L,S=1]\\\\\n&= \\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L] = \\mathbb{E}[Y\\mid do(A=a)]\n\\end{aligned}\n\\]\nThis is, the IPW estimator converges to the true counterfactual mean given by intervention \\(do(A=a)\\). The last two equalities come from these facts:\n\n\\(L\\) blocks all non-causal paths from \\(A\\) to \\(Y\\) when conditioning on \\(S=1\\)\n\\(Y\\perp S\\,\\mid L\\) in the back-door graph: the resulting DAG after removing all arrows coming out of \\(A\\)\n\\(\\mathbb{P}(L=l_{(k)})\\) is the population-distribution of \\(L\\)\n\n\n\n\nRevisiting the regression approach: generalized adjustment criteria\nNotice that, in the convergence proof for the IPW estimator, it was shown that the underlying estimand is algebraically equivalent to a regression-adjusted mean of \\(Y\\), averaged over the population distribution of \\(L\\).\n\\[\n\\mathbb{E}[Y\\mid do(A=a)] = \\mathbb{E}_L\\mathbb{E}(Y\\mid A=a,L,S=1)\n\\]\nExtensions of this results are covered by three generalized adjustment criteria (Correa, Tian, and Bareinboim 2018). This is, for this case, a mathematically equivalent result in term of consistency can be achieved via regression adjustment."
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html",
    "href": "posts/2023-06-01_missingOutcome/index.html",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "",
    "text": "Consider the following \\(m\\)-graph representing the causal relations among a set of random variables:\nConfounding bias and selection bias are together the most prevalent hurdles to the validity and generalizability of causal inference results. In general, both arise from uncontrolled extraneous flows of statistical information between treatment and outcome in the analysis. Their precise characterization in the Structural Causal Models framework (SCM), and potential corrections, differ in nature:"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#simple-simulation-setting",
    "href": "posts/2023-06-01_missingOutcome/index.html#simple-simulation-setting",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Simple simulation setting",
    "text": "Simple simulation setting\nLet us consider the SCM given by the following DAG and set of structural equations:"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#the-dag",
    "href": "posts/2023-06-01_missingOutcome/index.html#the-dag",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The DAG:",
    "text": "The DAG:\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  M ~ A,\n  S ~ A + L,\n  Y ~ A + M + L\n) %&gt;% tidy_dagitty(layout = \"nicely\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#structural-equations",
    "href": "posts/2023-06-01_missingOutcome/index.html#structural-equations",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Structural equations:",
    "text": "Structural equations:\n\\[\n\\begin{aligned}[c]\nL &\\sim\\text{Nor}(0,\\sigma^2_L) & &\\\\\nA &\\sim\\text{Ber}(q) & &\\\\\nM &= \\alpha_0 + \\alpha_1A + u_M & u_M &\\sim\\text{Nor}(0,\\sigma^2_M) \\\\\nS &= \\mathbb{I}[\\gamma_0 + \\gamma_1A + \\gamma_2M + \\gamma_3L + u_S &gt; 0] & u_S &\\sim\\text{Nor}(0,\\sigma^2_S) \\\\\nY &= \\beta_0 + \\beta_1A + \\beta_2M + \\beta_3L + u_Y & u_Y &\\sim\\text{Nor}(0,\\sigma^2_Y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#selection-mechanism",
    "href": "posts/2023-06-01_missingOutcome/index.html#selection-mechanism",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Selection mechanism:",
    "text": "Selection mechanism:\nWhen \\(S=1\\) for a particular unit, we get to observe their whole data \\((L,A,M,Y)\\). When \\(S=1\\), only the final outcome \\(Y\\) is missing, so we get to observe \\((L,A,M)\\).\nLet us put this SCM as a data-generating process (DGP) in R code:\n\n\nCode\n# Libraries needed\nlibrary(DescTools)\nlibrary(LaplacesDemon)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(zeallot)\nlibrary(reshape2)\n\n# Data generating process\ndgp = function(param){\n  \n  #### Parameters\n  c(n,      # Number of samples\n    sdl,    # Stdr. dev. of selection predictor\n    p,      # Treatment assigment probability\n    a0, a1, # Parameters of A-&gt;M relation\n    sdm,    # Stdr. dev. of mediator noise\n    g0, g1, # Parameters of A-&gt;S relation\n    g2,     # Parameter of M-&gt;S relation\n    g3,     # Parameter of L-&gt;S relation\n    sds,    # Stdr. dev. of selection noise\n    b0, b1, # Parameters of A-&gt;Y relation\n    b2,     # Parameter of M-&gt;Y relation\n    b3,     # Parameter of L-&gt;Y relation\n    sdy # Stdr. dev. of selection noise\n    ) %&lt;-% param     \n  \n  # Exogenous selection predictor\n  L = rnorm(n=n, mean=0, sd=sdl)\n  \n  # Treatment assigment\n  A = rbinom(n=n, size=1, prob=p)\n  \n  # Mediator\n  noise.M = rnorm(n=n, mean=0, sd=sdm)\n  M = a0 + a1*A + noise.M\n  \n  # Selection mechanism\n  noise.S = rnorm(n=n, mean=0, sd=sds)\n  S = ifelse(g0 + g1*A + g2*M + g3*L + noise.S &gt; 0, 1, 0)\n  \n  # Outcome\n  noise.Y = rnorm(n=n, mean=0, sd=sdy)\n  Y = b0 + b1*A + b2*M + b3*L + noise.Y\n  \n  # true ATE\n  true.ATE = b1 + b2*a1\n  \n  # Data\n  dat = data.frame(L,A,M,S,Y)\n  \n  # Return\n  return(list(dat,true.ATE))\n}"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "href": "posts/2023-06-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under no exclusion (complete data)",
    "text": "Results under no exclusion (complete data)\nFor a moment let us pretend we observe all variables for everyone. Let us examine the most interesting models that we can fit with the simulated data, to check their ability to recover population-level parameters under noisy samples.\n\n\nCode\n# Model for M, with complete data\nmod.M = lm(M ~ A, data=dat.1)\n\n# Model for S, with complete data\nmod.S = glm(S ~ A + L, family=binomial('probit'), data=dat.1)\n\n# Model for Y, with complete data\nmod.Y = lm(Y ~ A + M + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 1: Controlling for predictor of Y\nmod.ate.1 = lm(Y ~ A + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 2: Treatment is randomized, no controls needed\nmod.ate.2 = lm(Y ~ A, data=dat.1)\n\n# All models put together\nmodels.full = list(\"M\" = mod.M, \"S\" = mod.S, \"Y\" = mod.Y, \"ATE (O-set)\" = mod.ate.1, \"ATE (no adj)\" = mod.ate.2)\n\n# Summary of models\nmodelsummary(models.full, statistic = \"[{conf.low}, {conf.high}]\",\n             estimate  = \"{estimate}{stars}\") \n\n\n\n\n\n\nM\nS\nY\nATE (O-set)\nATE (no adj)\n\n\n\n\n(Intercept)\n0.104***\n−0.107+\n−1.579***\n−1.498***\n−1.524***\n\n\n\n[0.069, 0.139]\n[−0.219, 0.005]\n[−1.816, −1.342]\n[−1.732, −1.263]\n[−1.803, −1.245]\n\n\nA\n0.506***\n0.296***\n0.281\n0.677***\n0.742***\n\n\n\n[0.456, 0.556]\n[0.135, 0.457]\n[−0.114, 0.675]\n[0.340, 1.013]\n[0.342, 1.141]\n\n\nL\n\n0.427***\n1.755***\n1.768***\n\n\n\n\n\n[0.341, 0.514]\n[1.584, 1.925]\n[1.597, 1.940]\n\n\n\nM\n\n\n0.784***\n\n\n\n\n\n\n\n[0.369, 1.200]\n\n\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n1000\n\n\nR2\n0.283\n\n0.311\n0.301\n0.013\n\n\nR2 Adj.\n0.282\n\n0.308\n0.300\n0.012\n\n\nAIC\n1022.7\n1278.2\n4824.1\n4835.8\n5178.8\n\n\nBIC\n1037.5\n1292.9\n4848.7\n4855.5\n5193.5\n\n\nLog.Lik.\n−508.364\n−636.108\n−2407.072\n−2413.913\n−2586.403\n\n\nRMSE\n0.40\n0.47\n2.69\n2.70\n3.21\n\n\n\n\n\n\n\nSince the treatment is randomized, no control variables are required in the regression of \\(Y\\) against \\(A\\) to remove confounding bias. However, \\(L\\) is in the \\(O\\)-set of the effect; this is, controlling for \\(L\\) produces an asymptotically more efficient estimator. Such property is not seen in finite samples [see last two models].\nIt is no surprise that, with complete data, the point-estimate for the coefficient of \\(A\\) in the last two models lie close to the true ATE (0.75), even under a moderately noisy DGP (\\(R^2\\approx 0.3\\))."
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#results-under-exclusion",
    "href": "posts/2023-06-01_missingOutcome/index.html#results-under-exclusion",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under exclusion",
    "text": "Results under exclusion\nIgnoring samples for which \\(S=0\\) reduces sample size from 1000 to 514. This is, the probability of exclusion is about 0.486.\n\n\nCode\n# Number of observation per selection\n# 1 = selected / observed\n# 0 = excluded\ntable(dat.1$S) %&gt;% kbl(col.names = c('S','N')) %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nS\nN\n\n\n\n\n0\n486\n\n\n1\n514\n\n\n\n\n\n\n\nIn contrast with the complete data case, a regression-based approach for estimating the ATE with only the selected samples does require controlling for \\(L\\), since conditioning on \\(S\\) opens the non-causal path:\n\n\\(A\\longrightarrow S\\longleftarrow L\\longrightarrow Y\\)\n\nUnfortunately, controlling for \\(L\\) alone does not remove selection bias, despite blocking such path. This is because the distribution of \\(L\\) in the sample does not necessarily match the distribution in the population.\nLet us inspect it.\n\nRegression-based approach:\nWe fit a model for the outcome \\(Y\\) against \\(A\\), controlling for \\(L\\), using only the selected samples:\n\n\nCode\n# Data for the selected sample\ndat.s = dat.1 %&gt;% filter(S==1)\nN.s = nrow(dat.s)\n\n# Model for causal effect, with selected\nmod.ate.s = lm(Y ~ A + L, data=dat.s)\nmodelsummary(list(\"ATE(S=1)\" = mod.ate.s), \n             statistic = NULL,\n             estimate  = \"{estimate}{stars} [{conf.low}, {conf.high}]\") \n\n\n\n\n\n\nATE(S=1)\n\n\n\n\n(Intercept)\n−1.341*** [−1.688, −0.995]\n\n\nA\n0.516* [0.058, 0.974]\n\n\nL\n1.843*** [1.601, 2.085]\n\n\nNum.Obs.\n514\n\n\nR2\n0.307\n\n\nR2 Adj.\n0.304\n\n\nAIC\n2458.1\n\n\nBIC\n2475.0\n\n\nLog.Lik.\n−1225.026\n\n\nRMSE\n2.62\n\n\n\n\n\n\n\nThe resulted 95% confidence interval for the coefficient of \\(A\\), using only samples for which \\(S=1\\), still covers the true ATE (0.75). Yet, a downwards bias shrinks the point-estimate and its lower bound noticeably. Using this, we would conclude the ATE is smaller than what truly is.\nLet us implement an IPW-based approach to compare against:\n\n\nIPW-based approach:\nGiven the SCM, and the assumption of \\(\\gamma_2=0\\), we can express:\n\n\\(w_i=\\mathbb{P}(S=1)/\\mathbb{P}(S=1\\mid A=a_i,L=l_i)\\)\n\\(v_i(a)=1/\\mathbb{P}(A=a) = 1/q\\) because treatment is randomized\n\nThe derived finite-sample estimator of the mean counterfactuals / potential outcomes is: \\[\n\\hat{Y}^a = N^{-1}\\sum_{i=1}^N\\frac{\\mathbb{I}(A_i=a)\\cdot\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\n\\]\nPutting all ingredients together we get point-estimates \\(\\hat{Y}^1=\\hat{\\mathbb{E}}[Y\\mid do(A=1)]\\) and \\(\\hat{Y}^0=\\hat{\\mathbb{E}}[Y\\mid do(A=0)]\\):\n\n\nCode\n# Compute the probability of selecton for all the units selected\n# We leverage the model mod.S, trained on complete data since S, A and L are\n# always observed (Y is the only one missing) and predict only on the selected units\nprob.s.unit = predict(mod.S, newdata=dat.s, type='response')\n\n# The unconditional probability of selection can be consistently estimated \n#... with counts from the total sample size and selected sample size\nprob.s = nrow(dat.s) / nrow(dat.1)\n\n# Since the treatment is randomized, the propensity score in the population is known at 0.5. \n# It can also be estimated from counts in the complete dataset\n# prop.score = sum(dat.1$A==1)/nrow(dat.1)\nprop.score = 0.5\n\n# Estimated counterfactuals/potential outcomes via IPPW\nest.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.Y = weights * Y) %&gt;% group_by(A) %&gt;% \n  summarise('PO'=sum(weighted.Y)/N.s)\n\n# Print resulted estimates\nhead(est.PO) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nA\nPO\n\n\n\n\n0\n-1.4318076\n\n\n1\n-0.7537995\n\n\n\n\n\n\n\nWhich, can be used to compute a point-estimate of the ATE: \\(\\hat{ATE}=\\hat{Y}^1-\\hat{Y}^0\\):\n\n\nCode\n# Print estimated ATE\nest.ATE = as.numeric(est.PO[2,2]-est.PO[1,2])\ndata.frame('IPW ATE'=est.ATE) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\n\n\n\n\n0.678008\n\n\n\n\n\n\n\nWe can see that the IPW-based approach produces a point-estimate closer to the true ATE. To get valid confidence intervals, however, a bootstrapping or asymptotic analysis need to be invoked. We can compute naïve confidence interval without resampling, using the fact:\n\\[\n\\hat{\\text{var}}(\\hat{\\text{ATE}}) \\geq \\hat{\\text{var}}(\\hat{Y}^1)+\\hat{\\text{var}}(\\hat{Y}^0)=(N-2)^{-2}\\sum_{a\\in\\{0,1\\}}\\sum_{i=1}^N \\mathbb{I}(A_i=a)\\cdot \\hat{v}_i(a)^2\\hat{w}_i^2(y_i-\\hat{Y}^a)^2\n\\]Such variance is naïve in the sense that it is overconfident due to ignoring the covariance between the potential outcomes, and the higher-order contributions of the weighting factors in the total variance. Anyway, using it will get us:\n\n\nCode\n# Estimated counterfactuals/potential outcomes via IPPW\nsd.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.var = weights^2 * ( A* (Y-est.PO[2,2])^2 + (1-A)* (Y-est.PO[1,2])^2)) %&gt;% \n  group_by(A) %&gt;% \n  summarise('sd'=sqrt(sum(weighted.var))/(N.s-2))\n\n# Naïve confidence interval\nnaivebounds = qnorm(0.975)*sum(sd.PO[,2])\ndata.frame('IPW ATE'=as.numeric(est.ATE),\n           'naïve LB'=as.numeric(est.ATE)-naivebounds,\n           'naïve UB'=as.numeric(est.ATE)+naivebounds) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\nnaïve.LB\nnaïve.UB\n\n\n\n\n0.678008\n0.5741569\n0.7818592\n\n\n\n\n\n\n\nAs noted, the naïve confidence interval is tighter than those produced by inference on the complete data. This means such confidence interval is not statistically valid, but it can still help us visualize the convergence of IPW estimator.\nNow, if we simulate and repeat the DGP and same analysis for different sample sizes, consistency would be visually perceived if we see:\n\nPoint-estimate converging to the true ATE\nConfidence intervals shrinking at a fast (**) rate\n\nLet us test it. We run 20 simulations with different complete sample sizes, from \\(N=500\\) to \\(N=23\\,000\\) [number of complete samples from \\(N_s=250\\) to \\(N_s=12\\,000\\)]. We repeat the procedure three times and average the results on those three repetitions. Such averages are presented in the following table:\n\n\nCode\n# Data frame to save results from iterations\nrounds.IPPW = data.frame(N=NA, Ns=NA, EST=NA, LB=NA, UB=NA)\n\n# All sample sizes to consider\nsamplesizes = round(500*exp(0.2*(0:19)))\n\n# Number of repetitions\nM = 3\n\n# Paramaters are the same as before\nparam.iter = param.1\n\n# Seed\nset.seed(66)\n\n# Loop\nfor(m in 1:M){\n  for(n in samplesizes){\n    \n    # Change sample size\n    param.iter[1] = n\n    \n    # Generate the data (complete and selected)\n    dat.iter = dgp(param.iter)[[1]]\n    dat.s.iter = dat.iter %&gt;% filter(S==1)\n    \n    # Estimated probability of selection\n    N.s.iter = nrow(dat.s.iter)\n    prob.s.iter = N.s.iter / n\n    \n    # Model for S, with complete data\n    mod.S.iter = glm(S ~ A + L, family=binomial('probit'), data=dat.iter)\n    prob.s.unit.iter = predict(mod.S.iter, newdata=dat.s.iter, type='response')\n  \n    # Propensity score model\n    prop.score.iter = sum(dat.iter$A==1)/nrow(dat.iter)\n    \n    # Put everything together\n    row.iter = dat.s.iter %&gt;% mutate(\n      prob.s.unit = as.numeric(prob.s.unit.iter),\n      prob.s = as.numeric(prob.s.iter),\n      prop.score = as.numeric(prop.score.iter),\n      pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n      weights = pro.weight * prob.s * (1/prob.s.unit),\n      weighted.Y = weights * Y) \n    \n    # Estimated counterfactuals/potential outcomes via IPPW\n    po.iter = row.iter %&gt;% group_by(A) %&gt;% \n      summarise('Y(A)'=sum(weighted.Y)/N.s.iter)\n    \n    # Estimated ATE\n    ATE.iter = as.numeric(po.iter[2,2]-po.iter[1,2])\n    \n    # Naïve variances\n    sd.PO.iter = row.iter %&gt;% mutate(\n      weighted.var = weights^2 * ( A* (Y-po.iter[2,2])^2 + (1-A)* (Y-po.iter[1,2])^2)) %&gt;% \n    group_by(A) %&gt;% \n    summarise('sd'=sqrt(sum(weighted.var))/(N.s.iter-1))\n    \n    # Naïve confidence interval\n    naivebounds = qnorm(0.975)*sum(sd.PO.iter[,2])\n    \n    rounds.IPPW = rbind.data.frame(rounds.IPPW,\n                                   data.frame(N=n,\n                                              Ns=N.s.iter,\n                                              EST=ATE.iter,\n                                              LB=ATE.iter-naivebounds,\n                                              UB=ATE.iter+naivebounds))\n  }\n}\n# Print resulted estimates\nrounds.IPPW = rounds.IPPW[-1,] %&gt;%\n  group_by(N) %&gt;%\n  summarise_all(mean)\n\nhead(rounds.IPPW) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nN\nNs\nEST\nLB\nUB\n\n\n\n\n500\n249.6667\n0.5919352\n-0.1461034\n1.3299737\n\n\n611\n309.0000\n0.7701710\n0.1782066\n1.3621354\n\n\n746\n379.6667\n0.8962619\n0.2991885\n1.4933353\n\n\n911\n453.6667\n0.7438565\n0.5811669\n0.9065460\n\n\n1113\n556.0000\n0.7817429\n0.3579693\n1.2055165\n\n\n1359\n679.0000\n0.6411555\n0.3223225\n0.9599884\n\n\n\n\n\n\n\nAn the following plot:\n\n\nCode\n### Plot results from rounds\nrounds.IPPW[,-1] %&gt;% melt(id.vars = 'Ns') %&gt;% \n  mutate(type = ifelse(variable=='EST','EST','N. C.I.') ) %&gt;% \n  ggplot(aes(x=Ns,y=value,group=variable)) + \n  geom_point(aes(shape=type,color=type)) + \n  geom_smooth(method = lm, formula = y ~ x + I(sqrt(x)), se = FALSE) + ## O(N^0.5) convergence\n  labs(y='Value of estimate / bound', x='number of complete samples') +\n  geom_hline(yintercept=T.ATE, col = 'red') +\n  theme_bw()\n\n\n\n\n\nWe can appreciate that the estimator converges to the true ATE [in red], and its uncertainty reduce at a sustained rate; there is convergence in probability. In other words, the IPW-based estimator is consistent.\n\nMathematical justification of consistency\nWe can prove consistency mathematically for this SCM. However, to avoid measure-theoretic conundrums, let us consider the case for which all variables are discrete. Results are generalizable for mixed discrete-continuous cases with positive distributions (no zero-measure events).\nLet us assume \\(Y\\) has support on \\(\\{y_{(c)}\\}_{c=1}^C\\), and \\(L\\) has support on \\(\\{l_{(k)}\\}_{k=1}^K\\), then:\n\\[\n\\begin{aligned}\n\\hat{Y}^a &= N^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\\cdot \\mathbb{I}(A_i=a) \\\\\n&=\nN^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a\\mid L=l_i)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a,L=l_i)}\\cdot y_i\\cdot\\mathbb{I}(A_i=a,S_i=1)\\\\\n&=\n\\sum_{i=1}^N\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_i)\n}\\cdot\n\\frac{\ny_i\\cdot\\mathbb{I}(A_i=a,S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_i,S=1)\n}\\\\\n&= \\sum_{i=1}^N\\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{i}\\cdot\\mathbb{I}(A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\sum_{i=1}^N\\mathbb{I}(Y_i=y_{(c)},A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\hat{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\n\\end{aligned}\n\\]\nAssuming the propensity scores and the probability of selection are both correctly specified, all finite-sample approximations \\(\\hat{\\mathbb{P}}\\) converge in the limit to the true distributions \\(\\mathbb{P}\\). Then:\n\\[\n\\begin{aligned}\n\\text{plim}_{N\\rightarrow\\infty}\n\\hat{Y}^a\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(S=1)}{\n{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(L=l_{(k)})}{\n{\\mathbb{P}}(L=l_{(k)}\\mid S=1)\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n{\\mathbb{P}}(L=l_{(k)})\n\\cdot\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)} S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\sum_{c}\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)}, S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\mathbb{E}(Y\\mid A=a,L, S=1)\\\\\n&=\n\\mathbb{E}_L\n\\mathbb{E}(Y\\mid A=a,L, S=1) =\\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L,S=1]\\\\\n&= \\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L] = \\mathbb{E}[Y\\mid do(A=a)]\n\\end{aligned}\n\\]\nThis is, the IPW estimator converges to the true counterfactual mean given by intervention \\(do(A=a)\\). The last two equalities come from these facts:\n\n\\(L\\) blocks all non-causal paths from \\(A\\) to \\(Y\\) when conditioning on \\(S=1\\)\n\\(Y\\perp S\\,\\mid L\\) in the back-door graph: the resulting DAG after removing all arrows coming out of \\(A\\)\n\\(\\mathbb{P}(L=l_{(k)})\\) is the population-distribution of \\(L\\)\n\n\n\n\nRevisiting the regression approach: generalized adjustment criteria\nNotice that, in the convergence proof for the IPW estimator, it was shown that the underlying estimand is algebraically equivalent to a regression-adjusted mean of \\(Y\\), averaged over the population distribution of \\(L\\).\n\\[\n\\mathbb{E}[Y\\mid do(A=a)] = \\mathbb{E}_L\\mathbb{E}(Y\\mid A=a,L,S=1)\n\\]\nExtensions of this results are covered by three generalized adjustment criteria (Correa, Tian, and Bareinboim 2018). This is, for this case, a mathematically equivalent result in term of consistency can be achieved via regression adjustment."
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html",
    "href": "posts/2023-08-01_missingOutcome/index.html",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "",
    "text": "Consider the following \\(m\\)-graph, \\(\\mathcal{G}\\), representing the causal relations among a set of random variables \\(\\mathcal{V}=\\{H,A,M,Y,R_Y\\}\\), where:\nOur goal is to estimate the average treatment effect (ATE), \\(\\psi\\), in the target population, defined as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}[Y\\mid do(A=a)] := \\mathbb{E}[Y\\mid do(A=1)]-\\mathbb{E}[Y\\mid do(A=0)]\n\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#simple-simulation-setting",
    "href": "posts/2023-08-01_missingOutcome/index.html#simple-simulation-setting",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Simple simulation setting",
    "text": "Simple simulation setting\nLet us consider the SCM given by the following DAG and set of structural equations:"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#the-dag",
    "href": "posts/2023-08-01_missingOutcome/index.html#the-dag",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The DAG:",
    "text": "The DAG:\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  M ~ A,\n  S ~ A + L,\n  Y ~ A + M + L\n) %&gt;% tidy_dagitty(layout = \"nicely\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#structural-equations",
    "href": "posts/2023-08-01_missingOutcome/index.html#structural-equations",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Structural equations:",
    "text": "Structural equations:\n\\[\n\\begin{aligned}[c]\nL &\\sim\\text{Nor}(0,\\sigma^2_L) & &\\\\\nA &\\sim\\text{Ber}(q) & &\\\\\nM &= \\alpha_0 + \\alpha_1A + u_M & u_M &\\sim\\text{Nor}(0,\\sigma^2_M) \\\\\nS &= \\mathbb{I}[\\gamma_0 + \\gamma_1A + \\gamma_2M + \\gamma_3L + u_S &gt; 0] & u_S &\\sim\\text{Nor}(0,\\sigma^2_S) \\\\\nY &= \\beta_0 + \\beta_1A + \\beta_2M + \\beta_3L + u_Y & u_Y &\\sim\\text{Nor}(0,\\sigma^2_Y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#selection-mechanism",
    "href": "posts/2023-08-01_missingOutcome/index.html#selection-mechanism",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Selection mechanism:",
    "text": "Selection mechanism:\nWhen \\(S=1\\) for a particular unit, we get to observe their whole data \\((L,A,M,Y)\\). When \\(S=1\\), only the final outcome \\(Y\\) is missing, so we get to observe \\((L,A,M)\\).\nLet us put this SCM as a data-generating process (DGP) in R code:\n\n\nCode\n# Libraries needed\nlibrary(DescTools)\nlibrary(LaplacesDemon)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(zeallot)\nlibrary(reshape2)\n\n# Data generating process\ndgp = function(param){\n  \n  #### Parameters\n  c(n,      # Number of samples\n    sdl,    # Stdr. dev. of selection predictor\n    p,      # Treatment assigment probability\n    a0, a1, # Parameters of A-&gt;M relation\n    sdm,    # Stdr. dev. of mediator noise\n    g0, g1, # Parameters of A-&gt;S relation\n    g2,     # Parameter of M-&gt;S relation\n    g3,     # Parameter of L-&gt;S relation\n    sds,    # Stdr. dev. of selection noise\n    b0, b1, # Parameters of A-&gt;Y relation\n    b2,     # Parameter of M-&gt;Y relation\n    b3,     # Parameter of L-&gt;Y relation\n    sdy # Stdr. dev. of selection noise\n    ) %&lt;-% param     \n  \n  # Exogenous selection predictor\n  L = rnorm(n=n, mean=0, sd=sdl)\n  \n  # Treatment assigment\n  A = rbinom(n=n, size=1, prob=p)\n  \n  # Mediator\n  noise.M = rnorm(n=n, mean=0, sd=sdm)\n  M = a0 + a1*A + noise.M\n  \n  # Selection mechanism\n  noise.S = rnorm(n=n, mean=0, sd=sds)\n  S = ifelse(g0 + g1*A + g2*M + g3*L + noise.S &gt; 0, 1, 0)\n  \n  # Outcome\n  noise.Y = rnorm(n=n, mean=0, sd=sdy)\n  Y = b0 + b1*A + b2*M + b3*L + noise.Y\n  \n  # true ATE\n  true.ATE = b1 + b2*a1\n  \n  # Data\n  dat = data.frame(L,A,M,S,Y)\n  \n  # Return\n  return(list(dat,true.ATE))\n}"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "href": "posts/2023-08-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under no exclusion (complete data)",
    "text": "Results under no exclusion (complete data)\nFor a moment let us pretend we observe all variables for everyone. Let us examine the most interesting models that we can fit with the simulated data, to check their ability to recover population-level parameters under noisy samples.\n\n\nCode\n# Model for M, with complete data\nmod.M = lm(M ~ A, data=dat.1)\n\n# Model for S, with complete data\nmod.S = glm(S ~ A + L, family=binomial('probit'), data=dat.1)\n\n# Model for Y, with complete data\nmod.Y = lm(Y ~ A + M + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 1: Controlling for predictor of Y\nmod.ate.1 = lm(Y ~ A + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 2: Treatment is randomized, no controls needed\nmod.ate.2 = lm(Y ~ A, data=dat.1)\n\n# All models put together\nmodels.full = list(\"M\" = mod.M, \"S\" = mod.S, \"Y\" = mod.Y, \"ATE (O-set)\" = mod.ate.1, \"ATE (no adj)\" = mod.ate.2)\n\n# Summary of models\nmodelsummary(models.full, statistic = \"[{conf.low}, {conf.high}]\",\n             estimate  = \"{estimate}{stars}\") \n\n\n\n\n\n\nM\nS\nY\nATE (O-set)\nATE (no adj)\n\n\n\n\n(Intercept)\n0.104***\n−0.107+\n−1.579***\n−1.498***\n−1.524***\n\n\n\n[0.069, 0.139]\n[−0.219, 0.005]\n[−1.816, −1.342]\n[−1.732, −1.263]\n[−1.803, −1.245]\n\n\nA\n0.506***\n0.296***\n0.281\n0.677***\n0.742***\n\n\n\n[0.456, 0.556]\n[0.135, 0.457]\n[−0.114, 0.675]\n[0.340, 1.013]\n[0.342, 1.141]\n\n\nL\n\n0.427***\n1.755***\n1.768***\n\n\n\n\n\n[0.341, 0.514]\n[1.584, 1.925]\n[1.597, 1.940]\n\n\n\nM\n\n\n0.784***\n\n\n\n\n\n\n\n[0.369, 1.200]\n\n\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n1000\n\n\nR2\n0.283\n\n0.311\n0.301\n0.013\n\n\nR2 Adj.\n0.282\n\n0.308\n0.300\n0.012\n\n\nAIC\n1022.7\n1278.2\n4824.1\n4835.8\n5178.8\n\n\nBIC\n1037.5\n1292.9\n4848.7\n4855.5\n5193.5\n\n\nLog.Lik.\n−508.364\n−636.108\n−2407.072\n−2413.913\n−2586.403\n\n\nRMSE\n0.40\n0.47\n2.69\n2.70\n3.21\n\n\n\n\n\n\n\nSince the treatment is randomized, no control variables are required in the regression of \\(Y\\) against \\(A\\) to remove confounding bias. However, \\(L\\) is in the \\(O\\)-set of the effect; this is, controlling for \\(L\\) produces an asymptotically more efficient estimator. Such property is not seen in finite samples [see last two models].\nIt is no surprise that, with complete data, the point-estimate for the coefficient of \\(A\\) in the last two models lie close to the true ATE (0.75), even under a moderately noisy DGP (\\(R^2\\approx 0.3\\))."
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#results-under-exclusion",
    "href": "posts/2023-08-01_missingOutcome/index.html#results-under-exclusion",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under exclusion",
    "text": "Results under exclusion\nIgnoring samples for which \\(S=0\\) reduces sample size from 1000 to 514. This is, the probability of exclusion is about 0.486.\n\n\nCode\n# Number of observation per selection\n# 1 = selected / observed\n# 0 = excluded\ntable(dat.1$S) %&gt;% kbl(col.names = c('S','N')) %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nS\nN\n\n\n\n\n0\n486\n\n\n1\n514\n\n\n\n\n\n\n\nIn contrast with the complete data case, a regression-based approach for estimating the ATE with only the selected samples does require controlling for \\(L\\), since conditioning on \\(S\\) opens the non-causal path:\n\n\\(A\\longrightarrow S\\longleftarrow L\\longrightarrow Y\\)\n\nUnfortunately, controlling for \\(L\\) alone does not remove selection bias, despite blocking such path. This is because the distribution of \\(L\\) in the sample does not necessarily match the distribution in the population.\nLet us inspect it.\n\nRegression-based approach:\nWe fit a model for the outcome \\(Y\\) against \\(A\\), controlling for \\(L\\), using only the selected samples:\n\n\nCode\n# Data for the selected sample\ndat.s = dat.1 %&gt;% filter(S==1)\nN.s = nrow(dat.s)\n\n# Model for causal effect, with selected\nmod.ate.s = lm(Y ~ A + L, data=dat.s)\nmodelsummary(list(\"ATE(S=1)\" = mod.ate.s), \n             statistic = NULL,\n             estimate  = \"{estimate}{stars} [{conf.low}, {conf.high}]\") \n\n\n\n\n\n\nATE(S=1)\n\n\n\n\n(Intercept)\n−1.341*** [−1.688, −0.995]\n\n\nA\n0.516* [0.058, 0.974]\n\n\nL\n1.843*** [1.601, 2.085]\n\n\nNum.Obs.\n514\n\n\nR2\n0.307\n\n\nR2 Adj.\n0.304\n\n\nAIC\n2458.1\n\n\nBIC\n2475.0\n\n\nLog.Lik.\n−1225.026\n\n\nRMSE\n2.62\n\n\n\n\n\n\n\nThe resulted 95% confidence interval for the coefficient of \\(A\\), using only samples for which \\(S=1\\), still covers the true ATE (0.75). Yet, a downwards bias shrinks the point-estimate and its lower bound noticeably. Using this, we would conclude the ATE is smaller than what truly is.\nLet us implement an IPW-based approach to compare against:\n\n\nIPW-based approach:\nGiven the SCM, and the assumption of \\(\\gamma_2=0\\), we can express:\n\n\\(w_i=\\mathbb{P}(S=1)/\\mathbb{P}(S=1\\mid A=a_i,L=l_i)\\)\n\\(v_i(a)=1/\\mathbb{P}(A=a) = 1/q\\) because treatment is randomized\n\nThe derived finite-sample estimator of the mean counterfactuals / potential outcomes is: \\[\n\\hat{Y}^a = N^{-1}\\sum_{i=1}^N\\frac{\\mathbb{I}(A_i=a)\\cdot\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\n\\]\nPutting all ingredients together we get point-estimates \\(\\hat{Y}^1=\\hat{\\mathbb{E}}[Y\\mid do(A=1)]\\) and \\(\\hat{Y}^0=\\hat{\\mathbb{E}}[Y\\mid do(A=0)]\\):\n\n\nCode\n# Compute the probability of selecton for all the units selected\n# We leverage the model mod.S, trained on complete data since S, A and L are\n# always observed (Y is the only one missing) and predict only on the selected units\nprob.s.unit = predict(mod.S, newdata=dat.s, type='response')\n\n# The unconditional probability of selection can be consistently estimated \n#... with counts from the total sample size and selected sample size\nprob.s = nrow(dat.s) / nrow(dat.1)\n\n# Since the treatment is randomized, the propensity score in the population is known at 0.5. \n# It can also be estimated from counts in the complete dataset\n# prop.score = sum(dat.1$A==1)/nrow(dat.1)\nprop.score = 0.5\n\n# Estimated counterfactuals/potential outcomes via IPPW\nest.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.Y = weights * Y) %&gt;% group_by(A) %&gt;% \n  summarise('PO'=sum(weighted.Y)/N.s)\n\n# Print resulted estimates\nhead(est.PO) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nA\nPO\n\n\n\n\n0\n-1.4318076\n\n\n1\n-0.7537995\n\n\n\n\n\n\n\nWhich, can be used to compute a point-estimate of the ATE: \\(\\hat{ATE}=\\hat{Y}^1-\\hat{Y}^0\\):\n\n\nCode\n# Print estimated ATE\nest.ATE = as.numeric(est.PO[2,2]-est.PO[1,2])\ndata.frame('IPW ATE'=est.ATE) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\n\n\n\n\n0.678008\n\n\n\n\n\n\n\nWe can see that the IPW-based approach produces a point-estimate closer to the true ATE. To get valid confidence intervals, however, a bootstrapping or asymptotic analysis need to be invoked. We can compute naïve confidence interval without resampling, using the fact:\n\\[\n\\hat{\\text{var}}(\\hat{\\text{ATE}}) \\geq \\hat{\\text{var}}(\\hat{Y}^1)+\\hat{\\text{var}}(\\hat{Y}^0)=(N-2)^{-2}\\sum_{a\\in\\{0,1\\}}\\sum_{i=1}^N \\mathbb{I}(A_i=a)\\cdot \\hat{v}_i(a)^2\\hat{w}_i^2(y_i-\\hat{Y}^a)^2\n\\]Such variance is naïve in the sense that it is overconfident due to ignoring the covariance between the potential outcomes, and the higher-order contributions of the weighting factors in the total variance. Anyway, using it will get us:\n\n\nCode\n# Estimated counterfactuals/potential outcomes via IPPW\nsd.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.var = weights^2 * ( A* (Y-est.PO[2,2])^2 + (1-A)* (Y-est.PO[1,2])^2)) %&gt;% \n  group_by(A) %&gt;% \n  summarise('sd'=sqrt(sum(weighted.var))/(N.s-2))\n\n# Naïve confidence interval\nnaivebounds = qnorm(0.975)*sum(sd.PO[,2])\ndata.frame('IPW ATE'=as.numeric(est.ATE),\n           'naïve LB'=as.numeric(est.ATE)-naivebounds,\n           'naïve UB'=as.numeric(est.ATE)+naivebounds) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\nnaïve.LB\nnaïve.UB\n\n\n\n\n0.678008\n0.5741569\n0.7818592\n\n\n\n\n\n\n\nAs noted, the naïve confidence interval is tighter than those produced by inference on the complete data. This means such confidence interval is not statistically valid, but it can still help us visualize the convergence of IPW estimator.\nNow, if we simulate and repeat the DGP and same analysis for different sample sizes, consistency would be visually perceived if we see:\n\nPoint-estimate converging to the true ATE\nConfidence intervals shrinking at a fast (**) rate\n\nLet us test it. We run 20 simulations with different complete sample sizes, from \\(N=500\\) to \\(N=23\\,000\\) [number of complete samples from \\(N_s=250\\) to \\(N_s=12\\,000\\)]. We repeat the procedure three times and average the results on those three repetitions. Such averages are presented in the following table:\n\n\nCode\n# Data frame to save results from iterations\nrounds.IPPW = data.frame(N=NA, Ns=NA, EST=NA, LB=NA, UB=NA)\n\n# All sample sizes to consider\nsamplesizes = round(500*exp(0.2*(0:19)))\n\n# Number of repetitions\nM = 3\n\n# Paramaters are the same as before\nparam.iter = param.1\n\n# Seed\nset.seed(66)\n\n# Loop\nfor(m in 1:M){\n  for(n in samplesizes){\n    \n    # Change sample size\n    param.iter[1] = n\n    \n    # Generate the data (complete and selected)\n    dat.iter = dgp(param.iter)[[1]]\n    dat.s.iter = dat.iter %&gt;% filter(S==1)\n    \n    # Estimated probability of selection\n    N.s.iter = nrow(dat.s.iter)\n    prob.s.iter = N.s.iter / n\n    \n    # Model for S, with complete data\n    mod.S.iter = glm(S ~ A + L, family=binomial('probit'), data=dat.iter)\n    prob.s.unit.iter = predict(mod.S.iter, newdata=dat.s.iter, type='response')\n  \n    # Propensity score model\n    prop.score.iter = sum(dat.iter$A==1)/nrow(dat.iter)\n    \n    # Put everything together\n    row.iter = dat.s.iter %&gt;% mutate(\n      prob.s.unit = as.numeric(prob.s.unit.iter),\n      prob.s = as.numeric(prob.s.iter),\n      prop.score = as.numeric(prop.score.iter),\n      pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n      weights = pro.weight * prob.s * (1/prob.s.unit),\n      weighted.Y = weights * Y) \n    \n    # Estimated counterfactuals/potential outcomes via IPPW\n    po.iter = row.iter %&gt;% group_by(A) %&gt;% \n      summarise('Y(A)'=sum(weighted.Y)/N.s.iter)\n    \n    # Estimated ATE\n    ATE.iter = as.numeric(po.iter[2,2]-po.iter[1,2])\n    \n    # Naïve variances\n    sd.PO.iter = row.iter %&gt;% mutate(\n      weighted.var = weights^2 * ( A* (Y-po.iter[2,2])^2 + (1-A)* (Y-po.iter[1,2])^2)) %&gt;% \n    group_by(A) %&gt;% \n    summarise('sd'=sqrt(sum(weighted.var))/(N.s.iter-1))\n    \n    # Naïve confidence interval\n    naivebounds = qnorm(0.975)*sum(sd.PO.iter[,2])\n    \n    rounds.IPPW = rbind.data.frame(rounds.IPPW,\n                                   data.frame(N=n,\n                                              Ns=N.s.iter,\n                                              EST=ATE.iter,\n                                              LB=ATE.iter-naivebounds,\n                                              UB=ATE.iter+naivebounds))\n  }\n}\n# Print resulted estimates\nrounds.IPPW = rounds.IPPW[-1,] %&gt;%\n  group_by(N) %&gt;%\n  summarise_all(mean)\n\nhead(rounds.IPPW) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nN\nNs\nEST\nLB\nUB\n\n\n\n\n500\n249.6667\n0.5919352\n-0.1461034\n1.3299737\n\n\n611\n309.0000\n0.7701710\n0.1782066\n1.3621354\n\n\n746\n379.6667\n0.8962619\n0.2991885\n1.4933353\n\n\n911\n453.6667\n0.7438565\n0.5811669\n0.9065460\n\n\n1113\n556.0000\n0.7817429\n0.3579693\n1.2055165\n\n\n1359\n679.0000\n0.6411555\n0.3223225\n0.9599884\n\n\n\n\n\n\n\nAn the following plot:\n\n\nCode\n### Plot results from rounds\nrounds.IPPW[,-1] %&gt;% melt(id.vars = 'Ns') %&gt;% \n  mutate(type = ifelse(variable=='EST','EST','N. C.I.') ) %&gt;% \n  ggplot(aes(x=Ns,y=value,group=variable)) + \n  geom_point(aes(shape=type,color=type)) + \n  geom_smooth(method = lm, formula = y ~ x + I(sqrt(x)), se = FALSE) + ## O(N^0.5) convergence\n  labs(y='Value of estimate / bound', x='number of complete samples') +\n  geom_hline(yintercept=T.ATE, col = 'red') +\n  theme_bw()\n\n\n\n\n\nWe can appreciate that the estimator converges to the true ATE [in red], and its uncertainty reduce at a sustained rate; there is convergence in probability. In other words, the IPW-based estimator is consistent.\n\nMathematical justification of consistency\nWe can prove consistency mathematically for this SCM. However, to avoid measure-theoretic conundrums, let us consider the case for which all variables are discrete. Results are generalizable for mixed discrete-continuous cases with positive distributions (no zero-measure events).\nLet us assume \\(Y\\) has support on \\(\\{y_{(c)}\\}_{c=1}^C\\), and \\(L\\) has support on \\(\\{l_{(k)}\\}_{k=1}^K\\), then:\n\\[\n\\begin{aligned}\n\\hat{Y}^a &= N^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\\cdot \\mathbb{I}(A_i=a) \\\\\n&=\nN^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a\\mid L=l_i)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a,L=l_i)}\\cdot y_i\\cdot\\mathbb{I}(A_i=a,S_i=1)\\\\\n&=\n\\sum_{i=1}^N\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_i)\n}\\cdot\n\\frac{\ny_i\\cdot\\mathbb{I}(A_i=a,S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_i,S=1)\n}\\\\\n&= \\sum_{i=1}^N\\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{i}\\cdot\\mathbb{I}(A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\sum_{i=1}^N\\mathbb{I}(Y_i=y_{(c)},A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\hat{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\n\\end{aligned}\n\\]\nAssuming the propensity scores and the probability of selection are both correctly specified, all finite-sample approximations \\(\\hat{\\mathbb{P}}\\) converge in the limit to the true distributions \\(\\mathbb{P}\\). Then:\n\\[\n\\begin{aligned}\n\\text{plim}_{N\\rightarrow\\infty}\n\\hat{Y}^a\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(S=1)}{\n{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(L=l_{(k)})}{\n{\\mathbb{P}}(L=l_{(k)}\\mid S=1)\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n{\\mathbb{P}}(L=l_{(k)})\n\\cdot\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)} S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\sum_{c}\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)}, S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\mathbb{E}(Y\\mid A=a,L, S=1)\\\\\n&=\n\\mathbb{E}_L\n\\mathbb{E}(Y\\mid A=a,L, S=1) =\\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L,S=1]\\\\\n&= \\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L] = \\mathbb{E}[Y\\mid do(A=a)]\n\\end{aligned}\n\\]\nThis is, the IPW estimator converges to the true counterfactual mean given by intervention \\(do(A=a)\\). The last two equalities come from these facts:\n\n\\(L\\) blocks all non-causal paths from \\(A\\) to \\(Y\\) when conditioning on \\(S=1\\)\n\\(Y\\perp S\\,\\mid L\\) in the back-door graph: the resulting DAG after removing all arrows coming out of \\(A\\)\n\\(\\mathbb{P}(L=l_{(k)})\\) is the population-distribution of \\(L\\)\n\n\n\n\nRevisiting the regression approach: generalized adjustment criteria\nNotice that, in the convergence proof for the IPW estimator, it was shown that the underlying estimand is algebraically equivalent to a regression-adjusted mean of \\(Y\\), averaged over the population distribution of \\(L\\).\n\\[\n\\mathbb{E}[Y\\mid do(A=a)] = \\mathbb{E}_L\\mathbb{E}(Y\\mid A=a,L,S=1)\n\\]\nExtensions of this results are covered by three generalized adjustment criteria (Correa, Tian, and Bareinboim 2018). This is, for this case, a mathematically equivalent result in term of consistency can be achieved via regression adjustment."
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-identifiability",
    "href": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-identifiability",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The problem of identifiability",
    "text": "The problem of identifiability\nWhen there is no sample selection nor missingness, or when missing is completely at random (MCAR), all arrows pointing to \\(R_Y\\) are absent. The \\(m\\)-graph representing the system correspond to a causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\), and samples are obtained from the observational distribution \\(P(H,A,M,Y)\\). This is the traditional setting motivating causal inference with observational data.\n\n\n\nFigure 2: causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\)\n\n\nUnder the assumptions embedded in the causal graph \\(\\mathcal{G}'\\), and a special mutilation known as the back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\)2, \\(\\psi\\) is nonparametrically identifiable from \\(P(H,A,M,Y)\\) via the back-door formula (Pearl 1995, 2012), as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}[Y\\mid H,A=a]\n\\]\nGiven identifiability plus \\(N\\) i.i.d. sample from \\(P(H,A,M,Y)\\), a consistent estimator can be constructed using a regression model for the outcome \\(\\hat{Q}(H,A)=\\hat{\\mathbb{E}}[Y\\mid H,A]\\), and then proceeding with \\(g\\)-computation (Robins 1986):\n\\[\n\\hat{\\psi} = N^{-1}\\sum_{i=1}^{N}\\Delta_a\\hat{Q}(H_i,a)\n\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-recoverability",
    "href": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-recoverability",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The problem of recoverability",
    "text": "The problem of recoverability\nA (causal) parameter is said to be recoverable from the observed-data distribution \\(P(H,A,M,Y^*,R_Y)\\equiv\\{P(H,A,M,Y\\mid R_Y=1),P(H,A,M) \\}\\) if it can be uniquely computed from it using the assumptions embedded in \\(\\mathcal{G}\\) (and the necessary graph mutilation).\nUnder identifiability in the substantive model 3, \\(\\mathcal{G}[\\overline{R_Y}]\\), there is an ample number of methods to recover joint/conditional distributions from sample selection/missingness, based on different statistical theories. Although not originally motivated by graphical models, they can be seen as ad hoc solutions under special graphical conditions (Mohan and Pearl 2021). Table 1 presents a summary of literature review 4 benchmarking four methodological approaches in terms of:\n\nGraphical conditions for recoveravility: from hard (\\(\\bigstar\\)) to easy (\\(\\bigstar\\bigstar\\bigstar\\)) to fulfill/believe\nFlexibility in model specification: from parametric (\\(\\bigstar\\)) to ML/nonparametric (\\(\\bigstar\\bigstar\\bigstar\\))\nStatistical efficiency: from wider (\\(\\bigstar\\)) to narrower (\\(\\bigstar\\bigstar\\bigstar\\)) confidence/credible intervals\nComputational efficiency: from slow (\\(\\bigstar\\)) to fast (\\(\\bigstar\\bigstar\\bigstar\\)) computation/convergence\n\n\nTable 1: some statistical methods to address selection/missingness\n\n\n\n\n\n\n\n\n\nMethod\nGraph cond.\nFlex. spec.\nStat. eff.\nComp. eff.\n\n\n\n\nExpectation-maximization (Dempster, Laird, and Rubin 1977)\n\\(\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\star\\)\n\\(\\bigstar\\)\n\n\nMultiple imputation (Rubin 1976, 1978)\n\\(\\bigstar\\star\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\n\nInverse probability weighting (Robins and Rotnitzky 1992; Robins, Rotnitzky, and Zhao 1994)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\nRegression adjustment (Bareinboim, Tian, and Pearl 2014; J. Correa, Tian, and Bareinboim 2018)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\n\nArguably, the best set of properties come from IPW and regression adjustment, due to their direct derivation from graphical criteria, which might extend the Rubin-MAR setting. Moreover, both solutions have important theoretical results from the theory of semiparametric estimation, and produce doubly- or multiply-robustness when combined. These reasons have motivated syncretic estimators, such as:\n\nTable 2: some doubly/multiply-robust estimation methods\n\n\n\n\n\n\n\n\n\n\nMethod\nML & adaptive\nFast consistency\nPlug-in for target\nBayesian version\n# iter. steps\n\n\n\n\nAugmented inverse probability weighting (AIPW) (Robins, Rotnitzky, and Zhao 1994)\nHuh\nNo\nNo\nNo\n0\n\n\nTargeted learning (van der Laan and Rose 2011)\nYes\nYes\nYes\nYes, kinda\n\\(\\geq 1\\)\n\n\nDebiased machine learning (DML) (Chernozhukov et al. 2018)\nYes\nYes\nNo\nNo\n0\n\n\n\n\nRecoverability via IPW\nGraphical (NS) conditions for recoverability via IPW (Mohan and Pearl 2014), with missing data on \\(Y\\), are:\n\nThere is a back-door admissible set in the substantive model (\\(H\\))\nNo self-selection: there are no directed arrows between \\(Y\\) and \\(R_Y\\)\nNo open collider paths between \\(Y\\) and \\(R_Y\\) (open by variables involved in the query)\n(When there are multiple missingness mechanisms: \\(R_V\\cap R_{\\text{mb}(R_V)}=\\emptyset\\))\n\nOur \\(m\\)-graph \\(\\mathcal{G}\\) (figure 1) allows recoverability, so we can express : \\[\n\\begin{aligned}\n    p(Y\\mid do(A)) &= \\int\\frac{ \\text{d} H}{p(A\\mid H)}\\int \\frac{\\text{d} M}{\\mathbb{P}(R_Y=1\\mid H,M)}\\, p(H,A,M,Y\\mid R_Y=1)  \\\\\n    &= \\mathbb{E}_{H\\mid R_Y=1}\\left[\\frac{p(A,Y\\mid H,M,R_Y=1)}{p(A\\mid H)\\, \\mathbb{P}(R_Y=1\\mid H,M) }  \\right]\n\\end{aligned}\n\\]\nThus, the IPW-estimator of the ATE is: \\[\n    \\hat{\\psi}^{w} = N_1^{-1}\\sum_{i=1}^{N_1}\\frac{(2A^i-1)\\,Y^i}{\\hat{p}(A^i\\mid H^i)\\,\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i) }\n\\]\nIt requires two models:\n\nTreatment-assignment mechanism: \\(\\hat{p}(A^i\\mid H^i)\\). It does not involve the mediator \\(M\\)\nSelection mechanism: \\(\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i)\\). It does involve the mediator \\(M\\)\n\n\n\nRecoverability via regression adjustment\n\nSince the identification (+estimation) problem in the substantive model is solved via regression and \\(g\\)-computation, can this approach leverage recovery (+estimation)?\n\nNotice that, working with samples from \\(P(H,A,M,Y^*,R_Y)\\equiv\\{P(H,A,M,Y\\mid R_Y=1),P(H,A,M) \\}\\) implies conditioning on \\(R_Y=1\\) (Bareinboim and Pearl 2012). In the \\(m\\)-graph of figure 1, \\(\\mathcal{G}\\), such condition opens the following non-causal paths in the (proper) back-door graph:\n\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow M\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow M\\longrightarrow Y\\)\n\n\n\n\nFigure 3: The (proper) back-door graph\n\n\nGraphical (NS?) conditions for recoverability via GAC (J. Correa, Tian, and Bareinboim 2018), with missing data on \\(Y\\), using an adjustment set \\(Z\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(Z\\) and \\(R_Y\\): \\(Y\\perp A\\mid Z, R_Y\\) in the (proper) back-door graph\n\\(Z\\) \\(d\\)-separates \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid Z\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(Z\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nNo adjustment set fulfills all these critera. In particular, \\(Z=\\{H,M\\}\\) fulfills the first two, but not the third.\n\nThe criteria are incomplete, because they do not consider post-treatment selection influenced by mediators. Do not worry! I came with a fix\nde Aguas, Biele and Pensar’s recoverability criteria via regression adjustment with missing data on \\(Y\\) using pre-treatment set \\(H\\) and forbidden set \\(M\\subset \\text{fb}(A,Y;\\mathcal{G})\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(H\\) and \\(R_Y\\): \\(Y\\perp A\\mid H\\) in the (proper) back-door graph of the substantive model\n\\(H,M\\) \\(d\\)-separate \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid H,M\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(H\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nThis modification is not a revolutionary discovery. It is implied from the sequential factorization by Mohan and Pearl (2014), and from \\(c\\)-factorization by J. D. Correa, Tian, and Bareinboim (2019). Yet, the former does not do it in the context of causal inference, and the latter might be a fairly complicated overshoot. A nice list of graphical criteria, as in the case without forbidden nodes, might be more useful for researchers. Besides, IPW tends to be the first option in applied research, maybe it is thought that in some contexts regression adjustment is not possible.\n\nUnder the modified criteria, we have that:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}_{M\\mid H,A=a}\\mathbb{E}[Y\\mid H,A=a, M, R_Y=1]\n\\] Notice now the solution requires two models:\n\nAn outcome model, with \\(M\\) as predictor, for \\(Q(H,A,M)=\\mathbb{E}[Y\\mid H,A, M, R_Y=1]\\)\n\nA mediator model, to estimate \\(p(M\\mid H,A)\\)\n\nHow to specify these models? I see three options, including a dimension reduction \\(M'=\\Phi(M)\\) using either PCA, VAE, or representation learning:\n\nTable 3: methodological options to estimate \\(\\psi\\)\n\n\n\n\n\n\n\n\n\n\nOption\n\\(\\mathcal{M}_1\\)\n\\(\\mathcal{M}_2\\)\n\\(\\mathcal{M}_2\\) dim. reduction\nEasy to implement\nStat./TMLE friendly\n\n\n\n\n\\(Y\\)-regression and full \\(M\\)-model (Tchetgen and Shpitser 2012)\n\\(\\hat{Q}(H,A,M)\\)\n\\(\\hat{p}(M\\mid H,A)\\)\nNo\nSmall \\(M\\)\nYes\n\n\n\\(Y\\)-regression and \\(M\\)-reduction (Z. Xu et al. 2023; Nath et al. 2023)\n\\(\\hat{Q}(H,A,M')\\)\n\\(M'=\\Phi(M)\\) \\(\\hat{p}(M'\\mid H,A)\\)\nYes\nKinda\nMisspec!\n\n\nNested regressions (S. Xu, Liu, and Liu 2022)\n\\(\\hat{Q}(H,A,M)\\)\n\\(\\hat{Q}\\sim H,A\\)\nNo\nYes\nMaybe\n\n\n\n\n\nMultiply-robustness\nCombining IPW and regression adjustment solutions produces multiply-robustness; more specifically triply- in this case. An estimator for the ATE can be constructed such that is consistent if all semiparametric models involed are correctly specified, or in one of these scenarios:\n\n\\(M\\), \\(Y\\) are all well specified\n\\(A\\), \\(M\\), \\(R_Y\\) are all well specified\n\\(A\\), \\(Y\\), \\(R_Y\\) are all well specified\n\nNotice that, using a misspecified model for \\(M\\) (like when reducing its dimension with PCA) leaves the estimator worse than simply using IPW, as it requires correct specification of \\(A\\), \\(Y\\), \\(R_Y\\), whereas IPW only requires \\(A\\), \\(R_Y\\)"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#setting",
    "href": "posts/2023-08-01_missingOutcome/index.html#setting",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Setting",
    "text": "Setting\nConsider the following \\(m\\)-graph1, \\(\\mathcal{G}\\), representing the causal relations among a set of random variables \\(\\mathcal{V}=\\{H,A,M,Y,R_Y\\}\\), where:\n\n\\(H\\in\\mathbb{R}^d\\) is a vector of pre-treatment and context covariates\n\\(A\\in\\{0,1\\}\\) is a binary exposure\n\\(Y\\) is the outcome of interest, with general support (univariate or multivariate, discrete or continuous)\n\\(M\\) is a mediator on the causal pathway from \\(A\\) to \\(Y\\), with general support\n\\(R_Y\\in\\{0,1\\}\\) is an indicator of sample selection for \\(Y\\), i.e., for a given sample, \\(R_Y=1\\) means \\(Y\\) is observed; otherwise \\(Y\\) is missing (denoted with proxy \\(Y^*=\\emptyset\\)).\n\n\n\n\nFigure 1: \\(m\\)-graph \\(\\mathcal{G}\\)\n\n\nOur goal is to estimate the average treatment effect (ATE), \\(\\psi\\), in the target population, defined as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}[Y\\mid do(A=a)] := \\mathbb{E}[Y\\mid do(A=1)]-\\mathbb{E}[Y\\mid do(A=0)]\n\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#footnotes",
    "href": "posts/2023-08-01_missingOutcome/index.html#footnotes",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(m\\)-graphs generalize causal graphs in settings with sample selection (Hernán, Hernández-Díaz, and Robins 2004) and missing data (Mohan and Pearl 2021).↩︎\nThe back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\) is the graph resulting from removing in \\(\\mathcal{G}'\\) the first arrow of all directed paths from \\(A\\) to \\(Y\\). It is termed the proper back-door graph in multi-exposure settings, and results from removing the first arrow of all directed and non-self-intersecting paths from \\(A\\) to \\(Y\\).↩︎\nRecoverability might be possible without identifiability in the substantive model, via (fairly complicated) \\(c\\)-factorizations (J. D. Correa, Tian, and Bareinboim 2019) or \\(\\phi\\)-factorizations (Bhattacharya et al. 2020) related to the problem of \\(g\\)-identifiability↩︎\nLiterature review based on Seaman and White (2013), Dong and Peng (2013), Perkins et al. (2017), Lewin et al. (2018)↩︎"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#simulations",
    "href": "posts/2023-08-01_missingOutcome/index.html#simulations",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Simulations",
    "text": "Simulations\nCheck this"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#path-forward",
    "href": "posts/2023-08-01_missingOutcome/index.html#path-forward",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Path forward",
    "text": "Path forward\nLet us commit to:\n\nOne missing mechanism: \\(R_Y\\)\n\\(m\\)-graph depicted in figure 1, so we have identifiability in the substantive model, and recoverability via IPW and regression\nTMLE (the super-learner is default, we could restrict to only splines or BART, Bayesian?)\nFull \\(M\\)-model for one mediator, nested regressions for more\nTry simulations"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#setting",
    "href": "posts/2023-08-15_missingSimulation/index.html#setting",
    "title": "XXX",
    "section": "Setting",
    "text": "Setting\nConsider the following \\(m\\)-graph1, \\(\\mathcal{G}\\), representing the causal relations among a set of random variables \\(\\mathcal{V}=\\{H,A,M,Y,R_Y\\}\\), where:\n\n\\(H\\in\\mathbb{R}^d\\) is a vector of pre-treatment and context covariates\n\\(A\\in\\{0,1\\}\\) is a binary exposure\n\\(Y\\) is the outcome of interest, with general support (univariate or multivariate, discrete or continuous)\n\\(M\\) is a mediator on the causal pathway from \\(A\\) to \\(Y\\), with general support\n\\(R_Y\\in\\{0,1\\}\\) is an indicator of sample selection for \\(Y\\), i.e., for a given sample, \\(R_Y=1\\) means \\(Y\\) is observed; otherwise \\(Y\\) is missing (denoted with proxy \\(Y^*=\\emptyset\\)).\n\n\n\n\nFigure 1: \\(m\\)-graph \\(\\mathcal{G}\\)\n\n\nOur goal is to estimate the average treatment effect (ATE), \\(\\psi\\), in the target population, defined as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}[Y\\mid do(A=a)] := \\mathbb{E}[Y\\mid do(A=1)]-\\mathbb{E}[Y\\mid do(A=0)]\n\\]"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-identifiability",
    "href": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-identifiability",
    "title": "XXX",
    "section": "The problem of identifiability",
    "text": "The problem of identifiability\nWhen there is no sample selection nor missingness, or when missing is completely at random (MCAR), all arrows pointing to \\(R_Y\\) are absent. The \\(m\\)-graph representing the system correspond to a causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\), and samples are obtained from the observational distribution \\(P(H,A,M,Y)\\). This is the traditional setting motivating causal inference with observational data.\n\n\n\nFigure 2: causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\)\n\n\nUnder the assumptions embedded in the causal graph \\(\\mathcal{G}'\\), and a special mutilation known as the back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\)2, \\(\\psi\\) is nonparametrically identifiable from \\(P(H,A,M,Y)\\) via the back-door formula (Pearl 1995, 2012), as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}[Y\\mid H,A=a]\n\\]\nGiven identifiability plus \\(N\\) i.i.d. sample from \\(P(H,A,M,Y)\\), a consistent estimator can be constructed using a regression model for the outcome \\(\\hat{Q}(H,A)=\\hat{\\mathbb{E}}[Y\\mid H,A]\\), and then proceeding with \\(g\\)-computation (Robins 1986):\n\\[\n\\hat{\\psi} = N^{-1}\\sum_{i=1}^{N}\\Delta_a\\hat{Q}(H_i,a)\n\\]"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-recoverability",
    "href": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-recoverability",
    "title": "XXX",
    "section": "The problem of recoverability",
    "text": "The problem of recoverability\nA (causal) parameter is said to be recoverable from the observed-data distribution \\(P(H,A,M,Y^*,R_Y)\\equiv\\{P(H,A,M,Y\\mid R_Y=1),P(H,A,M) \\}\\) if it can be uniquely computed from it using the assumptions embedded in \\(\\mathcal{G}\\) (and the necessary graph mutilation).\nUnder identifiability in the substantive model 3, \\(\\mathcal{G}[\\overline{R_Y}]\\), there is an ample number of methods to recover joint/conditional distributions from sample selection/missingness, based on different statistical theories. Although not originally motivated by graphical models, they can be seen as ad hoc solutions under special graphical conditions (Mohan and Pearl 2021). Table 1 presents a summary of literature review 4 benchmarking four methodological approaches in terms of:\n\nGraphical conditions for recoveravility: from hard (\\(\\bigstar\\)) to easy (\\(\\bigstar\\bigstar\\bigstar\\)) to fulfill/believe\nFlexibility in model specification: from parametric (\\(\\bigstar\\)) to ML/nonparametric (\\(\\bigstar\\bigstar\\bigstar\\))\nStatistical efficiency: from wider (\\(\\bigstar\\)) to narrower (\\(\\bigstar\\bigstar\\bigstar\\)) confidence/credible intervals\nComputational efficiency: from slow (\\(\\bigstar\\)) to fast (\\(\\bigstar\\bigstar\\bigstar\\)) computation/convergence\n\n\nTable 1: some statistical methods to address selection/missingness\n\n\n\n\n\n\n\n\n\nMethod\nGraph cond.\nFlex. spec.\nStat. eff.\nComp. eff.\n\n\n\n\nExpectation-maximization (Dempster, Laird, and Rubin 1977)\n\\(\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\star\\)\n\\(\\bigstar\\)\n\n\nMultiple imputation (Rubin 1976, 1978)\n\\(\\bigstar\\star\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\n\nInverse probability weighting (Robins and Rotnitzky 1992; Robins, Rotnitzky, and Zhao 1994)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\nRegression adjustment (Bareinboim, Tian, and Pearl 2014; J. Correa, Tian, and Bareinboim 2018)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\n\nArguably, the best set of properties come from IPW and regression adjustment, due to their direct derivation from graphical criteria, which might extend the Rubin-MAR setting. Moreover, both solutions have important theoretical results from the theory of semiparametric estimation, and produce doubly- or multiply-robustness when combined. These reasons have motivated syncretic estimators, such as:\n\nTable 2: some doubly/multiply-robust estimation methods\n\n\n\n\n\n\n\n\n\n\nMethod\nML & adaptive\nFast consistency\nPlug-in for target\nBayesian version\n# iter. steps\n\n\n\n\nAugmented inverse probability weighting (AIPW) (Robins, Rotnitzky, and Zhao 1994)\nHuh\nNo\nNo\nNo\n0\n\n\nTargeted learning (van der Laan and Rose 2011)\nYes\nYes\nYes\nYes, kinda\n\\(\\geq 1\\)\n\n\nDebiased machine learning (DML) (Chernozhukov et al. 2018)\nYes\nYes\nNo\nNo\n0\n\n\n\n\nRecoverability via IPW\nGraphical (NS) conditions for recoverability via IPW (Mohan and Pearl 2014), with missing data on \\(Y\\), are:\n\nThere is a back-door admissible set in the substantive model (\\(H\\))\nNo self-selection: there are no directed arrows between \\(Y\\) and \\(R_Y\\)\nNo open collider paths between \\(Y\\) and \\(R_Y\\) (open by variables involved in the query)\n(When there are multiple missingness mechanisms: \\(R_V\\cap R_{\\text{mb}(R_V)}=\\emptyset\\))\n\nOur \\(m\\)-graph \\(\\mathcal{G}\\) (figure 1) allows recoverability, so we can express : \\[\n\\begin{aligned}\n    p(Y\\mid do(A)) &= \\int\\frac{ \\text{d} H}{p(A\\mid H)}\\int \\frac{\\text{d} M}{\\mathbb{P}(R_Y=1\\mid H,M)}\\, p(H,A,M,Y\\mid R_Y=1)  \\\\\n    &= \\mathbb{E}_{H\\mid R_Y=1}\\left[\\frac{p(A,Y\\mid H,M,R_Y=1)}{p(A\\mid H)\\, \\mathbb{P}(R_Y=1\\mid H,M) }  \\right]\n\\end{aligned}\n\\]\nThus, the IPW-estimator of the ATE is: \\[\n    \\hat{\\psi}^{w} = N_1^{-1}\\sum_{i=1}^{N_1}\\frac{(2A^i-1)\\,Y^i}{\\hat{p}(A^i\\mid H^i)\\,\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i) }\n\\]\nIt requires two models:\n\nTreatment-assignment mechanism: \\(\\hat{p}(A^i\\mid H^i)\\). It does not involve the mediator \\(M\\)\nSelection mechanism: \\(\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i)\\). It does involve the mediator \\(M\\)\n\n\n\nRecoverability via regression adjustment\n\nSince the identification (+estimation) problem in the substantive model is solved via regression and \\(g\\)-computation, can this approach leverage recovery (+estimation)?\n\nNotice that, working with samples from \\(P(H,A,M,Y^*,R_Y)\\equiv\\{P(H,A,M,Y\\mid R_Y=1),P(H,A,M) \\}\\) implies conditioning on \\(R_Y=1\\) (Bareinboim and Pearl 2012). In the \\(m\\)-graph of figure 1, \\(\\mathcal{G}\\), such condition opens the following non-causal paths in the (proper) back-door graph:\n\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow M\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow M\\longrightarrow Y\\)\n\n\n\n\nFigure 3: The (proper) back-door graph\n\n\nGraphical (NS?) conditions for recoverability via GAC (J. Correa, Tian, and Bareinboim 2018), with missing data on \\(Y\\), using an adjustment set \\(Z\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(Z\\) and \\(R_Y\\): \\(Y\\perp A\\mid Z, R_Y\\) in the (proper) back-door graph\n\\(Z\\) \\(d\\)-separates \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid Z\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(Z\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nNo adjustment set fulfills all these critera. In particular, \\(Z=\\{H,M\\}\\) fulfills the first two, but not the third.\n\nThe criteria are incomplete, because they do not consider post-treatment selection influenced by mediators. Do not worry! I came with a fix\nde Aguas, Biele and Pensar’s recoverability criteria via regression adjustment with missing data on \\(Y\\) using pre-treatment set \\(H\\) and forbidden set \\(M\\subset \\text{fb}(A,Y;\\mathcal{G})\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(H\\) and \\(R_Y\\): \\(Y\\perp A\\mid H\\) in the (proper) back-door graph of the substantive model\n\\(H,M\\) \\(d\\)-separate \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid H,M\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(H\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nThis modification is not a revolutionary discovery. It is implied from the sequential factorization by Mohan and Pearl (2014), and from \\(c\\)-factorization by J. D. Correa, Tian, and Bareinboim (2019). Yet, the former does not do it in the context of causal inference, and the latter might be a fairly complicated overshoot. A nice list of graphical criteria, as in the case without forbidden nodes, might be more useful for researchers. Besides, IPW tends to be the first option in applied research, maybe it is thought that in some contexts regression adjustment is not possible.\n\nUnder the modified criteria, we have that:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}_{M\\mid H,A=a}\\mathbb{E}[Y\\mid H,A=a, M, R_Y=1]\n\\] Notice now the solution requires two models:\n\nAn outcome model, with \\(M\\) as predictor, for \\(Q(H,A,M)=\\mathbb{E}[Y\\mid H,A, M, R_Y=1]\\)\n\nA mediator model, to estimate \\(p(M\\mid H,A)\\)\n\nHow to specify these models? I see three options, including a dimension reduction \\(M'=\\Phi(M)\\) using either PCA, VAE, or representation learning:\n\nTable 3: methodological options to estimate \\(\\psi\\)\n\n\n\n\n\n\n\n\n\n\nOption\n\\(\\mathcal{M}_1\\)\n\\(\\mathcal{M}_2\\)\n\\(\\mathcal{M}_2\\) dim. reduction\nEasy to implement\nStat./TMLE friendly\n\n\n\n\n\\(Y\\)-regression and full \\(M\\)-model (Tchetgen and Shpitser 2012)\n\\(\\hat{Q}(H,A,M)\\)\n\\(\\hat{p}(M\\mid H,A)\\)\nNo\nSmall \\(M\\)\nYes\n\n\n\\(Y\\)-regression and \\(M\\)-reduction (Z. Xu et al. 2023; Nath et al. 2023)\n\\(\\hat{Q}(H,A,M')\\)\n\\(M'=\\Phi(M)\\) \\(\\hat{p}(M'\\mid H,A)\\)\nYes\nKinda\nMisspec!\n\n\nNested regressions (S. Xu, Liu, and Liu 2022)\n\\(\\hat{Q}(H,A,M)\\)\n\\(\\hat{Q}\\sim H,A\\)\nNo\nYes\nMaybe\n\n\n\n\n\nMultiply-robustness\nCombining IPW and regression adjustment solutions produces multiply-robustness; more specifically triply- in this case. An estimator for the ATE can be constructed such that is consistent if all semiparametric models involed are correctly specified, or in one of these scenarios:\n\n\\(M\\), \\(Y\\) are all well specified\n\\(A\\), \\(M\\), \\(R_Y\\) are all well specified\n\\(A\\), \\(Y\\), \\(R_Y\\) are all well specified\n\nNotice that, using a misspecified model for \\(M\\) (like when reducing its dimension with PCA) leaves the estimator worse than simply using IPW, as it requires correct specification of \\(A\\), \\(Y\\), \\(R_Y\\), whereas IPW only requires \\(A\\), \\(R_Y\\)"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#simulations",
    "href": "posts/2023-08-15_missingSimulation/index.html#simulations",
    "title": "XXX",
    "section": "Simulations",
    "text": "Simulations\nCheck this"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#footnotes",
    "href": "posts/2023-08-15_missingSimulation/index.html#footnotes",
    "title": "XXX",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(m\\)-graphs generalize causal graphs in settings with sample selection (Hernán, Hernández-Díaz, and Robins 2004) and missing data (Mohan and Pearl 2021).↩︎\nThe back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\) is the graph resulting from removing in \\(\\mathcal{G}'\\) the first arrow of all directed paths from \\(A\\) to \\(Y\\). It is termed the proper back-door graph in multi-exposure settings, and results from removing the first arrow of all directed and non-self-intersecting paths from \\(A\\) to \\(Y\\).↩︎\nRecoverability might be possible without identifiability in the substantive model, via (fairly complicated) \\(c\\)-factorizations (J. D. Correa, Tian, and Bareinboim 2019) or \\(\\phi\\)-factorizations (Bhattacharya et al. 2020) related to the problem of \\(g\\)-identifiability↩︎\nLiterature review based on Seaman and White (2013), Dong and Peng (2013), Perkins et al. (2017), Lewin et al. (2018)↩︎"
  }
]