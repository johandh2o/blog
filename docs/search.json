[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data\n\n\n\nselection bias\n\n\nmediation\n\n\nregression\n\n\nIPW\n\n\ndoubly-robust\n\n\nTMLE\n\n\n\nA simulation exercise on missing data, selection bias, causal inference and TMLE\n\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructure learning for downstream causal inference with missing outcome data\n\n\n\nBayesian\n\n\nMCMC\n\n\nVI\n\n\ncausal inference\n\n\ngraphical models\n\n\nstructure learning\n\n\n\nA set of ideas on how to leverage structure learning and graph uncertainty quantification for downstream causal inference with missing data mechanisms in the outcome variable\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty modeling and quantification for causal inference\n\n\n\nUQ\n\n\ncausal inference\n\n\ngraphical models\n\n\nstructure learning\n\n\n\nA theoretical introduction of UQ in causal inference\n\n\n\n\n\n\nSep 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation task: recovering causal effects from post-treatment selection induced by missing outcome data\n\n\n\nselection bias\n\n\nmediation\n\n\nregression\n\n\nIPW\n\n\ndoubly-robust\n\n\nTMLE\n\n\n\nA simulation exercise on missing data, selection bias, causal inference and TMLE\n\n\n\n\n\n\nAug 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecovering causal effects from post-treatment selection induced by missing outcome data\n\n\n\nselection bias\n\n\nmediation\n\n\nregression\n\n\nIPW\n\n\ndoubly-robust\n\n\nTMLE\n\n\n\nSolutions to the problem of effect recoverability from selection/missingness via regression adjustment and doubly-robust estimation\n\n\n\n\n\n\nAug 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecovering from selection bias with IPW methods\n\n\n\nIPW\n\n\nselection bias\n\n\n\nOn the consistency of IPW methods to recover causal effects from selection bias\n\n\n\n\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html",
    "href": "posts/2023-03-01_ipw/index.html",
    "title": "Recovering from selection bias with IPW methods",
    "section": "",
    "text": "Confounding bias and selection bias are together the most prevalent hurdles to the validity and generalizability of causal inference results. In general, both arise from uncontrolled extraneous flows of statistical information between treatment and outcome in the analysis. Their precise characterization in the Structural Causal Models framework (SCM), and potential corrections, differ in nature:"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#simple-simulation-setting",
    "href": "posts/2023-03-01_ipw/index.html#simple-simulation-setting",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Simple simulation setting",
    "text": "Simple simulation setting\nLet us consider the SCM given by the following DAG and set of structural equations:"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#the-dag",
    "href": "posts/2023-03-01_ipw/index.html#the-dag",
    "title": "Recovering from selection bias with IPW methods",
    "section": "The DAG:",
    "text": "The DAG:\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  M ~ A,\n  S ~ A + L,\n  Y ~ A + M + L\n) %&gt;% tidy_dagitty(layout = \"nicely\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#structural-equations",
    "href": "posts/2023-03-01_ipw/index.html#structural-equations",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Structural equations:",
    "text": "Structural equations:\n\\[\n\\begin{aligned}[c]\nL &\\sim\\text{Nor}(0,\\sigma^2_L) & &\\\\\nA &\\sim\\text{Ber}(q) & &\\\\\nM &= \\alpha_0 + \\alpha_1A + u_M & u_M &\\sim\\text{Nor}(0,\\sigma^2_M) \\\\\nS &= \\mathbb{I}[\\gamma_0 + \\gamma_1A + \\gamma_2M + \\gamma_3L + u_S &gt; 0] & u_S &\\sim\\text{Nor}(0,\\sigma^2_S) \\\\\nY &= \\beta_0 + \\beta_1A + \\beta_2M + \\beta_3L + u_Y & u_Y &\\sim\\text{Nor}(0,\\sigma^2_Y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#selection-mechanism",
    "href": "posts/2023-03-01_ipw/index.html#selection-mechanism",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Selection mechanism:",
    "text": "Selection mechanism:\nWhen \\(S=1\\) for a particular unit, we get to observe their whole data \\((L,A,M,Y)\\). When \\(S=1\\), only the final outcome \\(Y\\) is missing, so we get to observe \\((L,A,M)\\).\nLet us put this SCM as a data-generating process (DGP) in R code:\n\n\nCode\n# Libraries needed\nlibrary(DescTools)\nlibrary(LaplacesDemon)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(zeallot)\nlibrary(reshape2)\n\n# Data generating process\ndgp = function(param){\n  \n  #### Parameters\n  c(n,      # Number of samples\n    sdl,    # Stdr. dev. of selection predictor\n    p,      # Treatment assigment probability\n    a0, a1, # Parameters of A-&gt;M relation\n    sdm,    # Stdr. dev. of mediator noise\n    g0, g1, # Parameters of A-&gt;S relation\n    g2,     # Parameter of M-&gt;S relation\n    g3,     # Parameter of L-&gt;S relation\n    sds,    # Stdr. dev. of selection noise\n    b0, b1, # Parameters of A-&gt;Y relation\n    b2,     # Parameter of M-&gt;Y relation\n    b3,     # Parameter of L-&gt;Y relation\n    sdy # Stdr. dev. of selection noise\n    ) %&lt;-% param     \n  \n  # Exogenous selection predictor\n  L = rnorm(n=n, mean=0, sd=sdl)\n  \n  # Treatment assigment\n  A = rbinom(n=n, size=1, prob=p)\n  \n  # Mediator\n  noise.M = rnorm(n=n, mean=0, sd=sdm)\n  M = a0 + a1*A + noise.M\n  \n  # Selection mechanism\n  noise.S = rnorm(n=n, mean=0, sd=sds)\n  S = ifelse(g0 + g1*A + g2*M + g3*L + noise.S &gt; 0, 1, 0)\n  \n  # Outcome\n  noise.Y = rnorm(n=n, mean=0, sd=sdy)\n  Y = b0 + b1*A + b2*M + b3*L + noise.Y\n  \n  # true ATE\n  true.ATE = b1 + b2*a1\n  \n  # Data\n  dat = data.frame(L,A,M,S,Y)\n  \n  # Return\n  return(list(dat,true.ATE))\n}"
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#results-under-no-exclusion-complete-data",
    "href": "posts/2023-03-01_ipw/index.html#results-under-no-exclusion-complete-data",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Results under no exclusion (complete data)",
    "text": "Results under no exclusion (complete data)\nFor a moment let us pretend we observe all variables for everyone. Let us examine the most interesting models that we can fit with the simulated data, to check their ability to recover population-level parameters under noisy samples.\n\n\nCode\n# Model for M, with complete data\nmod.M = lm(M ~ A, data=dat.1)\n\n# Model for S, with complete data\nmod.S = glm(S ~ A + L, family=binomial('probit'), data=dat.1)\n\n# Model for Y, with complete data\nmod.Y = lm(Y ~ A + M + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 1: Controlling for predictor of Y\nmod.ate.1 = lm(Y ~ A + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 2: Treatment is randomized, no controls needed\nmod.ate.2 = lm(Y ~ A, data=dat.1)\n\n# All models put together\nmodels.full = list(\"M\" = mod.M, \"S\" = mod.S, \"Y\" = mod.Y, \"ATE (O-set)\" = mod.ate.1, \"ATE (no adj)\" = mod.ate.2)\n\n# Summary of models\nmodelsummary(models.full, statistic = \"[{conf.low}, {conf.high}]\",\n             estimate  = \"{estimate}{stars}\") \n\n\n\n\n\n\nM\nS\nY\nATE (O-set)\nATE (no adj)\n\n\n\n\n(Intercept)\n0.104***\n−0.107+\n−1.579***\n−1.498***\n−1.524***\n\n\n\n[0.069, 0.139]\n[−0.219, 0.005]\n[−1.816, −1.342]\n[−1.732, −1.263]\n[−1.803, −1.245]\n\n\nA\n0.506***\n0.296***\n0.281\n0.677***\n0.742***\n\n\n\n[0.456, 0.556]\n[0.135, 0.457]\n[−0.114, 0.675]\n[0.340, 1.013]\n[0.342, 1.141]\n\n\nL\n\n0.427***\n1.755***\n1.768***\n\n\n\n\n\n[0.341, 0.514]\n[1.584, 1.925]\n[1.597, 1.940]\n\n\n\nM\n\n\n0.784***\n\n\n\n\n\n\n\n[0.369, 1.200]\n\n\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n1000\n\n\nR2\n0.283\n\n0.311\n0.301\n0.013\n\n\nR2 Adj.\n0.282\n\n0.308\n0.300\n0.012\n\n\nAIC\n1022.7\n1278.2\n4824.1\n4835.8\n5178.8\n\n\nBIC\n1037.5\n1292.9\n4848.7\n4855.5\n5193.5\n\n\nLog.Lik.\n−508.364\n−636.108\n−2407.072\n−2413.913\n−2586.403\n\n\nRMSE\n0.40\n0.47\n2.69\n2.70\n3.21\n\n\n\n\n\n\n\nSince the treatment is randomized, no control variables are required in the regression of \\(Y\\) against \\(A\\) to remove confounding bias. However, \\(L\\) is in the \\(O\\)-set of the effect; this is, controlling for \\(L\\) produces an asymptotically more efficient estimator. Such property is not seen in finite samples [see last two models].\nIt is no surprise that, with complete data, the point-estimate for the coefficient of \\(A\\) in the last two models lie close to the true ATE (0.75), even under a moderately noisy DGP (\\(R^2\\approx 0.3\\))."
  },
  {
    "objectID": "posts/2023-03-01_ipw/index.html#results-under-exclusion",
    "href": "posts/2023-03-01_ipw/index.html#results-under-exclusion",
    "title": "Recovering from selection bias with IPW methods",
    "section": "Results under exclusion",
    "text": "Results under exclusion\nIgnoring samples for which \\(S=0\\) reduces sample size from 1000 to 514. This is, the probability of exclusion is about 0.486.\n\n\nCode\n# Number of observation per selection\n# 1 = selected / observed\n# 0 = excluded\ntable(dat.1$S) %&gt;% kbl(col.names = c('S','N')) %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nS\nN\n\n\n\n\n0\n486\n\n\n1\n514\n\n\n\n\n\n\n\nIn contrast with the complete data case, a regression-based approach for estimating the ATE with only the selected samples does require controlling for \\(L\\), since conditioning on \\(S\\) opens the non-causal path:\n\n\\(A\\longrightarrow S\\longleftarrow L\\longrightarrow Y\\)\n\nUnfortunately, controlling for \\(L\\) alone does not remove selection bias, despite blocking such path. This is because the distribution of \\(L\\) in the sample does not necessarily match the distribution in the population.\nLet us inspect it.\n\nRegression-based approach:\nWe fit a model for the outcome \\(Y\\) against \\(A\\), controlling for \\(L\\), using only the selected samples:\n\n\nCode\n# Data for the selected sample\ndat.s = dat.1 %&gt;% filter(S==1)\nN.s = nrow(dat.s)\n\n# Model for causal effect, with selected\nmod.ate.s = lm(Y ~ A + L, data=dat.s)\nmodelsummary(list(\"ATE(S=1)\" = mod.ate.s), \n             statistic = NULL,\n             estimate  = \"{estimate}{stars} [{conf.low}, {conf.high}]\") \n\n\n\n\n\n\nATE(S=1)\n\n\n\n\n(Intercept)\n−1.341*** [−1.688, −0.995]\n\n\nA\n0.516* [0.058, 0.974]\n\n\nL\n1.843*** [1.601, 2.085]\n\n\nNum.Obs.\n514\n\n\nR2\n0.307\n\n\nR2 Adj.\n0.304\n\n\nAIC\n2458.1\n\n\nBIC\n2475.0\n\n\nLog.Lik.\n−1225.026\n\n\nRMSE\n2.62\n\n\n\n\n\n\n\nThe resulted 95% confidence interval for the coefficient of \\(A\\), using only samples for which \\(S=1\\), still covers the true ATE (0.75). Yet, a downwards bias shrinks the point-estimate and its lower bound noticeably. Using this, we would conclude the ATE is smaller than what truly is.\nLet us implement an IPW-based approach to compare against:\n\n\nIPW-based approach:\nGiven the SCM, and the assumption of \\(\\gamma_2=0\\), we can express:\n\n\\(w_i=\\mathbb{P}(S=1)/\\mathbb{P}(S=1\\mid A=a_i,L=l_i)\\)\n\\(v_i(a)=1/\\mathbb{P}(A=a) = 1/q\\) because treatment is randomized\n\nThe derived finite-sample estimator of the mean counterfactuals / potential outcomes is: \\[\n\\hat{Y}^a = N^{-1}\\sum_{i=1}^N\\frac{\\mathbb{I}(A_i=a)\\cdot\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\n\\]\nPutting all ingredients together we get point-estimates \\(\\hat{Y}^1=\\hat{\\mathbb{E}}[Y\\mid do(A=1)]\\) and \\(\\hat{Y}^0=\\hat{\\mathbb{E}}[Y\\mid do(A=0)]\\):\n\n\nCode\n# Compute the probability of selecton for all the units selected\n# We leverage the model mod.S, trained on complete data since S, A and L are\n# always observed (Y is the only one missing) and predict only on the selected units\nprob.s.unit = predict(mod.S, newdata=dat.s, type='response')\n\n# The unconditional probability of selection can be consistently estimated \n#... with counts from the total sample size and selected sample size\nprob.s = nrow(dat.s) / nrow(dat.1)\n\n# Since the treatment is randomized, the propensity score in the population is known at 0.5. \n# It can also be estimated from counts in the complete dataset\n# prop.score = sum(dat.1$A==1)/nrow(dat.1)\nprop.score = 0.5\n\n# Estimated counterfactuals/potential outcomes via IPPW\nest.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.Y = weights * Y) %&gt;% group_by(A) %&gt;% \n  summarise('PO'=sum(weighted.Y)/N.s)\n\n# Print resulted estimates\nhead(est.PO) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nA\nPO\n\n\n\n\n0\n-1.4318076\n\n\n1\n-0.7537995\n\n\n\n\n\n\n\nWhich, can be used to compute a point-estimate of the ATE: \\(\\hat{ATE}=\\hat{Y}^1-\\hat{Y}^0\\):\n\n\nCode\n# Print estimated ATE\nest.ATE = as.numeric(est.PO[2,2]-est.PO[1,2])\ndata.frame('IPW ATE'=est.ATE) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\n\n\n\n\n0.678008\n\n\n\n\n\n\n\nWe can see that the IPW-based approach produces a point-estimate closer to the true ATE. To get valid confidence intervals, however, a bootstrapping or asymptotic analysis need to be invoked. We can compute naïve confidence interval without resampling, using the fact:\n\\[\n\\hat{\\text{var}}(\\hat{\\text{ATE}}) \\geq \\hat{\\text{var}}(\\hat{Y}^1)+\\hat{\\text{var}}(\\hat{Y}^0)=(N-2)^{-2}\\sum_{a\\in\\{0,1\\}}\\sum_{i=1}^N \\mathbb{I}(A_i=a)\\cdot \\hat{v}_i(a)^2\\hat{w}_i^2(y_i-\\hat{Y}^a)^2\n\\]Such variance is naïve in the sense that it is overconfident due to ignoring the covariance between the potential outcomes, and the higher-order contributions of the weighting factors in the total variance. Anyway, using it will get us:\n\n\nCode\n# Estimated counterfactuals/potential outcomes via IPPW\nsd.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.var = weights^2 * ( A* (Y-est.PO[2,2])^2 + (1-A)* (Y-est.PO[1,2])^2)) %&gt;% \n  group_by(A) %&gt;% \n  summarise('sd'=sqrt(sum(weighted.var))/(N.s-2))\n\n# Naïve confidence interval\nnaivebounds = qnorm(0.975)*sum(sd.PO[,2])\ndata.frame('IPW ATE'=as.numeric(est.ATE),\n           'naïve LB'=as.numeric(est.ATE)-naivebounds,\n           'naïve UB'=as.numeric(est.ATE)+naivebounds) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\nnaïve.LB\nnaïve.UB\n\n\n\n\n0.678008\n0.5741569\n0.7818592\n\n\n\n\n\n\n\nAs noted, the naïve confidence interval is tighter than those produced by inference on the complete data. This means such confidence interval is not statistically valid, but it can still help us visualize the convergence of IPW estimator.\nNow, if we simulate and repeat the DGP and same analysis for different sample sizes, consistency would be visually perceived if we see:\n\nPoint-estimate converging to the true ATE\nConfidence intervals shrinking at a fast (**) rate\n\nLet us test it. We run 20 simulations with different complete sample sizes, from \\(N=500\\) to \\(N=23\\,000\\) [number of complete samples from \\(N_s=250\\) to \\(N_s=12\\,000\\)]. We repeat the procedure three times and average the results on those three repetitions. Such averages are presented in the following table:\n\n\nCode\n# Data frame to save results from iterations\nrounds.IPPW = data.frame(N=NA, Ns=NA, EST=NA, LB=NA, UB=NA)\n\n# All sample sizes to consider\nsamplesizes = round(500*exp(0.2*(0:19)))\n\n# Number of repetitions\nM = 3\n\n# Paramaters are the same as before\nparam.iter = param.1\n\n# Seed\nset.seed(66)\n\n# Loop\nfor(m in 1:M){\n  for(n in samplesizes){\n    \n    # Change sample size\n    param.iter[1] = n\n    \n    # Generate the data (complete and selected)\n    dat.iter = dgp(param.iter)[[1]]\n    dat.s.iter = dat.iter %&gt;% filter(S==1)\n    \n    # Estimated probability of selection\n    N.s.iter = nrow(dat.s.iter)\n    prob.s.iter = N.s.iter / n\n    \n    # Model for S, with complete data\n    mod.S.iter = glm(S ~ A + L, family=binomial('probit'), data=dat.iter)\n    prob.s.unit.iter = predict(mod.S.iter, newdata=dat.s.iter, type='response')\n  \n    # Propensity score model\n    prop.score.iter = sum(dat.iter$A==1)/nrow(dat.iter)\n    \n    # Put everything together\n    row.iter = dat.s.iter %&gt;% mutate(\n      prob.s.unit = as.numeric(prob.s.unit.iter),\n      prob.s = as.numeric(prob.s.iter),\n      prop.score = as.numeric(prop.score.iter),\n      pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n      weights = pro.weight * prob.s * (1/prob.s.unit),\n      weighted.Y = weights * Y) \n    \n    # Estimated counterfactuals/potential outcomes via IPPW\n    po.iter = row.iter %&gt;% group_by(A) %&gt;% \n      summarise('Y(A)'=sum(weighted.Y)/N.s.iter)\n    \n    # Estimated ATE\n    ATE.iter = as.numeric(po.iter[2,2]-po.iter[1,2])\n    \n    # Naïve variances\n    sd.PO.iter = row.iter %&gt;% mutate(\n      weighted.var = weights^2 * ( A* (Y-po.iter[2,2])^2 + (1-A)* (Y-po.iter[1,2])^2)) %&gt;% \n    group_by(A) %&gt;% \n    summarise('sd'=sqrt(sum(weighted.var))/(N.s.iter-1))\n    \n    # Naïve confidence interval\n    naivebounds = qnorm(0.975)*sum(sd.PO.iter[,2])\n    \n    rounds.IPPW = rbind.data.frame(rounds.IPPW,\n                                   data.frame(N=n,\n                                              Ns=N.s.iter,\n                                              EST=ATE.iter,\n                                              LB=ATE.iter-naivebounds,\n                                              UB=ATE.iter+naivebounds))\n  }\n}\n# Print resulted estimates\nrounds.IPPW = rounds.IPPW[-1,] %&gt;%\n  group_by(N) %&gt;%\n  summarise_all(mean)\n\nhead(rounds.IPPW) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nN\nNs\nEST\nLB\nUB\n\n\n\n\n500\n249.6667\n0.5919352\n-0.1461034\n1.3299737\n\n\n611\n309.0000\n0.7701710\n0.1782066\n1.3621354\n\n\n746\n379.6667\n0.8962619\n0.2991885\n1.4933353\n\n\n911\n453.6667\n0.7438565\n0.5811669\n0.9065460\n\n\n1113\n556.0000\n0.7817429\n0.3579693\n1.2055165\n\n\n1359\n679.0000\n0.6411555\n0.3223225\n0.9599884\n\n\n\n\n\n\n\nAn the following plot:\n\n\nCode\n### Plot results from rounds\nrounds.IPPW[,-1] %&gt;% melt(id.vars = 'Ns') %&gt;% \n  mutate(type = ifelse(variable=='EST','EST','N. C.I.') ) %&gt;% \n  ggplot(aes(x=Ns,y=value,group=variable)) + \n  geom_point(aes(shape=type,color=type)) + \n  geom_smooth(method = lm, formula = y ~ x + I(sqrt(x)), se = FALSE) + ## O(N^0.5) convergence\n  labs(y='Value of estimate / bound', x='number of complete samples') +\n  geom_hline(yintercept=T.ATE, col = 'red') +\n  theme_bw()\n\n\n\n\n\nWe can appreciate that the estimator converges to the true ATE [in red], and its uncertainty reduce at a sustained rate; there is convergence in probability. In other words, the IPW-based estimator is consistent.\n\nMathematical justification of consistency\nWe can prove consistency mathematically for this SCM. However, to avoid measure-theoretic conundrums, let us consider the case for which all variables are discrete. Results are generalizable for mixed discrete-continuous cases with positive distributions (no zero-measure events).\nLet us assume \\(Y\\) has support on \\(\\{y_{(c)}\\}_{c=1}^C\\), and \\(L\\) has support on \\(\\{l_{(k)}\\}_{k=1}^K\\), then:\n\\[\n\\begin{aligned}\n\\hat{Y}^a &= N^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\\cdot \\mathbb{I}(A_i=a) \\\\\n&=\nN^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a\\mid L=l_i)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a,L=l_i)}\\cdot y_i\\cdot\\mathbb{I}(A_i=a,S_i=1)\\\\\n&=\n\\sum_{i=1}^N\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_i)\n}\\cdot\n\\frac{\ny_i\\cdot\\mathbb{I}(A_i=a,S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_i,S=1)\n}\\\\\n&= \\sum_{i=1}^N\\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{i}\\cdot\\mathbb{I}(A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\sum_{i=1}^N\\mathbb{I}(Y_i=y_{(c)},A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\hat{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\n\\end{aligned}\n\\]\nAssuming the propensity scores and the probability of selection are both correctly specified, all finite-sample approximations \\(\\hat{\\mathbb{P}}\\) converge in the limit to the true distributions \\(\\mathbb{P}\\). Then:\n\\[\n\\begin{aligned}\n\\text{plim}_{N\\rightarrow\\infty}\n\\hat{Y}^a\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(S=1)}{\n{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(L=l_{(k)})}{\n{\\mathbb{P}}(L=l_{(k)}\\mid S=1)\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n{\\mathbb{P}}(L=l_{(k)})\n\\cdot\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)} S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\sum_{c}\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)}, S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\mathbb{E}(Y\\mid A=a,L, S=1)\\\\\n&=\n\\mathbb{E}_L\n\\mathbb{E}(Y\\mid A=a,L, S=1) =\\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L,S=1]\\\\\n&= \\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L] = \\mathbb{E}[Y\\mid do(A=a)]\n\\end{aligned}\n\\]\nThis is, the IPW estimator converges to the true counterfactual mean given by intervention \\(do(A=a)\\). The last two equalities come from these facts:\n\n\\(L\\) blocks all non-causal paths from \\(A\\) to \\(Y\\) when conditioning on \\(S=1\\)\n\\(Y\\perp S\\,\\mid L\\) in the back-door graph: the resulting DAG after removing all arrows coming out of \\(A\\)\n\\(\\mathbb{P}(L=l_{(k)})\\) is the population-distribution of \\(L\\)\n\n\n\n\nRevisiting the regression approach: generalized adjustment criteria\nNotice that, in the convergence proof for the IPW estimator, it was shown that the underlying estimand is algebraically equivalent to a regression-adjusted mean of \\(Y\\), averaged over the population distribution of \\(L\\).\n\\[\n\\mathbb{E}[Y\\mid do(A=a)] = \\mathbb{E}_L\\mathbb{E}(Y\\mid A=a,L,S=1)\n\\]\nExtensions of this results are covered by three generalized adjustment criteria (Correa, Tian, and Bareinboim 2018). This is, for this case, a mathematically equivalent result in term of consistency can be achieved via regression adjustment."
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html",
    "href": "posts/2023-06-01_missingOutcome/index.html",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "",
    "text": "Consider the following \\(m\\)-graph representing the causal relations among a set of random variables:\nConfounding bias and selection bias are together the most prevalent hurdles to the validity and generalizability of causal inference results. In general, both arise from uncontrolled extraneous flows of statistical information between treatment and outcome in the analysis. Their precise characterization in the Structural Causal Models framework (SCM), and potential corrections, differ in nature:"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#simple-simulation-setting",
    "href": "posts/2023-06-01_missingOutcome/index.html#simple-simulation-setting",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Simple simulation setting",
    "text": "Simple simulation setting\nLet us consider the SCM given by the following DAG and set of structural equations:"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#the-dag",
    "href": "posts/2023-06-01_missingOutcome/index.html#the-dag",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The DAG:",
    "text": "The DAG:\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  M ~ A,\n  S ~ A + L,\n  Y ~ A + M + L\n) %&gt;% tidy_dagitty(layout = \"nicely\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#structural-equations",
    "href": "posts/2023-06-01_missingOutcome/index.html#structural-equations",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Structural equations:",
    "text": "Structural equations:\n\\[\n\\begin{aligned}[c]\nL &\\sim\\text{Nor}(0,\\sigma^2_L) & &\\\\\nA &\\sim\\text{Ber}(q) & &\\\\\nM &= \\alpha_0 + \\alpha_1A + u_M & u_M &\\sim\\text{Nor}(0,\\sigma^2_M) \\\\\nS &= \\mathbb{I}[\\gamma_0 + \\gamma_1A + \\gamma_2M + \\gamma_3L + u_S &gt; 0] & u_S &\\sim\\text{Nor}(0,\\sigma^2_S) \\\\\nY &= \\beta_0 + \\beta_1A + \\beta_2M + \\beta_3L + u_Y & u_Y &\\sim\\text{Nor}(0,\\sigma^2_Y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#selection-mechanism",
    "href": "posts/2023-06-01_missingOutcome/index.html#selection-mechanism",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Selection mechanism:",
    "text": "Selection mechanism:\nWhen \\(S=1\\) for a particular unit, we get to observe their whole data \\((L,A,M,Y)\\). When \\(S=1\\), only the final outcome \\(Y\\) is missing, so we get to observe \\((L,A,M)\\).\nLet us put this SCM as a data-generating process (DGP) in R code:\n\n\nCode\n# Libraries needed\nlibrary(DescTools)\nlibrary(LaplacesDemon)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(zeallot)\nlibrary(reshape2)\n\n# Data generating process\ndgp = function(param){\n  \n  #### Parameters\n  c(n,      # Number of samples\n    sdl,    # Stdr. dev. of selection predictor\n    p,      # Treatment assigment probability\n    a0, a1, # Parameters of A-&gt;M relation\n    sdm,    # Stdr. dev. of mediator noise\n    g0, g1, # Parameters of A-&gt;S relation\n    g2,     # Parameter of M-&gt;S relation\n    g3,     # Parameter of L-&gt;S relation\n    sds,    # Stdr. dev. of selection noise\n    b0, b1, # Parameters of A-&gt;Y relation\n    b2,     # Parameter of M-&gt;Y relation\n    b3,     # Parameter of L-&gt;Y relation\n    sdy # Stdr. dev. of selection noise\n    ) %&lt;-% param     \n  \n  # Exogenous selection predictor\n  L = rnorm(n=n, mean=0, sd=sdl)\n  \n  # Treatment assigment\n  A = rbinom(n=n, size=1, prob=p)\n  \n  # Mediator\n  noise.M = rnorm(n=n, mean=0, sd=sdm)\n  M = a0 + a1*A + noise.M\n  \n  # Selection mechanism\n  noise.S = rnorm(n=n, mean=0, sd=sds)\n  S = ifelse(g0 + g1*A + g2*M + g3*L + noise.S &gt; 0, 1, 0)\n  \n  # Outcome\n  noise.Y = rnorm(n=n, mean=0, sd=sdy)\n  Y = b0 + b1*A + b2*M + b3*L + noise.Y\n  \n  # true ATE\n  true.ATE = b1 + b2*a1\n  \n  # Data\n  dat = data.frame(L,A,M,S,Y)\n  \n  # Return\n  return(list(dat,true.ATE))\n}"
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "href": "posts/2023-06-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under no exclusion (complete data)",
    "text": "Results under no exclusion (complete data)\nFor a moment let us pretend we observe all variables for everyone. Let us examine the most interesting models that we can fit with the simulated data, to check their ability to recover population-level parameters under noisy samples.\n\n\nCode\n# Model for M, with complete data\nmod.M = lm(M ~ A, data=dat.1)\n\n# Model for S, with complete data\nmod.S = glm(S ~ A + L, family=binomial('probit'), data=dat.1)\n\n# Model for Y, with complete data\nmod.Y = lm(Y ~ A + M + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 1: Controlling for predictor of Y\nmod.ate.1 = lm(Y ~ A + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 2: Treatment is randomized, no controls needed\nmod.ate.2 = lm(Y ~ A, data=dat.1)\n\n# All models put together\nmodels.full = list(\"M\" = mod.M, \"S\" = mod.S, \"Y\" = mod.Y, \"ATE (O-set)\" = mod.ate.1, \"ATE (no adj)\" = mod.ate.2)\n\n# Summary of models\nmodelsummary(models.full, statistic = \"[{conf.low}, {conf.high}]\",\n             estimate  = \"{estimate}{stars}\") \n\n\n\n\n\n\nM\nS\nY\nATE (O-set)\nATE (no adj)\n\n\n\n\n(Intercept)\n0.104***\n−0.107+\n−1.579***\n−1.498***\n−1.524***\n\n\n\n[0.069, 0.139]\n[−0.219, 0.005]\n[−1.816, −1.342]\n[−1.732, −1.263]\n[−1.803, −1.245]\n\n\nA\n0.506***\n0.296***\n0.281\n0.677***\n0.742***\n\n\n\n[0.456, 0.556]\n[0.135, 0.457]\n[−0.114, 0.675]\n[0.340, 1.013]\n[0.342, 1.141]\n\n\nL\n\n0.427***\n1.755***\n1.768***\n\n\n\n\n\n[0.341, 0.514]\n[1.584, 1.925]\n[1.597, 1.940]\n\n\n\nM\n\n\n0.784***\n\n\n\n\n\n\n\n[0.369, 1.200]\n\n\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n1000\n\n\nR2\n0.283\n\n0.311\n0.301\n0.013\n\n\nR2 Adj.\n0.282\n\n0.308\n0.300\n0.012\n\n\nAIC\n1022.7\n1278.2\n4824.1\n4835.8\n5178.8\n\n\nBIC\n1037.5\n1292.9\n4848.7\n4855.5\n5193.5\n\n\nLog.Lik.\n−508.364\n−636.108\n−2407.072\n−2413.913\n−2586.403\n\n\nRMSE\n0.40\n0.47\n2.69\n2.70\n3.21\n\n\n\n\n\n\n\nSince the treatment is randomized, no control variables are required in the regression of \\(Y\\) against \\(A\\) to remove confounding bias. However, \\(L\\) is in the \\(O\\)-set of the effect; this is, controlling for \\(L\\) produces an asymptotically more efficient estimator. Such property is not seen in finite samples [see last two models].\nIt is no surprise that, with complete data, the point-estimate for the coefficient of \\(A\\) in the last two models lie close to the true ATE (0.75), even under a moderately noisy DGP (\\(R^2\\approx 0.3\\))."
  },
  {
    "objectID": "posts/2023-06-01_missingOutcome/index.html#results-under-exclusion",
    "href": "posts/2023-06-01_missingOutcome/index.html#results-under-exclusion",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under exclusion",
    "text": "Results under exclusion\nIgnoring samples for which \\(S=0\\) reduces sample size from 1000 to 514. This is, the probability of exclusion is about 0.486.\n\n\nCode\n# Number of observation per selection\n# 1 = selected / observed\n# 0 = excluded\ntable(dat.1$S) %&gt;% kbl(col.names = c('S','N')) %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nS\nN\n\n\n\n\n0\n486\n\n\n1\n514\n\n\n\n\n\n\n\nIn contrast with the complete data case, a regression-based approach for estimating the ATE with only the selected samples does require controlling for \\(L\\), since conditioning on \\(S\\) opens the non-causal path:\n\n\\(A\\longrightarrow S\\longleftarrow L\\longrightarrow Y\\)\n\nUnfortunately, controlling for \\(L\\) alone does not remove selection bias, despite blocking such path. This is because the distribution of \\(L\\) in the sample does not necessarily match the distribution in the population.\nLet us inspect it.\n\nRegression-based approach:\nWe fit a model for the outcome \\(Y\\) against \\(A\\), controlling for \\(L\\), using only the selected samples:\n\n\nCode\n# Data for the selected sample\ndat.s = dat.1 %&gt;% filter(S==1)\nN.s = nrow(dat.s)\n\n# Model for causal effect, with selected\nmod.ate.s = lm(Y ~ A + L, data=dat.s)\nmodelsummary(list(\"ATE(S=1)\" = mod.ate.s), \n             statistic = NULL,\n             estimate  = \"{estimate}{stars} [{conf.low}, {conf.high}]\") \n\n\n\n\n\n\nATE(S=1)\n\n\n\n\n(Intercept)\n−1.341*** [−1.688, −0.995]\n\n\nA\n0.516* [0.058, 0.974]\n\n\nL\n1.843*** [1.601, 2.085]\n\n\nNum.Obs.\n514\n\n\nR2\n0.307\n\n\nR2 Adj.\n0.304\n\n\nAIC\n2458.1\n\n\nBIC\n2475.0\n\n\nLog.Lik.\n−1225.026\n\n\nRMSE\n2.62\n\n\n\n\n\n\n\nThe resulted 95% confidence interval for the coefficient of \\(A\\), using only samples for which \\(S=1\\), still covers the true ATE (0.75). Yet, a downwards bias shrinks the point-estimate and its lower bound noticeably. Using this, we would conclude the ATE is smaller than what truly is.\nLet us implement an IPW-based approach to compare against:\n\n\nIPW-based approach:\nGiven the SCM, and the assumption of \\(\\gamma_2=0\\), we can express:\n\n\\(w_i=\\mathbb{P}(S=1)/\\mathbb{P}(S=1\\mid A=a_i,L=l_i)\\)\n\\(v_i(a)=1/\\mathbb{P}(A=a) = 1/q\\) because treatment is randomized\n\nThe derived finite-sample estimator of the mean counterfactuals / potential outcomes is: \\[\n\\hat{Y}^a = N^{-1}\\sum_{i=1}^N\\frac{\\mathbb{I}(A_i=a)\\cdot\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\n\\]\nPutting all ingredients together we get point-estimates \\(\\hat{Y}^1=\\hat{\\mathbb{E}}[Y\\mid do(A=1)]\\) and \\(\\hat{Y}^0=\\hat{\\mathbb{E}}[Y\\mid do(A=0)]\\):\n\n\nCode\n# Compute the probability of selecton for all the units selected\n# We leverage the model mod.S, trained on complete data since S, A and L are\n# always observed (Y is the only one missing) and predict only on the selected units\nprob.s.unit = predict(mod.S, newdata=dat.s, type='response')\n\n# The unconditional probability of selection can be consistently estimated \n#... with counts from the total sample size and selected sample size\nprob.s = nrow(dat.s) / nrow(dat.1)\n\n# Since the treatment is randomized, the propensity score in the population is known at 0.5. \n# It can also be estimated from counts in the complete dataset\n# prop.score = sum(dat.1$A==1)/nrow(dat.1)\nprop.score = 0.5\n\n# Estimated counterfactuals/potential outcomes via IPPW\nest.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.Y = weights * Y) %&gt;% group_by(A) %&gt;% \n  summarise('PO'=sum(weighted.Y)/N.s)\n\n# Print resulted estimates\nhead(est.PO) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nA\nPO\n\n\n\n\n0\n-1.4318076\n\n\n1\n-0.7537995\n\n\n\n\n\n\n\nWhich, can be used to compute a point-estimate of the ATE: \\(\\hat{ATE}=\\hat{Y}^1-\\hat{Y}^0\\):\n\n\nCode\n# Print estimated ATE\nest.ATE = as.numeric(est.PO[2,2]-est.PO[1,2])\ndata.frame('IPW ATE'=est.ATE) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\n\n\n\n\n0.678008\n\n\n\n\n\n\n\nWe can see that the IPW-based approach produces a point-estimate closer to the true ATE. To get valid confidence intervals, however, a bootstrapping or asymptotic analysis need to be invoked. We can compute naïve confidence interval without resampling, using the fact:\n\\[\n\\hat{\\text{var}}(\\hat{\\text{ATE}}) \\geq \\hat{\\text{var}}(\\hat{Y}^1)+\\hat{\\text{var}}(\\hat{Y}^0)=(N-2)^{-2}\\sum_{a\\in\\{0,1\\}}\\sum_{i=1}^N \\mathbb{I}(A_i=a)\\cdot \\hat{v}_i(a)^2\\hat{w}_i^2(y_i-\\hat{Y}^a)^2\n\\]Such variance is naïve in the sense that it is overconfident due to ignoring the covariance between the potential outcomes, and the higher-order contributions of the weighting factors in the total variance. Anyway, using it will get us:\n\n\nCode\n# Estimated counterfactuals/potential outcomes via IPPW\nsd.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.var = weights^2 * ( A* (Y-est.PO[2,2])^2 + (1-A)* (Y-est.PO[1,2])^2)) %&gt;% \n  group_by(A) %&gt;% \n  summarise('sd'=sqrt(sum(weighted.var))/(N.s-2))\n\n# Naïve confidence interval\nnaivebounds = qnorm(0.975)*sum(sd.PO[,2])\ndata.frame('IPW ATE'=as.numeric(est.ATE),\n           'naïve LB'=as.numeric(est.ATE)-naivebounds,\n           'naïve UB'=as.numeric(est.ATE)+naivebounds) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\nnaïve.LB\nnaïve.UB\n\n\n\n\n0.678008\n0.5741569\n0.7818592\n\n\n\n\n\n\n\nAs noted, the naïve confidence interval is tighter than those produced by inference on the complete data. This means such confidence interval is not statistically valid, but it can still help us visualize the convergence of IPW estimator.\nNow, if we simulate and repeat the DGP and same analysis for different sample sizes, consistency would be visually perceived if we see:\n\nPoint-estimate converging to the true ATE\nConfidence intervals shrinking at a fast (**) rate\n\nLet us test it. We run 20 simulations with different complete sample sizes, from \\(N=500\\) to \\(N=23\\,000\\) [number of complete samples from \\(N_s=250\\) to \\(N_s=12\\,000\\)]. We repeat the procedure three times and average the results on those three repetitions. Such averages are presented in the following table:\n\n\nCode\n# Data frame to save results from iterations\nrounds.IPPW = data.frame(N=NA, Ns=NA, EST=NA, LB=NA, UB=NA)\n\n# All sample sizes to consider\nsamplesizes = round(500*exp(0.2*(0:19)))\n\n# Number of repetitions\nM = 3\n\n# Paramaters are the same as before\nparam.iter = param.1\n\n# Seed\nset.seed(66)\n\n# Loop\nfor(m in 1:M){\n  for(n in samplesizes){\n    \n    # Change sample size\n    param.iter[1] = n\n    \n    # Generate the data (complete and selected)\n    dat.iter = dgp(param.iter)[[1]]\n    dat.s.iter = dat.iter %&gt;% filter(S==1)\n    \n    # Estimated probability of selection\n    N.s.iter = nrow(dat.s.iter)\n    prob.s.iter = N.s.iter / n\n    \n    # Model for S, with complete data\n    mod.S.iter = glm(S ~ A + L, family=binomial('probit'), data=dat.iter)\n    prob.s.unit.iter = predict(mod.S.iter, newdata=dat.s.iter, type='response')\n  \n    # Propensity score model\n    prop.score.iter = sum(dat.iter$A==1)/nrow(dat.iter)\n    \n    # Put everything together\n    row.iter = dat.s.iter %&gt;% mutate(\n      prob.s.unit = as.numeric(prob.s.unit.iter),\n      prob.s = as.numeric(prob.s.iter),\n      prop.score = as.numeric(prop.score.iter),\n      pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n      weights = pro.weight * prob.s * (1/prob.s.unit),\n      weighted.Y = weights * Y) \n    \n    # Estimated counterfactuals/potential outcomes via IPPW\n    po.iter = row.iter %&gt;% group_by(A) %&gt;% \n      summarise('Y(A)'=sum(weighted.Y)/N.s.iter)\n    \n    # Estimated ATE\n    ATE.iter = as.numeric(po.iter[2,2]-po.iter[1,2])\n    \n    # Naïve variances\n    sd.PO.iter = row.iter %&gt;% mutate(\n      weighted.var = weights^2 * ( A* (Y-po.iter[2,2])^2 + (1-A)* (Y-po.iter[1,2])^2)) %&gt;% \n    group_by(A) %&gt;% \n    summarise('sd'=sqrt(sum(weighted.var))/(N.s.iter-1))\n    \n    # Naïve confidence interval\n    naivebounds = qnorm(0.975)*sum(sd.PO.iter[,2])\n    \n    rounds.IPPW = rbind.data.frame(rounds.IPPW,\n                                   data.frame(N=n,\n                                              Ns=N.s.iter,\n                                              EST=ATE.iter,\n                                              LB=ATE.iter-naivebounds,\n                                              UB=ATE.iter+naivebounds))\n  }\n}\n# Print resulted estimates\nrounds.IPPW = rounds.IPPW[-1,] %&gt;%\n  group_by(N) %&gt;%\n  summarise_all(mean)\n\nhead(rounds.IPPW) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nN\nNs\nEST\nLB\nUB\n\n\n\n\n500\n249.6667\n0.5919352\n-0.1461034\n1.3299737\n\n\n611\n309.0000\n0.7701710\n0.1782066\n1.3621354\n\n\n746\n379.6667\n0.8962619\n0.2991885\n1.4933353\n\n\n911\n453.6667\n0.7438565\n0.5811669\n0.9065460\n\n\n1113\n556.0000\n0.7817429\n0.3579693\n1.2055165\n\n\n1359\n679.0000\n0.6411555\n0.3223225\n0.9599884\n\n\n\n\n\n\n\nAn the following plot:\n\n\nCode\n### Plot results from rounds\nrounds.IPPW[,-1] %&gt;% melt(id.vars = 'Ns') %&gt;% \n  mutate(type = ifelse(variable=='EST','EST','N. C.I.') ) %&gt;% \n  ggplot(aes(x=Ns,y=value,group=variable)) + \n  geom_point(aes(shape=type,color=type)) + \n  geom_smooth(method = lm, formula = y ~ x + I(sqrt(x)), se = FALSE) + ## O(N^0.5) convergence\n  labs(y='Value of estimate / bound', x='number of complete samples') +\n  geom_hline(yintercept=T.ATE, col = 'red') +\n  theme_bw()\n\n\n\n\n\nWe can appreciate that the estimator converges to the true ATE [in red], and its uncertainty reduce at a sustained rate; there is convergence in probability. In other words, the IPW-based estimator is consistent.\n\nMathematical justification of consistency\nWe can prove consistency mathematically for this SCM. However, to avoid measure-theoretic conundrums, let us consider the case for which all variables are discrete. Results are generalizable for mixed discrete-continuous cases with positive distributions (no zero-measure events).\nLet us assume \\(Y\\) has support on \\(\\{y_{(c)}\\}_{c=1}^C\\), and \\(L\\) has support on \\(\\{l_{(k)}\\}_{k=1}^K\\), then:\n\\[\n\\begin{aligned}\n\\hat{Y}^a &= N^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\\cdot \\mathbb{I}(A_i=a) \\\\\n&=\nN^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a\\mid L=l_i)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a,L=l_i)}\\cdot y_i\\cdot\\mathbb{I}(A_i=a,S_i=1)\\\\\n&=\n\\sum_{i=1}^N\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_i)\n}\\cdot\n\\frac{\ny_i\\cdot\\mathbb{I}(A_i=a,S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_i,S=1)\n}\\\\\n&= \\sum_{i=1}^N\\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{i}\\cdot\\mathbb{I}(A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\sum_{i=1}^N\\mathbb{I}(Y_i=y_{(c)},A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\hat{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\n\\end{aligned}\n\\]\nAssuming the propensity scores and the probability of selection are both correctly specified, all finite-sample approximations \\(\\hat{\\mathbb{P}}\\) converge in the limit to the true distributions \\(\\mathbb{P}\\). Then:\n\\[\n\\begin{aligned}\n\\text{plim}_{N\\rightarrow\\infty}\n\\hat{Y}^a\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(S=1)}{\n{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(L=l_{(k)})}{\n{\\mathbb{P}}(L=l_{(k)}\\mid S=1)\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n{\\mathbb{P}}(L=l_{(k)})\n\\cdot\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)} S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\sum_{c}\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)}, S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\mathbb{E}(Y\\mid A=a,L, S=1)\\\\\n&=\n\\mathbb{E}_L\n\\mathbb{E}(Y\\mid A=a,L, S=1) =\\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L,S=1]\\\\\n&= \\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L] = \\mathbb{E}[Y\\mid do(A=a)]\n\\end{aligned}\n\\]\nThis is, the IPW estimator converges to the true counterfactual mean given by intervention \\(do(A=a)\\). The last two equalities come from these facts:\n\n\\(L\\) blocks all non-causal paths from \\(A\\) to \\(Y\\) when conditioning on \\(S=1\\)\n\\(Y\\perp S\\,\\mid L\\) in the back-door graph: the resulting DAG after removing all arrows coming out of \\(A\\)\n\\(\\mathbb{P}(L=l_{(k)})\\) is the population-distribution of \\(L\\)\n\n\n\n\nRevisiting the regression approach: generalized adjustment criteria\nNotice that, in the convergence proof for the IPW estimator, it was shown that the underlying estimand is algebraically equivalent to a regression-adjusted mean of \\(Y\\), averaged over the population distribution of \\(L\\).\n\\[\n\\mathbb{E}[Y\\mid do(A=a)] = \\mathbb{E}_L\\mathbb{E}(Y\\mid A=a,L,S=1)\n\\]\nExtensions of this results are covered by three generalized adjustment criteria (Correa, Tian, and Bareinboim 2018). This is, for this case, a mathematically equivalent result in term of consistency can be achieved via regression adjustment."
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html",
    "href": "posts/2023-08-01_missingOutcome/index.html",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "",
    "text": "Consider the following \\(m\\)-graph, \\(\\mathcal{G}\\), representing the causal relations among a set of random variables \\(\\mathcal{V}=\\{H,A,M,Y,R_Y\\}\\), where:\nOur goal is to estimate the average treatment effect (ATE), \\(\\psi\\), in the target population, defined as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}[Y\\mid do(A=a)] := \\mathbb{E}[Y\\mid do(A=1)]-\\mathbb{E}[Y\\mid do(A=0)]\n\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#simple-simulation-setting",
    "href": "posts/2023-08-01_missingOutcome/index.html#simple-simulation-setting",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Simple simulation setting",
    "text": "Simple simulation setting\nLet us consider the SCM given by the following DAG and set of structural equations:"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#the-dag",
    "href": "posts/2023-08-01_missingOutcome/index.html#the-dag",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The DAG:",
    "text": "The DAG:\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  M ~ A,\n  S ~ A + L,\n  Y ~ A + M + L\n) %&gt;% tidy_dagitty(layout = \"nicely\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#structural-equations",
    "href": "posts/2023-08-01_missingOutcome/index.html#structural-equations",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Structural equations:",
    "text": "Structural equations:\n\\[\n\\begin{aligned}[c]\nL &\\sim\\text{Nor}(0,\\sigma^2_L) & &\\\\\nA &\\sim\\text{Ber}(q) & &\\\\\nM &= \\alpha_0 + \\alpha_1A + u_M & u_M &\\sim\\text{Nor}(0,\\sigma^2_M) \\\\\nS &= \\mathbb{I}[\\gamma_0 + \\gamma_1A + \\gamma_2M + \\gamma_3L + u_S &gt; 0] & u_S &\\sim\\text{Nor}(0,\\sigma^2_S) \\\\\nY &= \\beta_0 + \\beta_1A + \\beta_2M + \\beta_3L + u_Y & u_Y &\\sim\\text{Nor}(0,\\sigma^2_Y)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#selection-mechanism",
    "href": "posts/2023-08-01_missingOutcome/index.html#selection-mechanism",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Selection mechanism:",
    "text": "Selection mechanism:\nWhen \\(S=1\\) for a particular unit, we get to observe their whole data \\((L,A,M,Y)\\). When \\(S=1\\), only the final outcome \\(Y\\) is missing, so we get to observe \\((L,A,M)\\).\nLet us put this SCM as a data-generating process (DGP) in R code:\n\n\nCode\n# Libraries needed\nlibrary(DescTools)\nlibrary(LaplacesDemon)\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(modelsummary)\nlibrary(gt)\nlibrary(zeallot)\nlibrary(reshape2)\n\n# Data generating process\ndgp = function(param){\n  \n  #### Parameters\n  c(n,      # Number of samples\n    sdl,    # Stdr. dev. of selection predictor\n    p,      # Treatment assigment probability\n    a0, a1, # Parameters of A-&gt;M relation\n    sdm,    # Stdr. dev. of mediator noise\n    g0, g1, # Parameters of A-&gt;S relation\n    g2,     # Parameter of M-&gt;S relation\n    g3,     # Parameter of L-&gt;S relation\n    sds,    # Stdr. dev. of selection noise\n    b0, b1, # Parameters of A-&gt;Y relation\n    b2,     # Parameter of M-&gt;Y relation\n    b3,     # Parameter of L-&gt;Y relation\n    sdy # Stdr. dev. of selection noise\n    ) %&lt;-% param     \n  \n  # Exogenous selection predictor\n  L = rnorm(n=n, mean=0, sd=sdl)\n  \n  # Treatment assigment\n  A = rbinom(n=n, size=1, prob=p)\n  \n  # Mediator\n  noise.M = rnorm(n=n, mean=0, sd=sdm)\n  M = a0 + a1*A + noise.M\n  \n  # Selection mechanism\n  noise.S = rnorm(n=n, mean=0, sd=sds)\n  S = ifelse(g0 + g1*A + g2*M + g3*L + noise.S &gt; 0, 1, 0)\n  \n  # Outcome\n  noise.Y = rnorm(n=n, mean=0, sd=sdy)\n  Y = b0 + b1*A + b2*M + b3*L + noise.Y\n  \n  # true ATE\n  true.ATE = b1 + b2*a1\n  \n  # Data\n  dat = data.frame(L,A,M,S,Y)\n  \n  # Return\n  return(list(dat,true.ATE))\n}"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "href": "posts/2023-08-01_missingOutcome/index.html#results-under-no-exclusion-complete-data",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under no exclusion (complete data)",
    "text": "Results under no exclusion (complete data)\nFor a moment let us pretend we observe all variables for everyone. Let us examine the most interesting models that we can fit with the simulated data, to check their ability to recover population-level parameters under noisy samples.\n\n\nCode\n# Model for M, with complete data\nmod.M = lm(M ~ A, data=dat.1)\n\n# Model for S, with complete data\nmod.S = glm(S ~ A + L, family=binomial('probit'), data=dat.1)\n\n# Model for Y, with complete data\nmod.Y = lm(Y ~ A + M + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 1: Controlling for predictor of Y\nmod.ate.1 = lm(Y ~ A + L, data=dat.1)\n\n# Model for causal effect, with complete data\n# Version 2: Treatment is randomized, no controls needed\nmod.ate.2 = lm(Y ~ A, data=dat.1)\n\n# All models put together\nmodels.full = list(\"M\" = mod.M, \"S\" = mod.S, \"Y\" = mod.Y, \"ATE (O-set)\" = mod.ate.1, \"ATE (no adj)\" = mod.ate.2)\n\n# Summary of models\nmodelsummary(models.full, statistic = \"[{conf.low}, {conf.high}]\",\n             estimate  = \"{estimate}{stars}\") \n\n\n\n\n\n\nM\nS\nY\nATE (O-set)\nATE (no adj)\n\n\n\n\n(Intercept)\n0.104***\n−0.107+\n−1.579***\n−1.498***\n−1.524***\n\n\n\n[0.069, 0.139]\n[−0.219, 0.005]\n[−1.816, −1.342]\n[−1.732, −1.263]\n[−1.803, −1.245]\n\n\nA\n0.506***\n0.296***\n0.281\n0.677***\n0.742***\n\n\n\n[0.456, 0.556]\n[0.135, 0.457]\n[−0.114, 0.675]\n[0.340, 1.013]\n[0.342, 1.141]\n\n\nL\n\n0.427***\n1.755***\n1.768***\n\n\n\n\n\n[0.341, 0.514]\n[1.584, 1.925]\n[1.597, 1.940]\n\n\n\nM\n\n\n0.784***\n\n\n\n\n\n\n\n[0.369, 1.200]\n\n\n\n\nNum.Obs.\n1000\n1000\n1000\n1000\n1000\n\n\nR2\n0.283\n\n0.311\n0.301\n0.013\n\n\nR2 Adj.\n0.282\n\n0.308\n0.300\n0.012\n\n\nAIC\n1022.7\n1278.2\n4824.1\n4835.8\n5178.8\n\n\nBIC\n1037.5\n1292.9\n4848.7\n4855.5\n5193.5\n\n\nLog.Lik.\n−508.364\n−636.108\n−2407.072\n−2413.913\n−2586.403\n\n\nRMSE\n0.40\n0.47\n2.69\n2.70\n3.21\n\n\n\n\n\n\n\nSince the treatment is randomized, no control variables are required in the regression of \\(Y\\) against \\(A\\) to remove confounding bias. However, \\(L\\) is in the \\(O\\)-set of the effect; this is, controlling for \\(L\\) produces an asymptotically more efficient estimator. Such property is not seen in finite samples [see last two models].\nIt is no surprise that, with complete data, the point-estimate for the coefficient of \\(A\\) in the last two models lie close to the true ATE (0.75), even under a moderately noisy DGP (\\(R^2\\approx 0.3\\))."
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#results-under-exclusion",
    "href": "posts/2023-08-01_missingOutcome/index.html#results-under-exclusion",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Results under exclusion",
    "text": "Results under exclusion\nIgnoring samples for which \\(S=0\\) reduces sample size from 1000 to 514. This is, the probability of exclusion is about 0.486.\n\n\nCode\n# Number of observation per selection\n# 1 = selected / observed\n# 0 = excluded\ntable(dat.1$S) %&gt;% kbl(col.names = c('S','N')) %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nS\nN\n\n\n\n\n0\n486\n\n\n1\n514\n\n\n\n\n\n\n\nIn contrast with the complete data case, a regression-based approach for estimating the ATE with only the selected samples does require controlling for \\(L\\), since conditioning on \\(S\\) opens the non-causal path:\n\n\\(A\\longrightarrow S\\longleftarrow L\\longrightarrow Y\\)\n\nUnfortunately, controlling for \\(L\\) alone does not remove selection bias, despite blocking such path. This is because the distribution of \\(L\\) in the sample does not necessarily match the distribution in the population.\nLet us inspect it.\n\nRegression-based approach:\nWe fit a model for the outcome \\(Y\\) against \\(A\\), controlling for \\(L\\), using only the selected samples:\n\n\nCode\n# Data for the selected sample\ndat.s = dat.1 %&gt;% filter(S==1)\nN.s = nrow(dat.s)\n\n# Model for causal effect, with selected\nmod.ate.s = lm(Y ~ A + L, data=dat.s)\nmodelsummary(list(\"ATE(S=1)\" = mod.ate.s), \n             statistic = NULL,\n             estimate  = \"{estimate}{stars} [{conf.low}, {conf.high}]\") \n\n\n\n\n\n\nATE(S=1)\n\n\n\n\n(Intercept)\n−1.341*** [−1.688, −0.995]\n\n\nA\n0.516* [0.058, 0.974]\n\n\nL\n1.843*** [1.601, 2.085]\n\n\nNum.Obs.\n514\n\n\nR2\n0.307\n\n\nR2 Adj.\n0.304\n\n\nAIC\n2458.1\n\n\nBIC\n2475.0\n\n\nLog.Lik.\n−1225.026\n\n\nRMSE\n2.62\n\n\n\n\n\n\n\nThe resulted 95% confidence interval for the coefficient of \\(A\\), using only samples for which \\(S=1\\), still covers the true ATE (0.75). Yet, a downwards bias shrinks the point-estimate and its lower bound noticeably. Using this, we would conclude the ATE is smaller than what truly is.\nLet us implement an IPW-based approach to compare against:\n\n\nIPW-based approach:\nGiven the SCM, and the assumption of \\(\\gamma_2=0\\), we can express:\n\n\\(w_i=\\mathbb{P}(S=1)/\\mathbb{P}(S=1\\mid A=a_i,L=l_i)\\)\n\\(v_i(a)=1/\\mathbb{P}(A=a) = 1/q\\) because treatment is randomized\n\nThe derived finite-sample estimator of the mean counterfactuals / potential outcomes is: \\[\n\\hat{Y}^a = N^{-1}\\sum_{i=1}^N\\frac{\\mathbb{I}(A_i=a)\\cdot\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\n\\]\nPutting all ingredients together we get point-estimates \\(\\hat{Y}^1=\\hat{\\mathbb{E}}[Y\\mid do(A=1)]\\) and \\(\\hat{Y}^0=\\hat{\\mathbb{E}}[Y\\mid do(A=0)]\\):\n\n\nCode\n# Compute the probability of selecton for all the units selected\n# We leverage the model mod.S, trained on complete data since S, A and L are\n# always observed (Y is the only one missing) and predict only on the selected units\nprob.s.unit = predict(mod.S, newdata=dat.s, type='response')\n\n# The unconditional probability of selection can be consistently estimated \n#... with counts from the total sample size and selected sample size\nprob.s = nrow(dat.s) / nrow(dat.1)\n\n# Since the treatment is randomized, the propensity score in the population is known at 0.5. \n# It can also be estimated from counts in the complete dataset\n# prop.score = sum(dat.1$A==1)/nrow(dat.1)\nprop.score = 0.5\n\n# Estimated counterfactuals/potential outcomes via IPPW\nest.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.Y = weights * Y) %&gt;% group_by(A) %&gt;% \n  summarise('PO'=sum(weighted.Y)/N.s)\n\n# Print resulted estimates\nhead(est.PO) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nA\nPO\n\n\n\n\n0\n-1.4318076\n\n\n1\n-0.7537995\n\n\n\n\n\n\n\nWhich, can be used to compute a point-estimate of the ATE: \\(\\hat{ATE}=\\hat{Y}^1-\\hat{Y}^0\\):\n\n\nCode\n# Print estimated ATE\nest.ATE = as.numeric(est.PO[2,2]-est.PO[1,2])\ndata.frame('IPW ATE'=est.ATE) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\n\n\n\n\n0.678008\n\n\n\n\n\n\n\nWe can see that the IPW-based approach produces a point-estimate closer to the true ATE. To get valid confidence intervals, however, a bootstrapping or asymptotic analysis need to be invoked. We can compute naïve confidence interval without resampling, using the fact:\n\\[\n\\hat{\\text{var}}(\\hat{\\text{ATE}}) \\geq \\hat{\\text{var}}(\\hat{Y}^1)+\\hat{\\text{var}}(\\hat{Y}^0)=(N-2)^{-2}\\sum_{a\\in\\{0,1\\}}\\sum_{i=1}^N \\mathbb{I}(A_i=a)\\cdot \\hat{v}_i(a)^2\\hat{w}_i^2(y_i-\\hat{Y}^a)^2\n\\]Such variance is naïve in the sense that it is overconfident due to ignoring the covariance between the potential outcomes, and the higher-order contributions of the weighting factors in the total variance. Anyway, using it will get us:\n\n\nCode\n# Estimated counterfactuals/potential outcomes via IPPW\nsd.PO = dat.s %&gt;% mutate(\n  prob.s.unit = as.numeric(prob.s.unit),\n  prob.s = as.numeric(prob.s),\n  prop.score = as.numeric(prop.score),\n  pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n  weights = pro.weight * prob.s * (1/prob.s.unit),\n  weighted.var = weights^2 * ( A* (Y-est.PO[2,2])^2 + (1-A)* (Y-est.PO[1,2])^2)) %&gt;% \n  group_by(A) %&gt;% \n  summarise('sd'=sqrt(sum(weighted.var))/(N.s-2))\n\n# Naïve confidence interval\nnaivebounds = qnorm(0.975)*sum(sd.PO[,2])\ndata.frame('IPW ATE'=as.numeric(est.ATE),\n           'naïve LB'=as.numeric(est.ATE)-naivebounds,\n           'naïve UB'=as.numeric(est.ATE)+naivebounds) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nIPW.ATE\nnaïve.LB\nnaïve.UB\n\n\n\n\n0.678008\n0.5741569\n0.7818592\n\n\n\n\n\n\n\nAs noted, the naïve confidence interval is tighter than those produced by inference on the complete data. This means such confidence interval is not statistically valid, but it can still help us visualize the convergence of IPW estimator.\nNow, if we simulate and repeat the DGP and same analysis for different sample sizes, consistency would be visually perceived if we see:\n\nPoint-estimate converging to the true ATE\nConfidence intervals shrinking at a fast (**) rate\n\nLet us test it. We run 20 simulations with different complete sample sizes, from \\(N=500\\) to \\(N=23\\,000\\) [number of complete samples from \\(N_s=250\\) to \\(N_s=12\\,000\\)]. We repeat the procedure three times and average the results on those three repetitions. Such averages are presented in the following table:\n\n\nCode\n# Data frame to save results from iterations\nrounds.IPPW = data.frame(N=NA, Ns=NA, EST=NA, LB=NA, UB=NA)\n\n# All sample sizes to consider\nsamplesizes = round(500*exp(0.2*(0:19)))\n\n# Number of repetitions\nM = 3\n\n# Paramaters are the same as before\nparam.iter = param.1\n\n# Seed\nset.seed(66)\n\n# Loop\nfor(m in 1:M){\n  for(n in samplesizes){\n    \n    # Change sample size\n    param.iter[1] = n\n    \n    # Generate the data (complete and selected)\n    dat.iter = dgp(param.iter)[[1]]\n    dat.s.iter = dat.iter %&gt;% filter(S==1)\n    \n    # Estimated probability of selection\n    N.s.iter = nrow(dat.s.iter)\n    prob.s.iter = N.s.iter / n\n    \n    # Model for S, with complete data\n    mod.S.iter = glm(S ~ A + L, family=binomial('probit'), data=dat.iter)\n    prob.s.unit.iter = predict(mod.S.iter, newdata=dat.s.iter, type='response')\n  \n    # Propensity score model\n    prop.score.iter = sum(dat.iter$A==1)/nrow(dat.iter)\n    \n    # Put everything together\n    row.iter = dat.s.iter %&gt;% mutate(\n      prob.s.unit = as.numeric(prob.s.unit.iter),\n      prob.s = as.numeric(prob.s.iter),\n      prop.score = as.numeric(prop.score.iter),\n      pro.weight = (A/prop.score) + (1-A)/(1-prop.score),\n      weights = pro.weight * prob.s * (1/prob.s.unit),\n      weighted.Y = weights * Y) \n    \n    # Estimated counterfactuals/potential outcomes via IPPW\n    po.iter = row.iter %&gt;% group_by(A) %&gt;% \n      summarise('Y(A)'=sum(weighted.Y)/N.s.iter)\n    \n    # Estimated ATE\n    ATE.iter = as.numeric(po.iter[2,2]-po.iter[1,2])\n    \n    # Naïve variances\n    sd.PO.iter = row.iter %&gt;% mutate(\n      weighted.var = weights^2 * ( A* (Y-po.iter[2,2])^2 + (1-A)* (Y-po.iter[1,2])^2)) %&gt;% \n    group_by(A) %&gt;% \n    summarise('sd'=sqrt(sum(weighted.var))/(N.s.iter-1))\n    \n    # Naïve confidence interval\n    naivebounds = qnorm(0.975)*sum(sd.PO.iter[,2])\n    \n    rounds.IPPW = rbind.data.frame(rounds.IPPW,\n                                   data.frame(N=n,\n                                              Ns=N.s.iter,\n                                              EST=ATE.iter,\n                                              LB=ATE.iter-naivebounds,\n                                              UB=ATE.iter+naivebounds))\n  }\n}\n# Print resulted estimates\nrounds.IPPW = rounds.IPPW[-1,] %&gt;%\n  group_by(N) %&gt;%\n  summarise_all(mean)\n\nhead(rounds.IPPW) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nN\nNs\nEST\nLB\nUB\n\n\n\n\n500\n249.6667\n0.5919352\n-0.1461034\n1.3299737\n\n\n611\n309.0000\n0.7701710\n0.1782066\n1.3621354\n\n\n746\n379.6667\n0.8962619\n0.2991885\n1.4933353\n\n\n911\n453.6667\n0.7438565\n0.5811669\n0.9065460\n\n\n1113\n556.0000\n0.7817429\n0.3579693\n1.2055165\n\n\n1359\n679.0000\n0.6411555\n0.3223225\n0.9599884\n\n\n\n\n\n\n\nAn the following plot:\n\n\nCode\n### Plot results from rounds\nrounds.IPPW[,-1] %&gt;% melt(id.vars = 'Ns') %&gt;% \n  mutate(type = ifelse(variable=='EST','EST','N. C.I.') ) %&gt;% \n  ggplot(aes(x=Ns,y=value,group=variable)) + \n  geom_point(aes(shape=type,color=type)) + \n  geom_smooth(method = lm, formula = y ~ x + I(sqrt(x)), se = FALSE) + ## O(N^0.5) convergence\n  labs(y='Value of estimate / bound', x='number of complete samples') +\n  geom_hline(yintercept=T.ATE, col = 'red') +\n  theme_bw()\n\n\n\n\n\nWe can appreciate that the estimator converges to the true ATE [in red], and its uncertainty reduce at a sustained rate; there is convergence in probability. In other words, the IPW-based estimator is consistent.\n\nMathematical justification of consistency\nWe can prove consistency mathematically for this SCM. However, to avoid measure-theoretic conundrums, let us consider the case for which all variables are discrete. Results are generalizable for mixed discrete-continuous cases with positive distributions (no zero-measure events).\nLet us assume \\(Y\\) has support on \\(\\{y_{(c)}\\}_{c=1}^C\\), and \\(L\\) has support on \\(\\{l_{(k)}\\}_{k=1}^K\\), then:\n\\[\n\\begin{aligned}\n\\hat{Y}^a &= N^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a_i,L=l_i)}\\cdot y_i\\cdot \\mathbb{I}(A_i=a) \\\\\n&=\nN^{-1}\\sum_{i=1}^N\\frac{\\hat{\\mathbb{P}}(S=1)}{\\hat{\\mathbb{P}}(A=a\\mid L=l_i)\\cdot\\hat{\\mathbb{P}}(S=1\\mid A=a,L=l_i)}\\cdot y_i\\cdot\\mathbb{I}(A_i=a,S_i=1)\\\\\n&=\n\\sum_{i=1}^N\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_i)\n}\\cdot\n\\frac{\ny_i\\cdot\\mathbb{I}(A_i=a,S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_i,S=1)\n}\\\\\n&= \\sum_{i=1}^N\\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{i}\\cdot\\mathbb{I}(A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\sum_{i=1}^N\\mathbb{I}(Y_i=y_{(c)},A_i=a,L_i=l_{(k)},S_i=1)/N}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&= \\sum_{k}\\sum_{c}\n\\frac{\\hat{\\mathbb{P}}(S=1)}{\n\\hat{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot\\hat{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n\\hat{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\n\\end{aligned}\n\\]\nAssuming the propensity scores and the probability of selection are both correctly specified, all finite-sample approximations \\(\\hat{\\mathbb{P}}\\) converge in the limit to the true distributions \\(\\mathbb{P}\\). Then:\n\\[\n\\begin{aligned}\n\\text{plim}_{N\\rightarrow\\infty}\n\\hat{Y}^a\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(S=1)}{\n{\\mathbb{P}}(S=1\\mid L=l_{(k)})\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n\\frac{{\\mathbb{P}}(L=l_{(k)})}{\n{\\mathbb{P}}(L=l_{(k)}\\mid S=1)\n}\\cdot\n\\frac{\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)},A=a,L=l_{(k)}\\mid S=1)}{\n{\\mathbb{P}}(A=a\\mid L=l_{(k)},S=1)\n}\\\\\n&=\n\\sum_{k}\\sum_{c}\n{\\mathbb{P}}(L=l_{(k)})\n\\cdot\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)} S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\sum_{c}\ny_{(c)}\\cdot{\\mathbb{P}}(Y=y_{(c)}\\mid A=a,L=l_{(k)}, S=1)\\\\\n&=\n\\sum_{k}\n{\\mathbb{P}}(L=l_{(k)})\n\\mathbb{E}(Y\\mid A=a,L, S=1)\\\\\n&=\n\\mathbb{E}_L\n\\mathbb{E}(Y\\mid A=a,L, S=1) =\\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L,S=1]\\\\\n&= \\mathbb{E}_L\\mathbb{E}[Y\\mid do(A=a),L] = \\mathbb{E}[Y\\mid do(A=a)]\n\\end{aligned}\n\\]\nThis is, the IPW estimator converges to the true counterfactual mean given by intervention \\(do(A=a)\\). The last two equalities come from these facts:\n\n\\(L\\) blocks all non-causal paths from \\(A\\) to \\(Y\\) when conditioning on \\(S=1\\)\n\\(Y\\perp S\\,\\mid L\\) in the back-door graph: the resulting DAG after removing all arrows coming out of \\(A\\)\n\\(\\mathbb{P}(L=l_{(k)})\\) is the population-distribution of \\(L\\)\n\n\n\n\nRevisiting the regression approach: generalized adjustment criteria\nNotice that, in the convergence proof for the IPW estimator, it was shown that the underlying estimand is algebraically equivalent to a regression-adjusted mean of \\(Y\\), averaged over the population distribution of \\(L\\).\n\\[\n\\mathbb{E}[Y\\mid do(A=a)] = \\mathbb{E}_L\\mathbb{E}(Y\\mid A=a,L,S=1)\n\\]\nExtensions of this results are covered by three generalized adjustment criteria (Correa, Tian, and Bareinboim 2018). This is, for this case, a mathematically equivalent result in term of consistency can be achieved via regression adjustment."
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-identifiability",
    "href": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-identifiability",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The problem of identifiability",
    "text": "The problem of identifiability\nWhen there is no sample selection nor missingness, or when missing is completely at random (MCAR), all arrows pointing to \\(R_Y\\) are absent. The \\(m\\)-graph representing the system correspond to a causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\), and samples are obtained from the observational distribution \\(P(H,A,M,Y)\\). This is the traditional setting motivating causal inference with observational data.\n\n\n\nFigure 2: causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\)\n\n\nUnder the assumptions embedded in the causal graph \\(\\mathcal{G}'\\), and a special mutilation known as the back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\)2, \\(\\psi\\) is nonparametrically identifiable from \\(P(H,A,M,Y)\\) via the back-door formula (Pearl 1995, 2012), as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}[Y\\mid H,A=a]\n\\tag{2}\\]\nGiven identifiability plus \\(N\\) i.i.d. samples from \\(P(H,A,M,Y)\\), a consistent estimator can be constructed using a regression model for the outcome \\(\\hat{Q}(H,A)=\\hat{\\mathbb{E}}[Y\\mid H,A]\\), and then proceeding with \\(g\\)-computation (Robins 1986):\n\\[\n\\hat{\\psi} = N^{-1}\\sum_{i=1}^{N}\\Delta_a\\hat{Q}(H_i,a)\n\\tag{3}\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-recoverability",
    "href": "posts/2023-08-01_missingOutcome/index.html#the-problem-of-recoverability",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "The problem of recoverability",
    "text": "The problem of recoverability\nA (causal) parameter is said to be recoverable from the observed-data distribution \\(P(H,A,M,Y^*,R_Y)\\) 3 if it can be uniquely computed from it using the assumptions embedded in \\(\\mathcal{G}\\) (and the necessary graph mutilation).\nUnder identifiability in the substantive model 4, there is an ample number of methods to recover joint/conditional distributions based on different statistical theories. Although not originally motivated by graphical models, they can be seen as ad hoc solutions under special graphical conditions (Mohan and Pearl 2021).\nTable 1 presents a summary of literature review 5 benchmarking four methodological approaches in terms of:\n\nGraphical conditions for recoveravility: from hard (\\(\\bigstar\\)) to easy (\\(\\bigstar\\bigstar\\bigstar\\)) to fulfill/believe\nFlexibility in model specification: from parametric (\\(\\bigstar\\)) to ML/nonparametric (\\(\\bigstar\\bigstar\\bigstar\\))\nStatistical efficiency: from wider (\\(\\bigstar\\)) to narrower (\\(\\bigstar\\bigstar\\bigstar\\)) confidence/credible intervals\nComputational efficiency: from slow (\\(\\bigstar\\)) to fast (\\(\\bigstar\\bigstar\\bigstar\\)) computation/convergence\n\n\nTable 1: some statistical methods to address selection/missingness\n\n\n\n\n\n\n\n\n\nMethod\nGraph cond.\nFlex. spec.\nStat. eff.\nComp. eff.\n\n\n\n\nExpectation-maximization (Dempster, Laird, and Rubin 1977)\n\\(\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\star\\)\n\\(\\bigstar\\)\n\n\nMultiple imputation (Rubin 1976, 1978)\n\\(\\bigstar\\star\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\n\nInverse probability weighting (Robins and Rotnitzky 1992; Robins, Rotnitzky, and Zhao 1994)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\nRegression adjustment (Bareinboim, Tian, and Pearl 2014; J. Correa, Tian, and Bareinboim 2018)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\n\nArguably, the best set of properties come from IPW and regression adjustment, due to their direct derivation from graphical criteria, which might extend the Rubin-MAR setting. Moreover, both solutions have important theoretical results from the theory of semiparametric estimation, and produce doubly-robustness when combined. These reasons have motivated syncretic estimators, such as:\n\nTable 2: some doubly-robust estimation methods\n\n\n\n\n\n\n\n\n\nMethod\nFast consistency\nPlug-in for target\nBayesian version\n# iter. steps\n\n\n\n\nAugmented inverse probability weighting (AIPW) (Robins, Rotnitzky, and Zhao 1994)\nNo\nNo\nNo\n0\n\n\nTargeted learning (van der Laan and Rose 2011)\nYes\nYes\nYes, kinda\n\\(\\geq 1\\)\n\n\nDebiased machine learning (DML) (Chernozhukov et al. 2018)\nYes\nNo\nNo\n0\n\n\n\n\nRecoverability via IPW\nGraphical conditions for recoverability via IPW (Mohan and Pearl 2014), with missing data on \\(Y\\), are:\n\nThere is a back-door admissible set in the substantive model (\\(H\\))\nNo self-selection: there are no directed arrows between \\(Y\\) and \\(R_Y\\)\nNo open collider paths between \\(Y\\) and \\(R_Y\\) (open by variables involved in the query)\n(When there are multiple missingness mechanisms: \\(R_V\\cap R_{\\text{mb}(R_V)}=\\emptyset\\))\n\nOur \\(m\\)-graph \\(\\mathcal{G}\\) (figure 1) allows recoverability, so we can express :\n\\[\n\\begin{aligned}\n    p(Y\\mid do(A)) &= \\int\\frac{ \\text{d} H}{p(A\\mid H)}\\int \\frac{\\text{d} M}{\\mathbb{P}(R_Y=1\\mid H,M)}\\, p(H,A,M,Y\\mid R_Y=1)  \\\\\n    &= \\mathbb{E}_{H\\mid R_Y=1}\\left[\\frac{p(A,Y\\mid H,M,R_Y=1)}{p(A\\mid H)\\, \\mathbb{P}(R_Y=1\\mid H,M) }  \\right]\n\\end{aligned}\n\\tag{4}\\]\nThus, the IPW-estimator of the ATE is:\n\\[\n    \\hat{\\psi}^{w} = N_1^{-1}\\sum_{i=1}^{N_1}\\frac{(2A^i-1)\\,Y^i}{\\hat{p}(A^i\\mid H^i)\\,\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i) }\n\\tag{5}\\]\nIt requires two models:\n\nTreatment-assignment mechanism: \\(\\hat{p}(A^i\\mid H^i)\\). It does not involve the mediator \\(M\\)\nSelection mechanism: \\(\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i)\\). It does involve the mediator \\(M\\)\n\n\n\nRecoverability via regression adjustment\nWorking with samples from \\(P(H,A,M,Y^*,R_Y)\\) implies conditioning on \\(R_Y=1\\) (Bareinboim and Pearl 2012). In the \\(m\\)-graph of figure 1, \\(\\mathcal{G}\\), such condition opens the following non-causal paths in the (proper) back-door graph:\n\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow M\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow M\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow M\\longleftarrow H\\longrightarrow Y\\)\n\n\n\n\nFigure 3: The (proper) back-door graph\n\n\nGraphical conditions for recoverability via GAC (J. Correa, Tian, and Bareinboim 2018), with missing data on \\(Y\\), using an adjustment set \\(Z\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(Z\\) and \\(R_Y\\): \\(Y\\perp A\\mid Z, R_Y\\) in the (proper) back-door graph\n\\(Z\\) \\(d\\)-separates \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid Z\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(Z\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nNo adjustment set fulfills all these critera. In particular, \\(Z=\\{H,M\\}\\) fulfills only the first two.\n\nThese criteria are incomplete, because they do not consider post-treatment selection influenced by mediators.\nFix: Our recoverability criteria via regression adjustment with missing data on \\(Y\\) using pre-treatment set \\(H\\) and forbidden set \\(M\\subset \\text{fb}(A,Y;\\mathcal{G})\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(H\\): \\(Y\\perp A\\mid H\\) in the (proper) back-door graph of the substantive model\n\\(H,M\\) \\(d\\)-separate \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid H,M\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(H\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nThis modification is not a revolutionary discovery. It is implied from the sequential factorization by Mohan and Pearl (2014), and from \\(c\\)-factorization by J. D. Correa, Tian, and Bareinboim (2019). Yet, the former does not address the causal query directly, and the latter might be a fairly complicated overshoot. A nice list of graphical criteria, as in the case without forbidden nodes, might be more useful for researchers. Besides, IPW tends to be the first option in applied research, maybe it is thought that in some contexts regression adjustment is not possible.\n\nUnder the modified criteria, we have that:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}_{M\\mid H,A=a}\\mathbb{E}[Y\\mid H,A=a, M, R_Y=1]\n\\tag{6}\\]\nThis estimand can be expressed in two different ways, each using two models.\nSolution 1: full mediator density-model:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\left[\\int \\underbrace{\\text{d}P(M\\mid H,A=a)}_{\\mathcal{M}_2}\\, \\underbrace{Q(H,a,M)}_{\\mathcal{M}_1} \\right]\n\\tag{7}\\]\nOne model for the expected-outcome model (with \\(M\\) as predictor): \\(Q(H,A,M)=\\mathbb{E}[Y\\mid H,A, M, R_Y=1]\\). The other model involved is a full mediator density-model: \\(p(M\\mid H,A)\\)\nSolution 2: nested/hierarchical regressions:\n\\[\n\\begin{aligned}\n&\\mathcal{M}_1: & Q_1(H,A,M) &=\\mathbb{E}[Y\\mid H,A, M, R_Y=1] \\\\\n&\\mathcal{M}_2: & Q_2(H,A) &=\\mathbb{E}_{M\\mid H,A}\\, Q_1(H,A,M) \\\\\n& & \\psi &= \\Delta_a\\mathbb{E}_H\\, Q_2(H,a)\n\\end{aligned}\n\\tag{8}\\]\nThe second model is not in the mediator support, but in the outcome support. \\(\\mathcal{M}_2\\) averages the predictions of \\(\\mathcal{M}_1\\), as a function of only \\(H,A\\).\nThis solution has a clear advantage: it does not require modeling the conditional density of \\(M\\), which is a hard job when such a variable is high-dimensional or combines discrete and continuous components. A little of math is required in the TMLE front.\n\n\nRecoverability via mediation analysis\nAn alternative approach, closely related to regression adjustment, is mediation analysis. This is a viable approach in some circumstances, as the natural indirect effect (NIE) and the total direct effect (TDE) might be recoverable using Correa’s GAC, even when the ATE is not. The latter can be reconstructed as ATE = NIE + TDE. Yet, this approach has drawbacks relative to regression adjustment:\n\nIdentifiability conditions in the substantive model are stronger for mediation effects than for the ATE: sequential ignorability (and cross-world assumption). Not clear to me how violations impact recoverability.\nTMLE for mediation effects become a bit more involved\nAn explicit model for \\(M\\) is required, but reformulations similar to solution 2 are possible (Xu, Liu, and Liu 2022)\n\n\n\nMultiply-robustness\nCombining IPW and regression adjustment solutions produces multiply-robustness. A doubly-robust estimator for the ATE can be constructed for solution 1: it will be consistent if all semiparametric models involved are correctly specified, or in one of these scenarios:\n\n\\(p(M\\mid H, A),\\quad\\) \\(\\mathbb{E}[Y\\mid H,A, M, R_Y=1]\\quad\\) are both well specified\n\\(p(A\\mid H)\\) and \\(p(R_Y\\mid A,H,M)\\) are both well specified\n\nNested regressions (solution 2) would produce doubly-robustness too, with consistency if either:\n\n\\(Q_2(H,A)=\\mathbb{E}_{M\\mid H,A=a}\\mathbb{E}[Y\\mid H,A=a, M, R_Y=1]\\) is well specified\n\\(p(A\\mid H)\\) and \\(p(R_Y\\mid A,H,M)\\) are both well specified\n\nMediation analysis would produce triply-robustness, with consistency if all semiparametric models involved are correctly specified, or in one of these scenarios:\n\n\\(p(M\\mid H, A),\\quad\\) \\(\\mathbb{E}[Y\\mid H,A, M, R_Y=1]\\quad\\) are all well specified\n\\(p(A\\mid H),\\quad\\) \\(p(R_Y\\mid A,H,M),\\quad\\) \\(p(M\\mid H, A)\\quad\\) are all well specified\n\\(p(A\\mid H),\\quad\\) \\(p(R_Y\\mid A,H,M),\\quad\\) \\(\\mathbb{E}[Y\\mid H,A, M, R_Y=1]\\quad\\) are all well specified\n\nNote that, using a misspecified model for \\(M\\) here (like when reducing its dimension with PCA, VAE or representation learning) leaves the estimator with worse robustness than simply using IPW, as it requires correct specification of \\(A\\), \\(R_Y\\) and \\(Y\\), whereas IPW only requires \\(A\\), \\(R_Y\\)"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#setting",
    "href": "posts/2023-08-01_missingOutcome/index.html#setting",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Setting",
    "text": "Setting\nConsider the following \\(m\\)-graph1, \\(\\mathcal{G}\\), representing the causal relations among a set of random variables \\(\\mathcal{V}=\\{H,A,M,Y,R_Y\\}\\), where:\n\n\\(H\\in\\mathbb{R}^d\\) is a vector of pre-treatment and context covariates\n\\(A\\in\\{0,1\\}\\) is a binary exposure\n\\(Y\\) is the outcome of interest, with general support (univariate or multivariate, discrete or continuous)\n\\(M\\) is a mediator on the causal pathway from \\(A\\) to \\(Y\\), with general support\n\\(R_Y\\in\\{0,1\\}\\) is an indicator of sample selection for \\(Y\\), i.e., for a given sample, \\(R_Y=1\\) means \\(Y\\) is observed; otherwise \\(Y\\) is missing (denoted with proxy \\(Y^*=\\emptyset\\)).\n\n\n\n\nFigure 1: \\(m\\)-graph \\(\\mathcal{G}\\)\n\n\nOur goal is to estimate the average treatment effect (ATE), \\(\\psi\\), in the target population, defined as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}[Y\\mid do(A=a)] := \\mathbb{E}[Y\\mid do(A=1)]-\\mathbb{E}[Y\\mid do(A=0)]\n\\tag{1}\\]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#footnotes",
    "href": "posts/2023-08-01_missingOutcome/index.html#footnotes",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(m\\)-graphs generalize causal graphs in settings with sample selection (Hernán, Hernández-Díaz, and Robins 2004) and missing data (Mohan and Pearl 2021).↩︎\nThe back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\) is the graph resulting from removing in \\(\\mathcal{G}'\\) the first arrow of all directed paths from \\(A\\) to \\(Y\\). It is termed the proper back-door graph in multi-exposure settings, and results from removing the first arrow of all directed and non-self-intersecting paths from \\(A\\) to \\(Y\\).↩︎\nSampling from \\(P(H,A,M,Y^*,R_Y)\\) is equivalent to sample from \\(\\{P(H,A,M,Y\\mid R_Y=1),P(H,A,M)\\}\\)↩︎\nRecoverability might be possible without identifiability in the substantive model, via (fairly complicated) \\(c\\)-factorizations (J. D. Correa, Tian, and Bareinboim 2019) or \\(\\phi\\)-factorizations (Bhattacharya et al. 2020) related to the problem of \\(g\\)-identifiability↩︎\nLiterature review based on Seaman and White (2013), Dong and Peng (2013), Perkins et al. (2017), Lewin et al. (2018)↩︎"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#simulations",
    "href": "posts/2023-08-01_missingOutcome/index.html#simulations",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Simulations",
    "text": "Simulations\nCheck this"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#path-forward",
    "href": "posts/2023-08-01_missingOutcome/index.html#path-forward",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Path forward",
    "text": "Path forward\nLet us commit to:\n\nOne missing mechanism: \\(R_Y\\)\n\\(m\\)-graph depicted in figure 1, so we have identifiability in the substantive model, and recoverability via IPW and regression\nTMLE (the super-learner is default, we could restrict to only splines or BART, Bayesian?)\n\n\nThe tmle package in R deals with missing data in a very basic manner, combining dropping observations and single median-imputations: preprocessing missing data for TMLE\n\n\nNested regressions, and full \\(M\\)-model only for one mediator\nTry simulations"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#setting",
    "href": "posts/2023-08-15_missingSimulation/index.html#setting",
    "title": "XXX",
    "section": "Setting",
    "text": "Setting\n\n\nCode\n# Load packages ----------------------------------------------\nlibrary(readr)        # Reads CSV      \nlibrary(data.table)   # Processes dataframes\nlibrary(dplyr)        # Processes dataframes\nlibrary(kableExtra)   # Styles tables\nlibrary(sl3)          # Performs super-learning\nlibrary(tmle3)        # Performs TMLE\nlibrary(tmle3mediate) # Performs TMLE for mediation analysis\n\n# Download example data ---------------------------------------\nwashb_data = read_csv(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\")\n)\nwashb_data = as.data.table(washb_data)\n\n# Glimpse of the data\nhead(washb_data) %&gt;% kbl() %&gt;% kable_styling(full_width = F)\n\n\n\n\n\nwhz\ntr\nfracode\nmonth\naged\nsex\nmomage\nmomedu\nmomheight\nhfiacat\nNlt18\nNcomp\nwatmin\nelec\nfloor\nwalls\nroof\nasset_wardrobe\nasset_table\nasset_chair\nasset_khat\nasset_chouki\nasset_tv\nasset_refrig\nasset_bike\nasset_moto\nasset_sewmach\nasset_mobile\n\n\n\n\n0.00\nControl\nN05265\n9\n268\nmale\n30\nPrimary (1-5y)\n146.40\nFood Secure\n3\n11\n0\n1\n0\n1\n1\n0\n1\n1\n1\n0\n1\n0\n0\n0\n0\n1\n\n\n-1.16\nControl\nN05265\n9\n286\nmale\n25\nPrimary (1-5y)\n148.75\nModerately Food Insecure\n2\n4\n0\n1\n0\n1\n1\n0\n1\n0\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n-1.05\nControl\nN08002\n9\n264\nmale\n25\nPrimary (1-5y)\n152.15\nFood Secure\n1\n10\n0\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n-1.26\nControl\nN08002\n9\n252\nfemale\n28\nPrimary (1-5y)\n140.25\nFood Secure\n3\n5\n0\n1\n0\n1\n1\n1\n1\n1\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n-0.59\nControl\nN06531\n9\n336\nfemale\n19\nSecondary (&gt;5y)\n150.95\nFood Secure\n2\n7\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n-0.51\nControl\nN06531\n9\n304\nmale\n20\nSecondary (&gt;5y)\n154.20\nSeverely Food Insecure\n0\n3\n1\n1\n0\n1\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-identifiability",
    "href": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-identifiability",
    "title": "XXX",
    "section": "The problem of identifiability",
    "text": "The problem of identifiability\nWhen there is no sample selection nor missingness, or when missing is completely at random (MCAR), all arrows pointing to \\(R_Y\\) are absent. The \\(m\\)-graph representing the system correspond to a causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\), and samples are obtained from the observational distribution \\(P(H,A,M,Y)\\). This is the traditional setting motivating causal inference with observational data.\n\n\n\nFigure 2: causal graph \\(\\mathcal{G}'\\equiv\\mathcal{G}[\\overline{R_Y}]\\)\n\n\nUnder the assumptions embedded in the causal graph \\(\\mathcal{G}'\\), and a special mutilation known as the back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\)2, \\(\\psi\\) is nonparametrically identifiable from \\(P(H,A,M,Y)\\) via the back-door formula (Pearl 1995, 2012), as:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}[Y\\mid H,A=a]\n\\]\nGiven identifiability plus \\(N\\) i.i.d. sample from \\(P(H,A,M,Y)\\), a consistent estimator can be constructed using a regression model for the outcome \\(\\hat{Q}(H,A)=\\hat{\\mathbb{E}}[Y\\mid H,A]\\), and then proceeding with \\(g\\)-computation (Robins 1986):\n\\[\n\\hat{\\psi} = N^{-1}\\sum_{i=1}^{N}\\Delta_a\\hat{Q}(H_i,a)\n\\]"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-recoverability",
    "href": "posts/2023-08-15_missingSimulation/index.html#the-problem-of-recoverability",
    "title": "XXX",
    "section": "The problem of recoverability",
    "text": "The problem of recoverability\nA (causal) parameter is said to be recoverable from the observed-data distribution \\(P(H,A,M,Y^*,R_Y)\\equiv\\{P(H,A,M,Y\\mid R_Y=1),P(H,A,M) \\}\\) if it can be uniquely computed from it using the assumptions embedded in \\(\\mathcal{G}\\) (and the necessary graph mutilation).\nUnder identifiability in the substantive model 3, \\(\\mathcal{G}[\\overline{R_Y}]\\), there is an ample number of methods to recover joint/conditional distributions from sample selection/missingness, based on different statistical theories. Although not originally motivated by graphical models, they can be seen as ad hoc solutions under special graphical conditions (Mohan and Pearl 2021). Table 1 presents a summary of literature review 4 benchmarking four methodological approaches in terms of:\n\nGraphical conditions for recoveravility: from hard (\\(\\bigstar\\)) to easy (\\(\\bigstar\\bigstar\\bigstar\\)) to fulfill/believe\nFlexibility in model specification: from parametric (\\(\\bigstar\\)) to ML/nonparametric (\\(\\bigstar\\bigstar\\bigstar\\))\nStatistical efficiency: from wider (\\(\\bigstar\\)) to narrower (\\(\\bigstar\\bigstar\\bigstar\\)) confidence/credible intervals\nComputational efficiency: from slow (\\(\\bigstar\\)) to fast (\\(\\bigstar\\bigstar\\bigstar\\)) computation/convergence\n\n\nTable 1: some statistical methods to address selection/missingness\n\n\n\n\n\n\n\n\n\nMethod\nGraph cond.\nFlex. spec.\nStat. eff.\nComp. eff.\n\n\n\n\nExpectation-maximization (Dempster, Laird, and Rubin 1977)\n\\(\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\star\\)\n\\(\\bigstar\\)\n\n\nMultiple imputation (Rubin 1976, 1978)\n\\(\\bigstar\\star\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\n\nInverse probability weighting (Robins and Rotnitzky 1992; Robins, Rotnitzky, and Zhao 1994)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\nRegression adjustment (Bareinboim, Tian, and Pearl 2014; J. Correa, Tian, and Bareinboim 2018)\n\\(\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\\(\\bigstar\\bigstar\\bigstar\\)\n\n\n\nArguably, the best set of properties come from IPW and regression adjustment, due to their direct derivation from graphical criteria, which might extend the Rubin-MAR setting. Moreover, both solutions have important theoretical results from the theory of semiparametric estimation, and produce doubly- or multiply-robustness when combined. These reasons have motivated syncretic estimators, such as:\n\nTable 2: some doubly/multiply-robust estimation methods\n\n\n\n\n\n\n\n\n\n\nMethod\nML & adaptive\nFast consistency\nPlug-in for target\nBayesian version\n# iter. steps\n\n\n\n\nAugmented inverse probability weighting (AIPW) (Robins, Rotnitzky, and Zhao 1994)\nHuh\nNo\nNo\nNo\n0\n\n\nTargeted learning (van der Laan and Rose 2011)\nYes\nYes\nYes\nYes, kinda\n\\(\\geq 1\\)\n\n\nDebiased machine learning (DML) (Chernozhukov et al. 2018)\nYes\nYes\nNo\nNo\n0\n\n\n\n\nRecoverability via IPW\nGraphical (NS) conditions for recoverability via IPW (Mohan and Pearl 2014), with missing data on \\(Y\\), are:\n\nThere is a back-door admissible set in the substantive model (\\(H\\))\nNo self-selection: there are no directed arrows between \\(Y\\) and \\(R_Y\\)\nNo open collider paths between \\(Y\\) and \\(R_Y\\) (open by variables involved in the query)\n(When there are multiple missingness mechanisms: \\(R_V\\cap R_{\\text{mb}(R_V)}=\\emptyset\\))\n\nOur \\(m\\)-graph \\(\\mathcal{G}\\) (figure 1) allows recoverability, so we can express : \\[\n\\begin{aligned}\n    p(Y\\mid do(A)) &= \\int\\frac{ \\text{d} H}{p(A\\mid H)}\\int \\frac{\\text{d} M}{\\mathbb{P}(R_Y=1\\mid H,M)}\\, p(H,A,M,Y\\mid R_Y=1)  \\\\\n    &= \\mathbb{E}_{H\\mid R_Y=1}\\left[\\frac{p(A,Y\\mid H,M,R_Y=1)}{p(A\\mid H)\\, \\mathbb{P}(R_Y=1\\mid H,M) }  \\right]\n\\end{aligned}\n\\]\nThus, the IPW-estimator of the ATE is: \\[\n    \\hat{\\psi}^{w} = N_1^{-1}\\sum_{i=1}^{N_1}\\frac{(2A^i-1)\\,Y^i}{\\hat{p}(A^i\\mid H^i)\\,\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i) }\n\\]\nIt requires two models:\n\nTreatment-assignment mechanism: \\(\\hat{p}(A^i\\mid H^i)\\). It does not involve the mediator \\(M\\)\nSelection mechanism: \\(\\hat{\\mathbb{P}}(R_Y=1\\mid H^i,M^i)\\). It does involve the mediator \\(M\\)\n\n\n\nRecoverability via regression adjustment\n\nSince the identification (+estimation) problem in the substantive model is solved via regression and \\(g\\)-computation, can this approach leverage recovery (+estimation)?\n\nNotice that, working with samples from \\(P(H,A,M,Y^*,R_Y)\\equiv\\{P(H,A,M,Y\\mid R_Y=1),P(H,A,M) \\}\\) implies conditioning on \\(R_Y=1\\) (Bareinboim and Pearl 2012). In the \\(m\\)-graph of figure 1, \\(\\mathcal{G}\\), such condition opens the following non-causal paths in the (proper) back-door graph:\n\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow H\\longrightarrow M\\longrightarrow Y\\)\n\\(A\\longrightarrow R_Y\\longleftarrow M\\longrightarrow Y\\)\n\n\n\n\nFigure 3: The (proper) back-door graph\n\n\nGraphical (NS?) conditions for recoverability via GAC (J. Correa, Tian, and Bareinboim 2018), with missing data on \\(Y\\), using an adjustment set \\(Z\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(Z\\) and \\(R_Y\\): \\(Y\\perp A\\mid Z, R_Y\\) in the (proper) back-door graph\n\\(Z\\) \\(d\\)-separates \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid Z\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(Z\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nNo adjustment set fulfills all these critera. In particular, \\(Z=\\{H,M\\}\\) fulfills the first two, but not the third.\n\nThe criteria are incomplete, because they do not consider post-treatment selection influenced by mediators. Do not worry! I came with a fix\nde Aguas, Biele and Pensar’s recoverability criteria via regression adjustment with missing data on \\(Y\\) using pre-treatment set \\(H\\) and forbidden set \\(M\\subset \\text{fb}(A,Y;\\mathcal{G})\\):\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(H\\) and \\(R_Y\\): \\(Y\\perp A\\mid H\\) in the (proper) back-door graph of the substantive model\n\\(H,M\\) \\(d\\)-separate \\(Y\\) from \\(R_Y\\): \\(Y\\perp R_Y\\mid H,M\\) in the (proper) back-door graph\nThe adjustment set contains no forbidden nodes: \\(H\\cap\\text{fb}(A,Y;\\mathcal{G})=\\emptyset\\)\n\n\nThis modification is not a revolutionary discovery. It is implied from the sequential factorization by Mohan and Pearl (2014), and from \\(c\\)-factorization by J. D. Correa, Tian, and Bareinboim (2019). Yet, the former does not do it in the context of causal inference, and the latter might be a fairly complicated overshoot. A nice list of graphical criteria, as in the case without forbidden nodes, might be more useful for researchers. Besides, IPW tends to be the first option in applied research, maybe it is thought that in some contexts regression adjustment is not possible.\n\nUnder the modified criteria, we have that:\n\\[\n\\psi = \\Delta_a\\mathbb{E}_H\\mathbb{E}_{M\\mid H,A=a}\\mathbb{E}[Y\\mid H,A=a, M, R_Y=1]\n\\] Notice now the solution requires two models:\n\nAn outcome model, with \\(M\\) as predictor, for \\(Q(H,A,M)=\\mathbb{E}[Y\\mid H,A, M, R_Y=1]\\)\n\nA mediator model, to estimate \\(p(M\\mid H,A)\\)\n\nHow to specify these models? I see three options, including a dimension reduction \\(M'=\\Phi(M)\\) using either PCA, VAE, or representation learning:\n\nTable 3: methodological options to estimate \\(\\psi\\)\n\n\n\n\n\n\n\n\n\n\nOption\n\\(\\mathcal{M}_1\\)\n\\(\\mathcal{M}_2\\)\n\\(\\mathcal{M}_2\\) dim. reduction\nEasy to implement\nStat./TMLE friendly\n\n\n\n\n\\(Y\\)-regression and full \\(M\\)-model (Tchetgen and Shpitser 2012)\n\\(\\hat{Q}(H,A,M)\\)\n\\(\\hat{p}(M\\mid H,A)\\)\nNo\nSmall \\(M\\)\nYes\n\n\n\\(Y\\)-regression and \\(M\\)-reduction (Z. Xu et al. 2023; Nath et al. 2023)\n\\(\\hat{Q}(H,A,M')\\)\n\\(M'=\\Phi(M)\\) \\(\\hat{p}(M'\\mid H,A)\\)\nYes\nKinda\nMisspec!\n\n\nNested regressions (S. Xu, Liu, and Liu 2022)\n\\(\\hat{Q}(H,A,M)\\)\n\\(\\hat{Q}\\sim H,A\\)\nNo\nYes\nMaybe\n\n\n\n\n\nMultiply-robustness\nCombining IPW and regression adjustment solutions produces multiply-robustness; more specifically triply- in this case. An estimator for the ATE can be constructed such that is consistent if all semiparametric models involed are correctly specified, or in one of these scenarios:\n\n\\(M\\), \\(Y\\) are all well specified\n\\(A\\), \\(M\\), \\(R_Y\\) are all well specified\n\\(A\\), \\(Y\\), \\(R_Y\\) are all well specified\n\nNotice that, using a misspecified model for \\(M\\) (like when reducing its dimension with PCA) leaves the estimator worse than simply using IPW, as it requires correct specification of \\(A\\), \\(Y\\), \\(R_Y\\), whereas IPW only requires \\(A\\), \\(R_Y\\)"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#simulations",
    "href": "posts/2023-08-15_missingSimulation/index.html#simulations",
    "title": "XXX",
    "section": "Simulations",
    "text": "Simulations\nCheck this"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#footnotes",
    "href": "posts/2023-08-15_missingSimulation/index.html#footnotes",
    "title": "XXX",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(m\\)-graphs generalize causal graphs in settings with sample selection (Hernán, Hernández-Díaz, and Robins 2004) and missing data (Mohan and Pearl 2021).↩︎\nThe back-door graph \\(\\mathcal{G}'[A\\!-\\!Y]\\) is the graph resulting from removing in \\(\\mathcal{G}'\\) the first arrow of all directed paths from \\(A\\) to \\(Y\\). It is termed the proper back-door graph in multi-exposure settings, and results from removing the first arrow of all directed and non-self-intersecting paths from \\(A\\) to \\(Y\\).↩︎\nRecoverability might be possible without identifiability in the substantive model, via (fairly complicated) \\(c\\)-factorizations (J. D. Correa, Tian, and Bareinboim 2019) or \\(\\phi\\)-factorizations (Bhattacharya et al. 2020) related to the problem of \\(g\\)-identifiability↩︎\nLiterature review based on Seaman and White (2013), Dong and Peng (2013), Perkins et al. (2017), Lewin et al. (2018)↩︎"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#objective",
    "href": "posts/2023-08-15_missingSimulation/index.html#objective",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Objective",
    "text": "Objective\nThe final goal is to estimate the effects of ADHD treatment on school performance, where:\n\nthe estimands: average treatment effect (ATE) and conditional average treatment effect (CATE)\nthe exposure: stimulant medication for ADHD\nthe outcome: raw score at the mathematics national test at grade 8\nthe target population: ADHD-diagnosed Norwegian schoolchildren between grades 6 and 8\n\nFor this task, we employ:\n\nThe structural causal models (SCM) framework (Pearl 2009)\nA synthetic observational dataset, generated by an SCM learnt from real-world data (functional mechanisms are treated as unknown in the analysis)\nA back-door admissible set of pre-treatment variables; i.e., the assumption of no latent confounding\nA missing-outcome mechanism that allows recoverability via IPW and regression adjustment (functional specification is unknown in the analysis)\nThe targeted minimum-loss estimation (TMLE) framework (van der Laan and Rose 2011)"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#using-synthetic-data",
    "href": "posts/2023-08-15_missingSimulation/index.html#using-synthetic-data",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Using synthetic data",
    "text": "Using synthetic data\n\n\nCode\n# Load packages --------------------------------------------------------------\nlibrary(readr)        # Reads CSV      \nlibrary(data.table)   # Processes dataframes\nlibrary(dplyr)        # Processes dataframes\nlibrary(kableExtra)   # Styles tables\nlibrary(sl3)          # Performs super-learning\nlibrary(tmle3)        # Performs TMLE\nlibrary(tmle3mediate) # Performs TMLE for mediation analysis\n\n# Read data -------------------------------------------------------------------\nload(file=\"syntheticADHDdata.RData\")\n\n# Glimpse of the data --------------------------------------------------------\nfinal.data %&gt;% head() %&gt;% kable()\n\n\n\n\n\nmom.vuln\nsex.girl\nmed.pre6\npre.diag\nntr.grd5\nreg.gpsp\nmed.6to8\nptr.diag\nptr.gpsp\nntr.grd8\nptr.diag.cntr\nptr.gpsp.cntr\nntr.grd8.cntr\nITE\n\n\n\n\n1.33\n0\n0\n0\n13.63\n29\n0\n0\n11\n13.68\n0\n10\n14.69\n1.01\n\n\n1.34\n1\n1\n0\n24.24\n160\n0\n0\n35\n10.30\n0\n29\n11.08\n0.78\n\n\n3.79\n0\n1\n1\n23.71\n57\n0\n1\n38\n13.74\n0\n12\n15.04\n1.30\n\n\n4.26\n1\n0\n1\n12.86\n64\n0\n1\n46\n20.81\n1\n46\n21.12\n0.31\n\n\n2.24\n0\n1\n0\n20.12\n44\n1\n0\n11\n22.69\n0\n10\n21.77\n0.92\n\n\n1.93\n0\n0\n1\n10.63\n10\n0\n0\n2\n18.98\n0\n8\n19.74\n0.76\n\n\n\n\n\n\n\nPre-treatment variables and exposure:\n\nmom.vuln = \\(H_1\\in\\mathbb{R}\\) = maternal vulnerability index: a PCA-based index summarizing mother’s diagnoses (+), level of education (-), age (-), and number of children (-)\nsex.girl = \\(H_2\\in\\{0,1\\}\\) = child’s sex at birth, female = 1\nmed.pre6 = \\(H_3\\in\\{0,1\\}\\) = prescription for ADHD medication before grade 6\npre.diag = \\(H_4\\in\\{0,1\\}\\) = diagnoses for comorbid disorders (ADHD-related, internalizing or externalizing) registered before grade 6\nntr.grd5 = \\(H_5\\in\\mathbb{R}\\) = raw points obtained at the national test for grade 5 (average math and reading)\nreg.gpsp = \\(H_6\\in\\mathbb{N}\\) = number of registrations for health services (GP and specialist) before grade 6\nmed.6to8 = \\(A\\) = prescription for ADHD medication between grades 6-8\n\nMediator variables and outcome:\n\nptr.diag = \\(M_1\\in\\{0,1\\}\\) = diagnoses for comorbid disorders (ADHD-related, internalizing or externalizing) registered between grades 6-8\nptr.gpsp = \\(M_2\\in\\mathbb{N}\\) = number of registrations for health services (GP and specialist) between grades 6-8\nntr.grd8 = \\(Y\\) = raw points obtained at the national test for grade 5 (math)\n\nCounterfactual variables:\n\nptr.diag.cntr = \\(M_1^{A=1-a}\\) = diagnoses for comorbid disorders (ADHD-related, internalizing or externalizing) registered between grades 6-8, had the individual taken the opposite treatment\nptr.gpsp.cntr = \\(M_2^{A=1-a}\\) = number of registrations for health services (GP and specialist) between grades 6-8, had the individual taken the opposite treatment\nntr.grd8.cntr = \\(Y^{A=1-a}\\) = raw points obtained at the national test for grade 5 (math)\nITE = \\(\\Delta_aY^{A=a}\\) = individual treatment effect"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#synthetic-data",
    "href": "posts/2023-08-15_missingSimulation/index.html#synthetic-data",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Synthetic data",
    "text": "Synthetic data\nLet \\(\\mathcal{G}'\\) be a directed acyclic graph (DAG) built from domain knowledge and temporal-order constraints, involving the exposure \\(A\\), the outcome \\(Y\\), a set of confounders \\(H\\), and mediators \\(M\\). We previously fit flexible models (random forests) to learn the causal mechanisms \\(\\hat{f}_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G}')\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\) from real-world data on the subject. This allows us to replicate such mechanisms to generate fake data from seeds (noises and exogenous variables) in a controlled environment, along with counterfactual outcomes. In this exercise, exogenous variables mimic the marginal distribution of their real-world counterpart, but not the joint distribution.\n\nBy design, conditional ignorability is satisfied in the synthetic system using the full confounder set. It might not be satisfied in the system where the real-world data come from.\n\nGenerated variables can be grouped in three categories:\n1. Pre-treatment / exogenous variables and the exposure:\n\nmom.vuln = \\(H_1\\in\\mathbb{R}\\) = maternal vulnerability index: a PCA-based index summarizing mother’s diagnoses (+), level of education (-), age (-), and number of children (-)\nsex.girl = \\(H_2\\in\\{0,1\\}\\) = child’s sex at birth, female = 1\nmed.pre6 = \\(H_3\\in\\{0,1\\}\\) = prescription for ADHD medication before grade 6\npre.diag = \\(H_4\\in\\{0,1\\}\\) = diagnoses for comorbid disorders (ADHD-related, internalizing or externalizing) registered before grade 6\nntr.grd5 = \\(H_5\\in\\mathbb{R}\\) = raw score obtained at the national test for grade 5 (average math and reading)\nreg.gpsp = \\(H_6\\in\\mathbb{N}\\) = number of registrations for health services (GP and specialist) before grade 6\nmed.6to8 = \\(A\\) = prescription for ADHD medication between grades 6-8\n\n2. Mediator variables and the outcome:\n\nptr.diag = \\(M_1\\in\\{0,1\\}\\) = diagnoses for comorbid disorders (ADHD-related, internalizing or externalizing) registered between grades 6-8\nptr.gpsp = \\(M_2\\in\\mathbb{N}\\) = number of registrations for health services (GP and specialist) between grades 6-8\nntr.grd8 = \\(Y\\) = raw score obtained at the national test for grade 8 (math)\n\n3. Counterfactual variables:\n\nptr.diag.cntr = \\(M_1^{1-A}\\) = diagnoses for comorbid disorders (ADHD-related, internalizing or externalizing) registered between grades 6-8, had the individual taken the opposite treatment\nptr.gpsp.cntr = \\(M_2^{1-A}\\) = number of registrations for health services (GP and specialist) between grades 6-8, had the individual taken the opposite treatment\nntr.grd8.cntr = \\(Y^{1-A}\\) = raw score obtained at the national test for grade 5 (math)\nITE = \\(Y^{1}-Y^{0}\\) = individual treatment effect\n\nA dataset (synth.data) of \\(N=5\\,000\\) samples was generated:\n\n\nCode\n# Load packages --------------------------------------------------------------\nlibrary(data.table)   # Processes dataframes\nlibrary(dplyr)        # Processes dataframes\nlibrary(kableExtra)   # Styles tables\nlibrary(speedglm)     # Performs fast fitting for GLM\nlibrary(ranger)       # Performs random forest learning\nlibrary(nnls)         # Performs non-negative least squares\nlibrary(Rsolnp)       # Augmented Lagrange optimizer\nlibrary(sl3)          # Performs super-learning\nlibrary(tmle3)        # Performs TMLE\nlibrary(tmle3mediate) # Performs TMLE for mediation analysis\nlibrary(ggplot2)      # Plots\n\n# Read data -------------------------------------------------------------------\nload(file=\"syntheticADHDdata.RData\")\n\n# Glimpse of the data ---------------------------------------------------------\nsynth.data = data.table(synth.data)\nsynth.data %&gt;% head() %&gt;% kable(caption = \"Table 1: A glimpse at the synthetic data\")\n\n\n\nTable 1: A glimpse at the synthetic data\n\n\nmom.vuln\nsex.girl\nmed.pre6\npre.diag\nntr.grd5\nreg.gpsp\nmed.6to8\nptr.diag\nptr.gpsp\nntr.grd8\nptr.diag.cntr\nptr.gpsp.cntr\nntr.grd8.cntr\nITE\n\n\n\n\n0.65\n0\n1\n0\n19.64\n14\n0\n0\n42\n16.77\n0\n44\n19.29\n2.52\n\n\n1.31\n1\n1\n0\n25.85\n1\n1\n0\n21\n26.72\n0\n19\n21.64\n5.08\n\n\n0.08\n0\n1\n1\n19.59\n56\n0\n0\n7\n21.38\n1\n46\n23.27\n1.89\n\n\n0.65\n0\n1\n1\n15.12\n23\n1\n1\n0\n17.62\n1\n0\n14.48\n3.14\n\n\n0.99\n1\n0\n1\n9.30\n27\n0\n0\n4\n15.02\n1\n22\n15.16\n0.14\n\n\n2.02\n0\n0\n1\n19.83\n128\n0\n1\n47\n19.62\n1\n75\n20.18\n0.56"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#oracle-analysis",
    "href": "posts/2023-08-15_missingSimulation/index.html#oracle-analysis",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Oracle analysis",
    "text": "Oracle analysis\nSynthetic data from a SCM allow the computation of unit-level counterfactuals: we can see both worlds at the same time.\n\n\nCode\n# Function to generate C.I. data -----------------------------------------------\n# Param: pre-treatment binary variable to stratify\n# Return: data ready to plot as confidence intervals\nget.ci.data = function(varname){\n  \n  # Temporal data.frame\n  dplot1 = synth.data\n  dplot1$dummy = dplot1[,get(varname)]\n    \n  # Two-sample t-test: biased! --------------------------------------------------\n  ttest.pop = t.test(dplot1[med.6to8==1, ntr.grd8], # all, med vs no-med\n                     dplot1[med.6to8==0, ntr.grd8]) \n  ttest.grl = t.test(dplot1[med.6to8==1 & dummy==1, ntr.grd8], # subpop 1, med vs no-med\n                     dplot1[med.6to8==0 & dummy==1, ntr.grd8])\n  ttest.boy = t.test(dplot1[med.6to8==1 & dummy==0, ntr.grd8], # subpop 0, med vs no-med\n                     dplot1[med.6to8==0 & dummy==0, ntr.grd8])\n\n  # One-sample t-test: oracle! --------------------------------------------------\n  otest.pop = t.test(dplot1[, ITE])            # oracle ITE, all\n  otest.grl = t.test(dplot1[dummy==1, ITE])    # oracle ITE, subpop 1\n  otest.boy = t.test(dplot1[dummy==0, ITE])    # oracle ITE, subpop 0\n  \n  dplot1 = data.table( \n    type = rep(c('biased','oracle'), each=3),               # label for type of analysis\n    subpop = rep(c('all','subpop 1','subpop 0'), 2),        # label for supopulation\n    low = c(ttest.pop$conf.int[1], ttest.grl$conf.int[1], ttest.boy$conf.int[1],  # lower bound\n            otest.pop$conf.int[1], otest.grl$conf.int[1], otest.boy$conf.int[1]),\n    upr = c(ttest.pop$conf.int[2], ttest.grl$conf.int[2], ttest.boy$conf.int[2],  # upper bound\n            otest.pop$conf.int[2], otest.grl$conf.int[2], otest.boy$conf.int[2]))\n  \n  dplot1 = dplot1[, midp := (low+upr)/2] # add midpoint\n  return(dplot1)                         # return dataframe\n}\n\n# Plot confidence intervals ---------------------------------------------------\nrbind.data.frame(\n  cbind.data.frame(get.ci.data('sex.girl'),strata='sex at birth'),    # C.I. sex stratification\n  cbind.data.frame(get.ci.data('med.pre6'),strata='prior treatment'), # C.I. prior treatment\n  cbind.data.frame(get.ci.data('pre.diag'),strata='comorbidity')) %&gt;% # C.I. comorbid status\n  ggplot(aes(x = subpop, y = midp, colour = type)) +                  # Plot together!\n  geom_errorbar(aes(ymax = upr, ymin = low), position = \"dodge\") +\n  geom_point(position = position_dodge(0.9)) +\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"red\") +\n  labs(y='ATE estimate', title='Naive two-sample test vs oracle ATE estimate',\n       x='Subpopulation (1 = YES comorbid diagnoses, YES prior treatment, girls)') +\n  facet_wrap(~strata) + theme_classic()"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#naive-analysis-vs-oracle",
    "href": "posts/2023-08-15_missingSimulation/index.html#naive-analysis-vs-oracle",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Naive analysis vs Oracle",
    "text": "Naive analysis vs Oracle\nSynthetic data from a SCM allow the computation of unit-level counterfactuals: we can see both worlds at the same time.\n\n\nCode\n# Function to generate C.I. data -----------------------------------------------\n# Param: pre-treatment binary variable to stratify\n# Return: data ready to plot as confidence intervals\nget.ci.data = function(varname){\n  \n  # Temporal data.frame\n  dplot1 = synth.data\n  dplot1$dummy = dplot1[,get(varname)]\n    \n  # Two-sample t-test: biased! --------------------------------------------------\n  ttest.pop = t.test(dplot1[med.6to8==1, ntr.grd8], # all, med vs no-med\n                     dplot1[med.6to8==0, ntr.grd8]) \n  ttest.grl = t.test(dplot1[med.6to8==1 & dummy==1, ntr.grd8], # subpop 1, med vs no-med\n                     dplot1[med.6to8==0 & dummy==1, ntr.grd8])\n  ttest.boy = t.test(dplot1[med.6to8==1 & dummy==0, ntr.grd8], # subpop 0, med vs no-med\n                     dplot1[med.6to8==0 & dummy==0, ntr.grd8])\n\n  # One-sample t-test: oracle! --------------------------------------------------\n  otest.pop = t.test(dplot1[, ITE])               # oracle ITE, all\n  otest.grl = t.test(dplot1[dummy==1, ITE])    # oracle ITE, subpop 1\n  otest.boy = t.test(dplot1[dummy==0, ITE])    # oracle ITE, subpop 0\n  \n  dplot1 = data.table( \n    type = rep(c('biased','oracle'), each=3),               # label for type of analysis\n    subpop = rep(c('all','subpop 1','subpop 0'), 2),        # label for supopulation\n    low = c(ttest.pop$conf.int[1], ttest.grl$conf.int[1], ttest.boy$conf.int[1],  # lower bound\n            otest.pop$conf.int[1], otest.grl$conf.int[1], otest.boy$conf.int[1]),\n    upr = c(ttest.pop$conf.int[2], ttest.grl$conf.int[2], ttest.boy$conf.int[2],  # upper bound\n            otest.pop$conf.int[2], otest.grl$conf.int[2], otest.boy$conf.int[2]))\n  \n  dplot1 = dplot1[, midp := (low+upr)/2] # add midpoint\n  return(dplot1)                         # return dataframe\n}\n\n# Plot confidence intervals ---------------------------------------------------\nrbind.data.frame(\n  cbind.data.frame(get.ci.data('sex.girl'),strata='sex at birth'),           # C.I. sex stratification\n  cbind.data.frame(get.ci.data('med.pre6'),strata='prior treatment'), # C.I. prior treatment\n  cbind.data.frame(get.ci.data('pre.diag'),strata='comorbidity')) %&gt;% # C.I. comorbid status\n  ggplot(aes(x = subpop, y = midp, colour = type)) +                         # Plot together!\n  geom_errorbar(aes(ymax = upr, ymin = low), position = \"dodge\") +\n  geom_point(position = position_dodge(0.9)) +\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"red\") +\n  labs(y='ATE estimate', title='Naive two-sample test vs oracle ATE estimate',\n       x='Subpopulation (1 = YES comorbid diagnoses, YES prior treatment, girls)') +\n  facet_wrap(~strata) + theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCode\n1-1\n\n\n[1] 0"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#naïve-analysis-vs-oracle",
    "href": "posts/2023-08-15_missingSimulation/index.html#naïve-analysis-vs-oracle",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Naïve analysis vs Oracle",
    "text": "Naïve analysis vs Oracle\nSynthetic data from a SCM allow the computation of unit-level counterfactuals: we can see both worlds at the same time.\n\n\nCode\n# Function to generate C.I. data -----------------------------------------------\n# Param: pre-treatment binary variable to stratify\n# Return: data ready to plot as confidence intervals\nget.ci.data = function(varname){\n  \n  # Temporal data.frame\n  dplot1 = synth.data\n  dplot1$dummy = dplot1[,get(varname)]\n    \n  # Two-sample t-test: biased! --------------------------------------------------\n  ttest.pop = t.test(dplot1[med.6to8==1, ntr.grd8], # all, med vs no-med\n                     dplot1[med.6to8==0, ntr.grd8]) \n  ttest.grl = t.test(dplot1[med.6to8==1 & dummy==1, ntr.grd8], # subpop 1, med vs no-med\n                     dplot1[med.6to8==0 & dummy==1, ntr.grd8])\n  ttest.boy = t.test(dplot1[med.6to8==1 & dummy==0, ntr.grd8], # subpop 0, med vs no-med\n                     dplot1[med.6to8==0 & dummy==0, ntr.grd8])\n\n  # One-sample t-test: oracle! --------------------------------------------------\n  otest.pop = t.test(dplot1[, ITE])               # oracle ITE, all\n  otest.grl = t.test(dplot1[dummy==1, ITE])    # oracle ITE, subpop 1\n  otest.boy = t.test(dplot1[dummy==0, ITE])    # oracle ITE, subpop 0\n  \n  dplot1 = data.table( \n    type = rep(c('biased','oracle'), each=3),               # label for type of analysis\n    subpop = rep(c('all','subpop 1','subpop 0'), 2),        # label for supopulation\n    low = c(ttest.pop$conf.int[1], ttest.grl$conf.int[1], ttest.boy$conf.int[1],  # lower bound\n            otest.pop$conf.int[1], otest.grl$conf.int[1], otest.boy$conf.int[1]),\n    upr = c(ttest.pop$conf.int[2], ttest.grl$conf.int[2], ttest.boy$conf.int[2],  # upper bound\n            otest.pop$conf.int[2], otest.grl$conf.int[2], otest.boy$conf.int[2]))\n  \n  dplot1 = dplot1[, midp := (low+upr)/2] # add midpoint\n  return(dplot1)                         # return dataframe\n}\n\n# Plot confidence intervals ---------------------------------------------------\nrbind.data.frame(\n  cbind.data.frame(get.ci.data('sex.girl'),strata='sex at birth'),           # C.I. sex stratification\n  cbind.data.frame(get.ci.data('med.pre6'),strata='prior treatment'), # C.I. prior treatment\n  cbind.data.frame(get.ci.data('pre.diag'),strata='comorbidity')) %&gt;% # C.I. comorbid status\n  ggplot(aes(x = subpop, y = midp, colour = type)) +                         # Plot together!\n  geom_errorbar(aes(ymax = upr, ymin = low), position = \"dodge\") +\n  geom_point(position = position_dodge(0.9)) +\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"red\") +\n  labs(y='ATE estimate', title='Naive two-sample test vs oracle ATE estimate',\n       x='Subpopulation (1 = YES comorbid diagnoses, YES prior treatment, girls)') +\n  facet_wrap(~strata) + theme_classic()\n\n\n\n\n\n\n\n\n\n\n\nCode\n1-1\n\n\n[1] 0"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#no-missingness",
    "href": "posts/2023-08-15_missingSimulation/index.html#no-missingness",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "No missingness",
    "text": "No missingness\nSynthetic data from a SCM allow the computation of unit-level counterfactuals: we can see both worlds at the same time.\n\n\nCode\n# Function to generate C.I. data from t-tests ---------------------------------\n# Param: pre-treatment binary variable to stratify\n# Return: data ready to plot as confidence intervals\n\nget.ci.data = function(varname){\n  \n  # Temporal data.frame\n  dplot1 = synth.data\n  dplot1$dummy = dplot1[,get(varname)]\n    \n  # Two-sample t-test: biased! -------------------\n  \n  # all, med vs no-med\n  ttest.pop = t.test(dplot1[med.6to8==1, ntr.grd8], \n                     dplot1[med.6to8==0, ntr.grd8]) \n  # subpop 0, med vs no-med\n  ttest.sp0 = t.test(dplot1[med.6to8==1 & dummy==0, ntr.grd8], \n                     dplot1[med.6to8==0 & dummy==0, ntr.grd8])\n  # subpop 1, med vs no-med\n  ttest.sp1 = t.test(dplot1[med.6to8==1 & dummy==1, ntr.grd8], \n                     dplot1[med.6to8==0 & dummy==1, ntr.grd8])\n\n  # One-sample t-test: oracle! -------------------\n  \n  # oracle ITE, all\n  otest.pop = t.test(dplot1[, ITE])            \n  # oracle ITE, subpop 0\n  otest.sp0 = t.test(dplot1[dummy==0, ITE])    \n  # oracle ITE, subpop 1\n  otest.sp1 = t.test(dplot1[dummy==1, ITE])    \n  \n  # data frame put together ----------------------\n  \n  dplot1 = data.table( \n    # label for type of analysis\n    type = rep(c('unadjusted','oracle SATE'), each=3),\n    # label for supopulation\n    subpop = rep(c('all','subpop 0','subpop 1'), 2),        \n    # lower bound\n    low = c(ttest.pop$conf.int[1], ttest.sp0$conf.int[1], ttest.sp1$conf.int[1],  \n            otest.pop$conf.int[1], otest.sp0$conf.int[1], otest.sp1$conf.int[1]),\n    # upper bound\n    upr = c(ttest.pop$conf.int[2], ttest.sp0$conf.int[2], ttest.sp1$conf.int[2],  \n            otest.pop$conf.int[2], otest.sp0$conf.int[2], otest.sp1$conf.int[2]))\n  \n  # add midpoint and return dataframe\n  dplot1 = dplot1[, midp := (low+upr)/2] \n  return(dplot1)                          \n}\n\n# Specified super learners for TMLE --------------------------------------------\n\n# Learning algorithms employed to learn treatment/outcome mechanisms\n\n# Linear model\nlrnr_lm = make_learner(Lrnr_glm_fast)\n# Random forest model\nlrnr_rf = make_learner(Lrnr_ranger)      \n\n# Meta-learners: to stack together predictions from the learners\n\n# Combine Y predictions with non-negative least squares\nmeta_Y = make_learner(Lrnr_nnls)   \n\n# Combine A predictions with logit likelihood (augmented Lagrange optimizer)\nmeta_A = make_learner(Lrnr_solnp,                     \n  loss_function = loss_loglik_binomial,              \n  learner_function = metalearner_logistic_binomial)   \n\n\n# Super-learners: learners + meta-learners together\n\n# Outcome super-learning\nsuper_Y = Lrnr_sl$new(learners = list(lrnr_lm, lrnr_rf), \n                      metalearner = meta_Y)\n# Exposure super-learning\nsuper_A = Lrnr_sl$new(learners = list(lrnr_lm, lrnr_rf), \n                      metalearner = meta_A)\n# Super-learners put together\nsuper_list = list(A = super_A, Y = super_Y)              \n\n# Confounder set labels\nH = colnames(synth.data[,1:6])\n\n# Function to generate C.I. data from TMLE---------------------------------------------\n# Param: pre-treatment binary variable to stratify\n# Return: data from confidence intervals\n\naux_tml_method = function(namevar){\n  tmle_CATE = tmle3(\n  tmle_spec = tmle_stratified(tmle_ATE(1,0)), # Causal contrast CATE: Y1-Y0\n  node_list = list(                           # Variables involved in the query\n    W = setdiff(H,namevar),                   # Label for non-stratum confounders\n    V = namevar,                              # Label for stratifying variable\n    A = \"med.6to8\",                           # Label for exposure\n    Y = \"ntr.grd8\"),                          # Label for outcome\n  data = synth.data,                          # Data\n  learner_list = super_list)                  # Super-learners specified\n  return(tmle_CATE$summary)                   # Return summary\n}\n\n# Generate data for C.I. on the ATE/CATE ---------------------------------------\n\n# t-test on previous treatment\nttest.med = cbind.data.frame(get.ci.data('med.pre6'), strata='prior treatment') \n# t-test on sex strata\nttest.sex = cbind.data.frame(get.ci.data('sex.girl'), strata='sex at birth')\n# TMLE on sex strata\ntlme.med = cbind.data.frame(type='TMLE', subpop=c('all','subpop 0','subpop 1'),\n                            tml.cate.med[order(tml.cate.med$param),c(9,10,8)],\n                            strata='prior treatment')\n# TMLE on sex strata\ntlme.sex = cbind.data.frame(type='TMLE', subpop=c('all','subpop 0','subpop 1'),\n                            tml.cate.sex[order(tml.cate.sex$param),c(9,10,8)],\n                            strata='sex at birth')\n# Unify column names\ncolnames(tlme.med) = colnames(ttest.med)\ncolnames(tlme.sex) = colnames(ttest.sex)\n\n# Put all together\nci.data = rbind.data.frame(ttest.med, ttest.sex, tlme.med, tlme.sex)\n\n\n\n\nCode\n# Plot confidence intervals ---------------------------------------------------\nci.data %&gt;% \n  ggplot(aes(x = subpop, y = midp, colour = type)) +                    \n  geom_errorbar(aes(ymax = upr, ymin = low), position = \"dodge\") +\n  geom_point(position = position_dodge(0.9)) +\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"red\") +\n  labs(y='ATE estimate', title='Unadjusted two-sample test vs oracle ATE estimate',\n       x='Subpopulation (1 = YES comorbid diagnoses, YES prior treatment, girls)') +\n  facet_wrap(~strata) + theme_classic()"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#no-missingness-case",
    "href": "posts/2023-08-15_missingSimulation/index.html#no-missingness-case",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "No missingness case",
    "text": "No missingness case\nA known SCM allows the computation of unit-level counterfactuals: we can see both worlds at the same time. In particular, we can compute the oracle sample average treatment effect (\\(^S\\)ATE): the within-sample average of the individual treatment effects (unobserved in real-world data). \\[\n^S\\text{ATE} = N^{-1}\\sum_{i=1}^N \\text{ITE}^{(i)} = N^{-1}\\sum_{i=1}^N (Y^{(i)}_{A=1}-Y^{(i)}_{A=0})\n\\] The \\(^S\\)ATE is itself a consistent (but impossible) estimator of the population ATE (\\(^P\\)ATE).\nLikewise, we can compute the oracle \\(^S\\)CATE for \\(X\\)-specific effects. If \\(X\\) is categorical, we can partition units into strata: \\(I(x):=\\{i\\in 1\\dots N : X^{(i)}=x\\}\\). \\[\n^S\\text{CATE}(x) = |I(x)|^{-1}\\sum_{i\\in I(x)} \\text{ITE}^{(i)} = |I(x)|^{-1}\\sum_{i\\in I(x)} (Y^{(i)}_{A=1}-Y^{(i)}_{A=0})\n\\]\nLet us compare the oracle effects against the results from two approaches:\n\n\\(t\\)-tests comparing the means of raw test scores between treated and non-treated. This estimator would be biased due to confounding\nThe TMLE estimator using super-learners (Phillips et al. 2023), combining:\n\nflexible libraries of base-learners to model \\(A\\) and \\(Y\\) given confounders \\(H\\). We employ linear models and random forests (ground-truth DGP is in this model-space)\nmeta-learners: to ensemble the base-learners together, weighted by their out-of-sample (CV) predictive measures to avoid over-fitting (logit for \\(A\\) and NNLS for \\(Y\\))\n\n\n\n\nCode\n###############################################################################\n# Solution via t-tests (biased by confounding)\n###############################################################################\n\n# Function to generate C.I. data from t-tests ---------------------------------\n# Param: pre-treatment binary variable to stratify\n# Return: data ready to plot as confidence intervals\n\nget.ci.data = function(varname){\n  \n  # Temporal data.frame\n  dplot1 = synth.data\n  dplot1$dummy = dplot1[,get(varname)]\n    \n  # Two-sample t-test: biased! -------------------\n  \n  # all, med vs no-med\n  ttest.pop = t.test(dplot1[med.6to8==1, ntr.grd8], # treated\n                     dplot1[med.6to8==0, ntr.grd8]) # control\n  # subpop 0, med vs no-med\n  ttest.sp0 = t.test(dplot1[med.6to8==1 & dummy==0, ntr.grd8], # treated (sp 0)\n                     dplot1[med.6to8==0 & dummy==0, ntr.grd8]) # control (sp 0)\n  # subpop 1, med vs no-med\n  ttest.sp1 = t.test(dplot1[med.6to8==1 & dummy==1, ntr.grd8], # treated (sp 1)\n                     dplot1[med.6to8==0 & dummy==1, ntr.grd8]) # control (sp 1)\n\n  # One-sample t-test: oracle! -------------------\n  \n  # oracle ITE, all\n  otest.pop = t.test(dplot1[, ITE])            \n  # oracle ITE, subpop 0\n  otest.sp0 = t.test(dplot1[dummy==0, ITE])    \n  # oracle ITE, subpop 1\n  otest.sp1 = t.test(dplot1[dummy==1, ITE])    \n  \n  # data frame put together ----------------------\n  \n  dplot1 = data.table( \n    # label for type of analysis\n    type = rep(c('t-test','oracle'), each=3),\n    # label for supopulation\n    subpop = rep(c('all','subpop 0','subpop 1'), 2),        \n    # lower bound\n    low = c(ttest.pop$conf.int[1], ttest.sp0$conf.int[1], ttest.sp1$conf.int[1],  \n            otest.pop$conf.int[1], otest.sp0$conf.int[1], otest.sp1$conf.int[1]),\n    # upper bound\n    upr = c(ttest.pop$conf.int[2], ttest.sp0$conf.int[2], ttest.sp1$conf.int[2],  \n            otest.pop$conf.int[2], otest.sp0$conf.int[2], otest.sp1$conf.int[2]))\n  \n  # add midpoint and return dataframe\n  dplot1 = dplot1[, midp := (low+upr)/2] \n  return(dplot1)                          \n}\n\n###############################################################################\n# Solution via adjusted regression / TMLE\n###############################################################################\n\n# Learning algorithms employed to learn treatment/outcome mechanisms -----------\n\n# Linear model\nlrnr_lm = make_learner(Lrnr_glm_fast)\n# Random forest model\nlrnr_rf = make_learner(Lrnr_ranger)      \n\n# Meta-learners: to stack together predictions from the learners ---------------\n\n# Combine Y predictions with non-negative least squares\nmeta_Y = make_learner(Lrnr_nnls)   \n\n# Combine A predictions with logit likelihood (augmented Lagrange optimizer)\nmeta_A = make_learner(Lrnr_solnp,                     \n  loss_function = loss_loglik_binomial,              \n  learner_function = metalearner_logistic_binomial)   \n\n# Super-learners: learners + meta-learners together ----------------------------\n\n# Outcome super-learning\nsuper_Y = Lrnr_sl$new(learners = list(lrnr_lm, lrnr_rf), \n                      metalearner = meta_Y)\n# Exposure super-learning\nsuper_A = Lrnr_sl$new(learners = list(lrnr_lm, lrnr_rf), \n                      metalearner = meta_A)\n# Super-learners put together\nsuper_list = list(A = super_A,\n                  Y = super_Y)              \n\n# Confounder set labels\nH = colnames(synth.data[,1:6])\n\n# Function to generate C.I. data from TMLE--------------------------------------\n# Param: pre-treatment binary variable to stratify\n# Param: data\n# Return: data from confidence intervals\n\naux_tml_method = function(namevar, data=synth.data){\n  tmle_CATE = tmle3(\n  tmle_spec = tmle_stratified(tmle_ATE(1,0)), # Causal contrast CATE: Y1-Y0\n  node_list = list(                           # Variables involved in the query\n    W = setdiff(H,namevar),                   # Label for non-stratum confounders\n    V = namevar,                              # Label for stratifying variable\n    A = \"med.6to8\",                           # Label for exposure\n    Y = \"ntr.grd8\"),                          # Label for outcome\n  data = data,                                # Data\n  learner_list = super_list)                  # Super-learners specified\n  return(tmle_CATE$summary)                   # Return summary\n}\n\n###############################################################################\n# Generate confidence intervals to plot\n###############################################################################\n\n# t-test on prior treatment\nttest.med = cbind.data.frame(get.ci.data('med.pre6'),  # C.I. data\n                             strata='prior treatment') # Strata variable\n\n# t-test on sex strata\nttest.sex = cbind.data.frame(get.ci.data('sex.girl'), # C.I. data\n                             strata='sex at birth')   # Strata variable\n\n# TMLE on prior treatment\ntml.cate.med = aux_tml_method('med.pre6')                                       # TMLE results\ntlme.med = cbind.data.frame(type='TMLE',                                        # method label\n                            subpop=c('all','subpop 0','subpop 1'),              # subpop label\n                            tml.cate.med[order(tml.cate.med$param),c(9,10,8)],  # C.I. data\n                            strata='prior treatment')                           # Strata variable\n# TMLE on sex strata\ntml.cate.sex = aux_tml_method('sex.girl')                                       # TMLE results\ntlme.sex = cbind.data.frame(type='TMLE',                                        # method label\n                            subpop=c('all','subpop 0','subpop 1'),              # subpop label\n                            tml.cate.sex[order(tml.cate.sex$param),c(9,10,8)],  # C.I. data\n                            strata='sex at birth')                              # Strata variable\n\n# Unify column names\ncolnames(tlme.med) = colnames(ttest.med)\ncolnames(tlme.sex) = colnames(ttest.sex)\n\n# Put all together\nci.data.1 = rbind.data.frame(ttest.med, ttest.sex, tlme.med, tlme.sex)\n\n\nThe following plot presents the point-estimate and 95% confidence interval for the ATE/CATE resulting from oracle \\(^S\\)ATE/\\(^S\\)CATE, \\(t\\)-tests, and TMLE.\n\n\nCode\n# Plot confidence intervals ---------------------------------------------------\nci.data.1 %&gt;% \n  ggplot(aes(x = subpop, y = midp, colour = type)) +                    \n  geom_errorbar(aes(ymax = upr, ymin = low), position = \"dodge\") +\n  geom_point(position = position_dodge(0.9)) +\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"red\") +\n  labs(y='ATE/CATE estimate',\n       x='Subpopulation (1 = yes prior treatment, 1 = girls)') +\n  facet_wrap(~strata) + theme_classic()\n\n\n\n\n\n\n\n\n\nFrom the previous plot we can infer:\n1. There is some amount of confounding\n\nConfidence intervals from \\(t\\)-tests (green) do not always cover the sample-based oracle value (red). This is probably due to confounding bias, as \\(t\\)-test do not adjust for covariation with \\(H\\)\nIn general, confounding bias is negative: unadjusted effects are below the oracle result\nConfounding bias seems to be particularly problematic when stratifying on prior prescription for ADHD medication, which might be a strong predictor of the exposure\n\\(t\\)-test results might lead to conclude that the treatment is actually deleterious for those who had not started it before grade 6 (because of late diagnosis?). In fact, the treatment has positive, but extremely small, average effect in such subpopulation\n\\(t\\)-test results might also lead to conclude that the treatment effect is insignificant for boys. In fact, the effect on them is small but positive\n\n2. TMLE with covariate adjustment\n\nUnder some mild condition including back-door admissibility of \\(H\\), TMLE produces consistent estimators. This is, the amount of confounding bias becomes negligible in large finite samples. We can see that confidence intervals from TMLE (blue) cover the sample-based oracle value (red) in all cases\nTMLE results lead to correct interpretation of treatment effect for boys and the subpopulation who did not start treatment before grade 6\n\n3. In general, the effects of ADHD treatment on school performance are positive but small\n\nThe estimate ATE is 1.7, on an outcome with mean 21.0 and standard deviation 7.0"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#missingness-mechanisms",
    "href": "posts/2023-08-15_missingSimulation/index.html#missingness-mechanisms",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nLet us simulate the \\(Y\\) missingness mechanism as a binary random variable \\(R_Y\\), childless in the associated \\(m\\)-DAG. For a particular individual \\(i\\), \\(R_Y^{(i)}=1\\), indicates that its respective outcome realization \\(Y^{(i)}\\) is observed, and \\(R_Y^{(i)}=0\\) signifies \\(Y^{(i)}\\) is missing.\n\n\nCode\n# Building an endogenous selection/missingness mechanism -----------------------\n\n# Parameter for the glm-probit specification, only main effects\nset.seed(9)\nnoise.R = rnorm(n=5e3, 0, 1)\ntheta.i = 3.77 \ntheta.c = c(-0.01, -1.97, -2.03, 0.18, -0.03, 0.01, -0.05, 0.64, -0.01)\nlin.pred = as.matrix(synth.data[,1:9]) %*% theta.c + theta.i + noise.R\n\n# Realization of missingness mechanism\nsynth.data$RY = ifelse(lin.pred &gt; 0, 1, 0)\n\n# Count how many selected / observed ----------------------------------------\ntable(synth.data$RY) %&gt;% \n  kable(col.names = c(\"R(Y)\",\"n\"),\n    caption = \"Table 2: Missing/selected unit-outcomes\",\n    table.attr = \"quarto-disable-processing=true\") %&gt;% kable_styling(full_width = FALSE)\n\n\n\n\nTable 2: Missing/selected unit-outcomes\n \n  \n    R(Y) \n    n \n  \n \n\n  \n    0 \n    751 \n  \n  \n    1 \n    4249 \n  \n\n\n\n\n\n\n\nCode\n# COMPLETE CASE ANALYSIS for C.I. on the ATE/CATE ------------------------------\n\n# TMLE on prior treatment\ntml.cate.med.cc = aux_tml_method('med.pre6', data=synth.data[RY==1,])\ntlme.med.cc = cbind.data.frame(type='TMLE-cc', subpop=c('all','subpop 0','subpop 1'),\n                            tml.cate.med.cc[order(tml.cate.med.cc$param),c(9,10,8)],\n                            strata='prior treatment')\n# TMLE on sex strata\ntml.cate.sex.cc = aux_tml_method('sex.girl', data=synth.data[RY==1,])\ntlme.sex.cc = cbind.data.frame(type='TMLE-cc', subpop=c('all','subpop 0','subpop 1'),\n                            tml.cate.sex.cc[order(tml.cate.sex.cc$param),c(9,10,8)],\n                            strata='sex at birth')\n\n# MEDIAN-IMPUTATION for C.I. on the ATE/CATE ------------------------------\n\n\nobserved.data = synth.data\nobserved.data$ntr.grd8 = ifelse(observed.data$RY==0, NA, observed.data$ntr.grd8)\n\nprocessed = process_missing(observed.data,\n                            node_list = list(W = H,\n                                             A = \"med.6to8\",\n                                             Y = \"ntr.grd8\"),\n                            complete_nodes = c('W','A'),\n                            impute_nodes = 'Y')\n\n# TMLE on prior treatment\ntml.cate.med.im = aux_tml_method('med.pre6', data=processed$data)\ntlme.med.im = cbind.data.frame(type='TMLE-im', subpop=c('all','subpop 0','subpop 1'),\n                            tml.cate.med.im[order(tml.cate.med.im$param),c(9,10,8)],\n                            strata='prior treatment')\n# TMLE on sex strata\ntml.cate.sex.im = aux_tml_method('sex.girl', data=processed$data)\ntlme.sex.im = cbind.data.frame(type='TMLE-im', subpop=c('all','subpop 0','subpop 1'),\n                            tml.cate.sex.im[order(tml.cate.sex.im$param),c(9,10,8)],\n                            strata='sex at birth')\n\n# Unify column names\ncolnames(tlme.med.cc) = colnames(ttest.med)\ncolnames(tlme.sex.cc) = colnames(ttest.sex)\ncolnames(tlme.med.im) = colnames(ttest.med)\ncolnames(tlme.sex.im) = colnames(ttest.sex)\n\n# Put all together\nci.data.2 = rbind.data.frame(ttest.med[ttest.med$type=='oracle',],\n                             ttest.sex[ttest.sex$type=='oracle',],\n                             tlme.med.cc, tlme.sex.cc,\n                             tlme.med.im, tlme.sex.im)\n\n\nThe following plot presents the point-estimate and 95% confidence interval for the ATE/CATE resulting from oracle \\(^S\\)ATE/\\(^S\\)CATE, \\(t\\)-tests, and TMLE.\n\n\nCode\n# Plot confidence intervals ---------------------------------------------------\nci.data.2 %&gt;% \n  ggplot(aes(x = subpop, y = midp, colour = type)) +                    \n  geom_errorbar(aes(ymax = upr, ymin = low), position = \"dodge\") +\n  geom_point(position = position_dodge(0.9)) +\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"red\") +\n  labs(y='ATE/CATE estimate',\n       x='Subpopulation (1 = yes prior treatment, 1 = girls)') +\n  facet_wrap(~strata) + theme_classic()"
  },
  {
    "objectID": "posts/2023-08-15_missingSimulation/index.html#outcome-missingness-mechanisms",
    "href": "posts/2023-08-15_missingSimulation/index.html#outcome-missingness-mechanisms",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Outcome-missingness mechanisms",
    "text": "Outcome-missingness mechanisms\nLet us simulate the \\(Y\\)-missingness mechanism as a binary random variable \\(R_Y\\), childless in the associated \\(m\\)-graph \\(\\mathcal{G}\\). For a particular individual \\(i\\), \\(R_Y^{(i)}=1\\) indicates that its respective outcome realization \\(Y^{(i)}\\) is observed, while \\(R_Y^{(i)}=0\\) signifies \\(Y^{(i)}\\) is missing. This mechanism has a probit specification:\n\\[\nR_Y = \\mathbb{I}[\\theta_0 + \\theta_H^\\top H + \\theta_A A + \\theta_M^\\top M + U_R &gt; 0],\\quad U_R\\sim N(0,1)\n\\]\nFixing the parameters \\(\\theta\\) to certain value (unknown in the analysis), a realization of \\(U_R\\) produces 751 (15%) missing outcomes:\n\n\nCode\n###############################################################################\n# Building an endogenous selection/missingness mechanism\n###############################################################################\n\n# Parameter for the glm-probit specification, only main effects\nset.seed(9)\nnoise.R = rnorm(n=5e3, 0, 1)\ntheta.i = 3.77 \ntheta.c = c(-0.01, -1.97, -2.03, 0.18, -0.03, 0.01, -0.05, 0.64, -0.01)\nlin.pred = as.matrix(synth.data[,1:9]) %*% theta.c + theta.i + noise.R\n\n# Realization of missingness mechanism\nsynth.data$RY = ifelse(lin.pred &gt; 0, 1, 0)\n\n# Count how many selected / observed -------------------------------------------\ntable(synth.data$RY) %&gt;% \n  kable(col.names = c(\"R(Y)\",\"n\"),\n    caption = \"Table 2: Missing/selected unit-outcomes\",\n    table.attr = \"quarto-disable-processing=true\") %&gt;% \n  kable_styling(full_width = FALSE)\n\n\n\n\nTable 2: Missing/selected unit-outcomes\n \n  \n    R(Y) \n    n \n  \n \n\n  \n    0 \n    751 \n  \n  \n    1 \n    4249 \n  \n\n\n\n\n\nUnder the TMLE framework, let us first consider four options to recover the ATE/CATE:\n\nTMLE-cc: TMLE with complete cases. We discard entire units with \\(R_Y=0\\)\n\nThis approach would produce a consistent estimator under MCAR. However, as \\(R_Y\\) is MAR in our case, results would be biased\n\n\n\n\nCode\n###############################################################################\n# Complete-case analysis for TMLE\n###############################################################################\n\n# TMLE on prior treatment\ntml.cate.med.cc = aux_tml_method('med.pre6',                                            \n                                 data=synth.data[RY==1,])                               # TMLE results\ntlme.med.cc = cbind.data.frame(type='TMLE-cc',                                          # method label\n                               subpop=c('all','subpop 0','subpop 1'),                   # subpop label\n                               tml.cate.med.cc[order(tml.cate.med.cc$param),c(9,10,8)], # C.I. data\n                               strata='prior treatment')                                # Strata variable\n# TMLE on sex strata\ntml.cate.sex.cc = aux_tml_method('sex.girl',\n                                 data=synth.data[RY==1,])                               # TMLE results\ntlme.sex.cc = cbind.data.frame(type='TMLE-cc',                                          # method label\n                               subpop=c('all','subpop 0','subpop 1'),                   # subpop label\n                               tml.cate.sex.cc[order(tml.cate.sex.cc$param),c(9,10,8)], # C.I. data\n                               strata='sex at birth')                                   # Strata variable\n\n\n\nTMLE-im: TMLE with single median-inputation. We replace \\(Y^*\\) with the median of the observed \\(Y\\)\n\nThis approach would also produce biased results, as all missing slots are inputed with the same numeric value independen of \\(H,A\\). One preferable approach would be multiple model-based inputations (consistent under MAR). Yet, single median-inputation is the built-in preprocessing method in the tmle package\n\n\\[\nY^* \\leftarrow \\text{med}(Y\\mid R_Y=1)\n\\]\n\n\n\nCode\n###############################################################################\n# Median-based single imputation for missing outcome\n###############################################################################\n\n# Replace Y-values for NA whenever R(Y)=0\nobserved.data = copy(synth.data)\nobserved.data$ntr.grd8 = ifelse(observed.data$RY==0, NA, observed.data$ntr.grd8)\n\n# Apply process_missing function on TMLE package\nprocessed = process_missing(observed.data,                     # data\n                            node_list = list(W = H,            # confounders\n                                             A = \"med.6to8\",   # exposure\n                                             Y = \"ntr.grd8\"),  # outcome\n                            complete_nodes = c('W','A'))       # complete variables\n\n# TMLE on prior treatment\ntml.cate.med.im = aux_tml_method('med.pre6',\n                                 data=processed$data)                                   # TMLE results\ntlme.med.im = cbind.data.frame(type='TMLE-im',                                          # method label\n                               subpop=c('all','subpop 0','subpop 1'),                   # subpop label\n                               tml.cate.med.im[order(tml.cate.med.im$param),c(9,10,8)], # C.I. data\n                               strata='prior treatment')                                # Strata variable\n# TMLE on sex strata\ntml.cate.sex.im = aux_tml_method('sex.girl',\n                                 data=processed$data)                                   # TMLE results\ntlme.sex.im = cbind.data.frame(type='TMLE-im',                                          # method label\n                               subpop=c('all','subpop 0','subpop 1'),                   # subpop label\n                               tml.cate.sex.im[order(tml.cate.sex.im$param),c(9,10,8)], # C.I. data\n                               strata='sex at birth')                                   # Strata variable\n\n# Unify column names\ncolnames(tlme.med.cc) = colnames(ttest.med)\ncolnames(tlme.sex.cc) = colnames(ttest.sex)\ncolnames(tlme.med.im) = colnames(ttest.med)\ncolnames(tlme.sex.im) = colnames(ttest.sex)\n\n# Put all together\nci.data.2 = rbind.data.frame(ttest.med[ttest.med$type=='oracle',],\n                             ttest.sex[ttest.sex$type=='oracle',],\n                             tlme.med.cc, tlme.sex.cc,\n                             tlme.med.im, tlme.sex.im)\n\n\n\nTMLE-mm: TMLE with explicit model for \\(M\\) (only considering ptr.diag)\n\nLet us assume we are sure that post-treatment diagnoses (ptr.diag) have a direct effect on attrition, but post-treatment visits (ptr.gpsp) do not. Since the variable ptr.diag is binary, we can pose an explicit model using a superlearner based on logistic regression. If such assumption is sound, this approach should remove most of the selection bias.\n\n\\[\n\\begin{aligned}\n&\\text{Outcome model on selected } & Q_1(H,A,M) = \\mathbb{E}[Y\\mid H,A,M,R_Y=1] & \\\\\n&\\text{M-model (\\texttt{ptr.diag})}  & m(H,A) = \\mathbb{P}[M=1\\mid H,A] &\\\\\n&\\text{M-averaged expected outcome}  & Q_2(H,a) = m(H,a)Q_1(H,a,1) +(1-m(H,a))Q_1(H,a,0) &\\\\\n&\\text{Initial ATE} &  \\psi_0 = \\mathbb{E}_H\\Delta_a Q_2(H,a)&\\\\\n&\\text{Propensity score (nuisance)}  & e(H) = \\mathbb{P}[A=1\\mid H] &\\\\\n&\\text{Selection score (nuisance)}  & s(H,A,M) = \\mathbb{P}[R_Y=1\\mid H,A,M] &\\\\\n\\end{aligned}\n\\]\n\n\n\nCode\n###############################################################################\n# Recovering causal effect via regression and explicit M-model\n###############################################################################\n\n# The library of base learners: GAM and random forests\nstack = Stack$new(Lrnr_gam$new(), Lrnr_ranger$new())\n\n# Super-learner for continuous outcome: NNLS\nsuperl = Lrnr_sl$new(learners = stack, metalearner = Lrnr_nnls$new())\n\n# Super-learner for binary variables: default logit\nsuperb = Lrnr_sl$new(learners = stack)\n\n# Function to generate C.I. data from TMLE--------------------------------------\n# Param: pre-treatment binary variable to stratify\n# Param: data\n# Return: data from confidence intervals\n\naux_tml_method_mmod = function(namevar, input.data){\n  \n  # Remove namevar from adjustment set\n  H = setdiff(H, namevar)\n  \n  # Learning the Q-function ---------------------------------------------------\n  \n  # Training task for Q, fit superlearner \n  # on selected-outcome data, including M1 as predictors\n  train.Q = make_sl3_Task(\n    data = input.data[RY==1,],\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag')\n  )\n  Q_fit = superl$train(task = train.Q)\n  \n  # Training task for M, all data\n  train.M = make_sl3_Task(\n    data = input.data,\n    outcome = 'ptr.diag',\n    covariates = c(H,'med.6to8')\n  )\n  M_fit = superb$train(task = train.M)\n  \n  # Case A=1 ------------------------------------------------------------------\n  \n  # Data to make counterfactual predictions \n  temp.dt = copy(input.data)\n  \n  # Prediction task for p(M=1 |A=1)\n  temp.dt$med.6to8 = 1\n  pred.M = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ptr.diag',\n    covariates = c(H,'med.6to8')\n  )\n  pM = M_fit$predict(task = pred.M)\n  \n  # Prediction task for E(Y |A=1, M=1)\n  temp.dt$ptr.diag = 1\n  pred.Q1 = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag')\n  )\n  \n  # Prediction task for E(Y |A=1, M=0)\n  temp.dt$ptr.diag = 0\n  pred.Q0 = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag')\n  )\n  \n  # Save INTEGRAL dP(M |A=1) E(Y |A=1, M)\n  input.data$Q_1m = pM * Q_fit$predict(task = pred.Q1) +\n    (1-pM) * Q_fit$predict(task = pred.Q0)\n  \n  # Case A=0 ------------------------------------------------------------------\n  \n  # Prediction task for p(M=1 |A=0)\n  temp.dt$med.6to8 = 0\n  pred.M = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ptr.diag',\n    covariates = c(H,'med.6to8')\n  )\n  pM = M_fit$predict(task = pred.M)\n  \n  # Prediction task for E(Y |A=0, M=1)\n  temp.dt$ptr.diag = 1\n  pred.Q1 = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag')\n  )\n  \n  # Prediction task for E(Y |A=0, M=0)\n  temp.dt$ptr.diag = 0\n  pred.Q0 = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag')\n  )\n  \n  # Save INTEGRAL dP(M |A=0) E(Y |A=0, M)\n  input.data$Q_0m = pM * Q_fit$predict(task = pred.Q1) +\n    (1-pM) * Q_fit$predict(task = pred.Q0)\n  \n  # Save Qm as the M-average of Q1 and Q0\n  input.data[,Qm := med.6to8*Q_1m + (1-med.6to8)*Q_0m]\n  \n  # Nuisance parameters: A  ----------------------------------------------------\n  \n  # Training task for A, fit superlearner \n  train.A = make_sl3_Task(\n    data = input.data,\n    outcome = 'med.6to8',\n    covariates = H\n  )\n  A_fit = superb$train(task = train.A)\n  input.data$p.score = A_fit$predict(task = train.A)\n  \n  # Nuisance parameters: R(Y)  -------------------------------------------------\n  \n  # Training task for R(Y), fit superlearner \n  train.R = make_sl3_Task(\n    data = input.data,\n    outcome = 'RY',\n    covariates = c(H,'med.6to8','ptr.diag')\n  )\n  R_fit = superb$train(task = train.R)\n  input.data$p.select = R_fit$predict(task = train.R)\n  \n  # Count parameters  ---------------------------------------------------------\n  N = nrow(input.data)\n  n = nrow(input.data[RY==1,])\n  r = n/N\n  \n  # Nuisance parameters: R(Y)  ---------------------------------------------------\n  input.data[, H:= r/(p.select) * (med.6to8/p.score - (1-med.6to8)/(1-p.score)) ]\n  \n  # Updating model  ---------------------------------------------------\n  updating = lm(ntr.grd8 ~ -1 + offset(Qm) + H, data=input.data)\n  eps = as.numeric(coef(updating))\n  \n  # Update Q-values\n  input.data[, Q_1f:= Q_1m + eps * r/(p.select*p.score)]\n  input.data[, Q_0f:= Q_0m - eps * r/(p.select*(1-p.score))]\n  input.data[, D:= H*(ntr.grd8 - Qm) + Q_1m - Q_0m]\n  \n  # Compute ATE, standard errors and CI\n  ate = mean(input.data$Q_1f - input.data$Q_0f)\n  ate.se = sd(input.data$D, na.rm=T)/sqrt(n)\n  ate.lo = ate - qnorm(0.975)*ate.se\n  ate.up = ate + qnorm(0.975)*ate.se\n  \n  # Return CI\n  return(c(ate.lo, ate.up, ate))\n}\n\n# Generate CI data\nate = aux_tml_method_mmod('', observed.data)\ncate.nop =  aux_tml_method_mmod('med.pre6', observed.data[med.pre6==0,])\ncate.yes =  aux_tml_method_mmod('med.pre6', observed.data[med.pre6==1,])\ncate.boy =  aux_tml_method_mmod('sex.girl', observed.data[sex.girl==0,])\ncate.grl =  aux_tml_method_mmod('sex.girl', observed.data[sex.girl==1,])\n\n# Put all together\nci.data.3 = ci.data[13:18,]\nci.data.3$type = 'TMLE-mm'\nci.data.3$midp = c(ate[3], cate.nop[3], cate.yes[3], ate[3], cate.boy[3], cate.grl[3])\nci.data.3$low = c(ate[1], cate.nop[1], cate.yes[1], ate[1], cate.boy[1], cate.grl[1])\nci.data.3$upr = c(ate[2], cate.nop[2], cate.yes[2], ate[2], cate.boy[2], cate.grl[2])\n\n\n\nTMLE-nr: TMLE with nested regressions.\n\nLet us assume we are sure that post-treatment diagnoses (ptr.diag) have a direct effect on attrition, but post-treatment visits (ptr.gpsp) do not. Since the variable ptr.diag is binary, we can pose an explicit model using a superlearner based on logistic regression. If such assumption is sound, this approach should remove most of the selection bias.\n\n\\[\n\\begin{aligned}\n&\\text{Outcome model on selected } & Q_1(H,A,M) = \\mathbb{E}[Y\\mid H,A,M,R_Y=1] & \\\\\n&\\text{High-level regression}  & Q_2(H,a) = \\mathbb{E}[\\hat{Q}_1\\mid H,A=a] &\\\\\n&\\text{Initial ATE} &  \\psi_0 = \\mathbb{E}_H\\Delta_a Q_2(H,a)&\\\\\n&\\text{Propensity score (nuisance)}  & e(H) = \\mathbb{P}[A=1\\mid H] &\\\\\n&\\text{Selection score (nuisance)}  & s(H,A,M) = \\mathbb{P}[R_Y=1\\mid H,A,M] &\\\\\n\\end{aligned}\n\\]\n\n\n\nCode\n###############################################################################\n# Recovering causal effect via nested regressions\n###############################################################################\n\n# Function to generate C.I. data from TMLE--------------------------------------\n# Param: pre-treatment binary variable to stratify\n# Param: data\n# Return: data from confidence intervals\n\naux_tml_method_nest = function(namevar, input.data){\n  \n  # Remove namevar from adjustment set\n  H = setdiff(H, namevar)\n  \n  # Learning the Q-function ---------------------------------------------------\n  \n  # Training task for Q, fit superlearner \n  # on selected-outcome data, including M1 as predictors\n  train.Q = make_sl3_Task(\n    data = input.data[RY==1,],\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag','ptr.gpsp')\n  )\n  \n  Q_fit = superl$train(task = train.Q)\n  \n  # Data to make counterfactual predictions \n  temp.dt = copy(input.data)\n  \n  # Prediction task for E(Y |A=1, M)\n  temp.dt$med.6to8 = 1\n  pred.Q1 = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag','ptr.gpsp')\n  )\n  \n  input.data$Q1.pred = Q_fit$predict(task = pred.Q1)\n  \n  # Prediction task for E(Y |A=0, M)\n  temp.dt$med.6to8 = 0\n  pred.Q0 = make_sl3_Task(\n    data = temp.dt,\n    outcome = 'ntr.grd8',\n    covariates = c(H,'med.6to8','ptr.diag','ptr.gpsp')\n  )\n  \n  input.data$Q0.pred = Q_fit$predict(task = pred.Q0)\n  \n  # Training task for Q21\n  train.Q21 = make_sl3_Task(\n    data = input.data,\n    outcome = 'Q1.pred',\n    covariates = H # No treatment in here, its fixed\n  )\n  Q21_fit = superl$train(task = train.Q21)\n  input.data$Q1.fin = Q21_fit$predict(task = train.Q21)\n  \n  # Training task for Q20\n  train.Q20 = make_sl3_Task(\n    data = input.data,\n    outcome = 'Q0.pred',\n    covariates = H # No treatment in here, its fixed\n  )\n  Q20_fit = superl$train(task = train.Q20)\n  input.data$Q0.fin = Q20_fit$predict(task = train.Q20)\n  \n  # m-Average\n  input.data[, Qa:= med.6to8*Q1.fin + (1-med.6to8)*Q0.fin]\n  \n  # Nuisance parameters: A  ----------------------------------------------------\n  \n  # Training task for A, fit superlearner \n  train.A = make_sl3_Task(\n    data = input.data,\n    outcome = 'med.6to8',\n    covariates = H\n  )\n  A_fit = superb$train(task = train.A)\n  input.data$p.score = A_fit$predict(task = train.A)\n  \n  # Nuisance parameters: R(Y)  -------------------------------------------------\n  \n  # Training task for R(Y), fit superlearner \n  train.R = make_sl3_Task(\n    data = input.data,\n    outcome = 'RY',\n    covariates = c(H,'med.6to8','ptr.diag')\n  )\n  R_fit = superb$train(task = train.R)\n  input.data$p.select = R_fit$predict(task = train.R)\n  \n  # Count parameters  ---------------------------------------------------------\n  N = nrow(input.data)\n  n = nrow(input.data[RY==1,])\n  r = n/N\n  \n  # Nuisance parameters: R(Y)  ---------------------------------------------------\n  input.data[, H:= r/(p.select) * (med.6to8/p.score - (1-med.6to8)/(1-p.score)) ]\n  \n  # Updating model  ---------------------------------------------------\n  updating = lm(ntr.grd8 ~ -1 + offset(Qa) + H, data=input.data)\n  eps = as.numeric(coef(updating))\n  \n  # Update Q-values\n  input.data[, Q_1f:= Q1.fin + eps * r/(p.select*p.score)]\n  input.data[, Q_0f:= Q0.fin - eps * r/(p.select*(1-p.score))]\n  input.data[, D:= H*(ntr.grd8 - Qa) + Q1.fin - Q0.fin]\n  \n  # Compute ATE, standard errors and CI\n  ate = mean(input.data$Q_1f - input.data$Q_0f)\n  ate.se = sd(input.data$D, na.rm=T)/sqrt(n)\n  ate.lo = ate - qnorm(0.975)*ate.se\n  ate.up = ate + qnorm(0.975)*ate.se\n  \n  # Return CI\n  return(c(ate.lo, ate.up, ate))\n}\n\n# Generate CI data\nate = aux_tml_method_nest('', observed.data)\ncate.nop =  aux_tml_method_nest('med.pre6', observed.data[med.pre6==0,])\ncate.yes =  aux_tml_method_nest('med.pre6', observed.data[med.pre6==1,])\ncate.boy =  aux_tml_method_nest('sex.girl', observed.data[sex.girl==0,])\ncate.grl =  aux_tml_method_nest('sex.girl', observed.data[sex.girl==1,])\n\n# Put all together\nci.data.4 = ci.data[13:18,]\nci.data.4$type = 'TMLE-nr'\nci.data.4$midp = c(ate[3], cate.nop[3], cate.yes[3], ate[3], cate.boy[3], cate.grl[3])\nci.data.4$low = c(ate[1], cate.nop[1], cate.yes[1], ate[1], cate.boy[1], cate.grl[1])\nci.data.4$upr = c(ate[2], cate.nop[2], cate.yes[2], ate[2], cate.boy[2], cate.grl[2])\n\n# All methods together\nci.data = rbind(ci.data.2, ci.data.3, ci.data.4)\n\n\nThe following plot presents the point-estimate and 95% confidence interval from the approaches presented above:\n\n\nCode\n# Plot confidence intervals ---------------------------------------------------\nrbind(ci.data) %&gt;% \n  ggplot(aes(x = subpop, y = midp, colour = type)) +                    \n  geom_errorbar(aes(ymax = upr, ymin = low), position = \"dodge\") +\n  geom_point(position = position_dodge(0.9)) +\n  geom_hline(yintercept=0, linetype=\"dashed\", color = \"red\") +\n  labs(y='ATE/CATE estimate',\n       x='Subpopulation (1 = yes prior treatment, 1 = girls)') +\n  facet_wrap(~strata) + theme_classic()\n\n\n\n\n\n\n\n\n\nRegression adjustment works\n\nConfidence intervals from complete-case analysis (brown) and single median-inputations (green) fail to cover the oracle SATE (red) in most cases, particularly for the population ATE.\nA regression-based solution via an explicit model for ptr.diag (blue) does a better job, and only struggle with small subpopulations in the data (no prior treatment or girls, about 30%).\nA nested-regressions approach (purple) performs well for the ATE, but struggle a little with small subpopulations.\n\nAll methods above via TMLE employ only one fluctuation parameter (on \\(Q_2\\)). Better performance can be obtained by working with a fluctuation parameter on \\(Q_1\\) or \\(p(M\\mid H,A)\\) as well, and by incorporating smart cross-fitting schemes."
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#objective",
    "href": "posts/2023-08-01_missingOutcome/index.html#objective",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Objective",
    "text": "Objective\nThe final goal is to estimate the average treatment effect (ATE) and conditional average treatment effect (CATE) using observational data with missing values on the outcome variable"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#conceptual-diagram",
    "href": "posts/2023-08-01_missingOutcome/index.html#conceptual-diagram",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Conceptual diagram",
    "text": "Conceptual diagram\n\n\n\n\nflowchart LR\n  A[\"Observed-data\n  distribution\"] -- \"recovery\" --&gt; B[\"Underlying\n  distribution\"]\n  B -- \"identification\" --&gt; C[\"Causal effect\"]\n  A -- \"recovery\" --&gt; C\n\n\n\n\n\n\n\n\n\nflowchart LR\n  D[\"Graphical\n  criteria\"] --&gt; E[\"IPW\"]\n  D --&gt; F[\"Regression\"]\n  D --&gt; O[\"Other\"]\n  F --&gt; G[\"Model M\"]\n  F --&gt; H[\"Nested\"]\n  E --&gt; I[\"Doubly-\n  robustness\"]\n  F --&gt; I\n  I --&gt; J[\"AIPW\"]\n  I --&gt; K[\"TMLE\"]\n  I --&gt; L[\"DML\"]"
  },
  {
    "objectID": "posts/2023-08-01_missingOutcome/index.html#conceptual-diagrams",
    "href": "posts/2023-08-01_missingOutcome/index.html#conceptual-diagrams",
    "title": "Recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Conceptual diagrams",
    "text": "Conceptual diagrams\n\n\n\n\nflowchart LR\n  A[\"Observed-data\n  distribution\"] -- \"recovery\" --&gt; B[\"Underlying\n  distribution\"]\n  B -- \"identification\" --&gt; C[\"Causal effect\"]\n  A -- \"recovery\" --&gt; C\n\n\n\n\n\nDiagram 1: relation between identifiability and recoverability:\n\n\n\n\nflowchart LR\n  D[\"Graphical\n  criteria\"] ==&gt; E[\"IPW\"]\n  D ==&gt; F[\"Regression\"]\n  D -.-&gt; M[\"Mediation\n  analysis\"]\n  F --&gt; G[\"M-model\"]\n  F ==&gt; H[\"Nested\"]\n  E ==&gt; I[\"Doubly-\n  robustness\"]\n  F ==&gt; I\n  I -.-&gt; J[\"AIPW\"]\n  I ==&gt; K[\"TMLE\"]\n  I -.-&gt; L[\"DML\"]\n  M -.-&gt; N[\"Triply-\n  robustness\"]\n\n\n\n\n\nDiagram 2: relation between recoverability criteria and some estimation procedures:"
  },
  {
    "objectID": "posts/2023-09-01_UQcausality/index.html#structural-causal-models",
    "href": "posts/2023-09-01_UQcausality/index.html#structural-causal-models",
    "title": "Uncertainty modeling and quantification for causal inference",
    "section": "Structural causal models",
    "text": "Structural causal models\nThe most complete and versatile axiomatic treatment of causal inference is given by the structural causal model (SCM) framework, by celebrated computer scientist and philosopher Dr. Judea Pearl (Pearl 2009). An SCM is a tuple of mathematical objects \\(\\mathfrak{M}=(\\mathcal{V},\\mathcal{U},\\mathcal{G},\\mathcal{F},{P}(\\mathcal{U}))\\)1, such that:\n\n\\(\\mathcal{V}\\) is a finite set of endogenous random variables (the data)\n\\(\\mathcal{U}\\) is a finite set of exogenous random variables (the noises)\n\\(\\mathcal{G}\\) is a directed acyclic graph on \\(\\mathcal{V}\\)\n\\(P(\\mathcal{U})\\) is a probability measure for \\(\\mathcal{U}\\)\n\\(\\mathcal{F}=\\{f_V\\}_{V\\in\\mathcal{V}}\\) is an indexed collection of measurable functions specifying the causal relations i.e., for every \\(V\\in\\mathcal{V}\\) there is a \\(U_V\\in\\mathcal{U}\\) and a function \\(f_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\), such that \\(V=f_V(\\text{pa}(V;\\mathcal{G}),U_V)\\) almost surely\n\nAn SCM is Markovian if the exogenous variables \\(\\{U_V\\}_{V\\in\\mathcal{V}}\\) are assumed to be mutually independent. In this case, the set of conditional independence statements encoded in \\(\\mathcal{G}\\) allows a Bayesian network representation that factorizes the joint distribution in terms of independent causal mechanisms (Pearl 2009):\n\\[\np(\\mathcal{V}) = \\prod_{V\\in\\mathcal{V}}p(V\\mid \\text{pa}(V;\\mathcal{G}))\n\\]\nA hard intervention on a collection of variables \\(A=(A_j)_{j\\in J}\\subset\\mathcal{V}\\) to the assigned value \\(a=(a_j)_{j\\in J}\\in\\text{supp}\\, A\\) is denoted \\(do(A=a)\\). Such intervention induces a new SCM \\(\\mathfrak{M}_{A=a}\\), where all \\(f_{A_j}\\) are replaced by constant functions that output the respective value \\(a_j\\). Its associated graph is the mutilated graph \\(\\mathcal{G}[\\overline{A}]\\) that removes all incoming arrows to \\(A\\) (Bareinboim et al. 2022)."
  },
  {
    "objectID": "posts/2023-09-01_UQcausality/index.html#footnotes",
    "href": "posts/2023-09-01_UQcausality/index.html#footnotes",
    "title": "Uncertainty modeling and quantification for causal inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome authors do not include \\(\\mathcal{G}\\) directly in \\(\\mathfrak{M}\\), but say that \\(\\mathcal{G}\\) is associated with \\(\\mathfrak{M}\\). This might be useful in some uncertainty-related context, such as when there are hidden variables, as many marginal graphs can be associated with the same SCM.↩︎\n\\(\\text{pa}(A;\\mathcal{G})\\) stands for the parents of node \\(A\\) in \\(\\mathcal{G}\\).↩︎\n\\(\\text{nd}(A;\\mathcal{G})\\) stands for the non-descendants of node \\(A\\) in \\(\\mathcal{G}\\).↩︎"
  },
  {
    "objectID": "posts/2023-09-01_UQcausality/index.html#structural-causal-models-a-technical-introduction",
    "href": "posts/2023-09-01_UQcausality/index.html#structural-causal-models-a-technical-introduction",
    "title": "Uncertainty modeling and quantification for causal inference",
    "section": "Structural causal models: a technical introduction",
    "text": "Structural causal models: a technical introduction\nAn SCM is a tuple of mathematical objects \\(\\mathfrak{M}=(\\mathcal{V},\\mathcal{U},\\mathcal{G},\\mathcal{F},{P}(\\mathcal{U}))\\)1, such that:\n\n\\(\\mathcal{V}\\) is a finite set of endogenous random variables (the data)\n\\(\\mathcal{U}\\) is a finite set of exogenous random variables (the noises)\n\\(\\mathcal{G}\\) is a directed acyclic graph on \\(\\mathcal{V}\\)\n\\(P(\\mathcal{U})\\) is a probability measure for \\(\\mathcal{U}\\)\n\\(\\mathcal{F}=\\{f_V\\}_{V\\in\\mathcal{V}}\\) is an indexed collection of measurable functions specifying the causal relations i.e., for every \\(V\\in\\mathcal{V}\\) there is a \\(U_V\\in\\mathcal{U}\\) and a function \\(f_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\), such that \\(V=f_V(\\text{pa}(V;\\mathcal{G}),U_V)\\) almost surely\n\nAn SCM is Markovian if all endogenous variables are observed and exogenous variables \\(\\{U_V\\}_{V\\in\\mathcal{V}}\\) are assumed to be mutually independent. In this case, the set of conditional independence statements encoded in \\(\\mathcal{G}\\) allows a Bayesian network representation that factorizes the joint distribution in terms of independent causal mechanisms (Pearl 2009):\n\\[\np(\\mathcal{V}) = \\prod_{V\\in\\mathcal{V}}p(V\\mid \\text{pa}(V;\\mathcal{G}))\n\\]\nA hard intervention on a collection of variables \\(A=(A_j)_{j\\in J}\\subset\\mathcal{V}\\) to the assigned value \\(a=(a_j)_{j\\in J}\\in\\text{supp}\\, A\\) is denoted \\(do(A=a)\\). Such intervention induces a new SCM \\(\\mathfrak{M}_{A=a}\\), where all \\(f_{A_j}\\) are replaced by constant functions that output the respective value \\(a_j\\). Its associated graph is the mutilated graph \\(\\mathcal{G}[\\overline{A}]\\) that removes all incoming arrows to \\(A\\) (Bareinboim et al. 2022).\nLet disjoint \\(A,Y\\subset\\mathcal{V}\\) denote respectively the exposure and outcome variables, the unit-level counterfactual \\(Y_{a}(u)\\) is the value \\(Y\\) takes according to \\(\\mathfrak{M}_{A=a}\\) in the individual context \\(\\mathcal{U}=u\\). Its induced population-level distribution, named the interventional distribution, can be expressed as:\n\\[\n    p(y\\mid do(A=a)) := p_{Y_{a}}(y)=\\int_{\\mathcal{U}_a[y]}\\text{d} P(u)\n\\]\nWhere \\(\\mathcal{U}_a[y]=\\{u\\in\\text{supp}\\, \\mathcal{U} : Y_{a}(u)=y \\}\\) is the inverse image of \\(y\\in\\text{supp}\\, Y\\) under \\(Y_{a}(u)\\) for a given \\(a\\in\\text{supp}\\, A\\).\nThe average treatment effect (ATE), \\(\\psi\\), and the \\(X\\)-specific conditional average treatment effect (CATE), \\(\\psi_X(\\cdot)\\), with \\(X\\subseteq\\text{nd}(A;\\mathcal{G})\\)2:, are two of the most commonly investigated causal effects/estimands. For binary \\(A\\), they correspond to difference functionals of the interventional distribution, and are defined as:\n\\[\n\\begin{aligned}\n    \\psi &:= \\Delta_a \\mathbb{E}\\left[Y\\mid do(A=a) \\right]\\\\\n    \\psi_X(x) &:= \\Delta_a \\mathbb{E}\\left[Y\\mid do(A=a),X=x \\right],\\,  x\\in\\text{supp}\\, X,\\, X\\subseteq\\text{nd}(A;\\mathcal{G})\n\\end{aligned}\n\\]\nAn interventional distribution or causal effect is said to be nonparametrically identifiable from positive \\(P(\\mathcal{V})\\) if it can be uniquely computed from it using the conditional independence statements embedded in \\(\\mathcal{G}\\) and its mutilation. In other words, a query \\(Q\\) (interventional distribution, ATE, CATE, etc.) is nonparametrically identifiable from \\(P(\\mathcal{V})\\), if there exists an identifying functional \\(\\Psi:P(\\mathcal{V})\\mapsto Q\\), such that \\(\\Psi\\) is an algorithm that inputs the conditional independence statements in \\(\\mathcal{G}\\) and its mutilation, and outputs an unique \\(Q\\)."
  },
  {
    "objectID": "posts/2023-09-01_UQcausality/index.html#structural-causal-models-a-simple-introduction-and-example",
    "href": "posts/2023-09-01_UQcausality/index.html#structural-causal-models-a-simple-introduction-and-example",
    "title": "Uncertainty modeling and quantification for causal inference",
    "section": "Structural causal models: a simple introduction and example",
    "text": "Structural causal models: a simple introduction and example\nThe most complete and versatile axiomatic treatment of causal inference is given by the structural causal model (SCM) framework, by celebrated computer scientist and philosopher Dr. Judea Pearl (Pearl 2009). Let us give a simple example:\nConsider an experiment where we apply an external force \\(F\\) on objects with different masses \\(m\\) and then measure their induced acceleration \\(a\\). Let us put all these variables together: \\(\\mathcal{V}=\\{m,F,a\\}\\). Here, \\(m\\) and \\(F\\) are controlled (possibly randomly assigned) by the researcher, so they might be considered exogenous.\nAcceleration measurements is noisy, due to random measurement errors or residual forces applied (air current). Such noise is captured by random variable \\(U_a\\). Let us put all exogenous variables and noises together in \\(\\mathcal{U}=\\{m,F,U_a\\}\\). Observations from these variables come respectively from \\(P(m), P(F), P(U_a)\\) (independent) distributions.\nSince we measure the induced acceleration after an applied force, and we know there’s a theoretical (causal) relation between them, we can say that such acceleration is caused by the force. We can represent this graphically as \\(F\\rightarrow a\\). We also know changes in the mass being pushed induce changes in the resulting acceleration, so the \\(m\\) is also a cause of \\(a\\), or \\(m\\rightarrow a\\). The external force is applied independent of the mass, so none is cause of the other. We can represent succinctly such causal structure with a directed acyclic graph \\(\\mathcal{G}\\):\n\\[\n\\mathcal{G} :\\quad F\\rightarrow a\\leftarrow m\n\\] By Newton’s second law formula (with noise), measured acceleration can be expressed as \\(a = m^{-1}F+U_a\\). This can be shortly expressed as \\(a=f_a(m,F,U_a)\\), where \\(f_a\\) is the acceleration’s causal mechanism: a deterministic function defined as \\(f_a(x,y,z)=x^{-1}y+z\\)\nThen, the tuple of mathematical objects \\(\\mathfrak{M}=(\\mathcal{V},\\mathcal{U},\\mathcal{G},f_a,{P}(\\mathcal{U}))\\) is a structural causal model (SCM) that fully describes the system, and can be leveraged to answer queries in three levels:\n\nObservational: keeping the mass constant at \\(m=2\\)kg, what is the observed curve \\(F\\) vs. \\(a\\)?\nCausal: what’s the average change in \\(a\\) if \\(F\\) changes from \\(F=1\\)N to \\(F=2\\)N with a mass of \\(1\\)kg?\nCounterfactual: what would have been the value of \\(a\\) under \\(F=2\\)N for a mass \\(m=1\\)kg that actually experienced \\(F=1\\)N and measured \\(a=1\\)?"
  },
  {
    "objectID": "posts/2023-09-01_UQcausality/index.html#structural-causal-models-a-more-technical-introduction",
    "href": "posts/2023-09-01_UQcausality/index.html#structural-causal-models-a-more-technical-introduction",
    "title": "Uncertainty modeling and quantification for causal inference",
    "section": "Structural causal models: a more technical introduction",
    "text": "Structural causal models: a more technical introduction\nAn SCM is a tuple of mathematical objects \\(\\mathfrak{M}=(\\mathcal{V},\\mathcal{U},\\mathcal{G},\\mathcal{F},{P}(\\mathcal{U}))\\)1, such that:\n\n\\(\\mathcal{V}\\) is a finite set of relevant random variables (the data)\n\\(\\mathcal{U}\\) is a finite set of exogenous random variables and background noises\n\\(\\mathcal{G}\\) is a directed acyclic graph on \\(\\mathcal{V}\\)\n\\(P(\\mathcal{U})\\) is a probability measure for \\(\\mathcal{U}\\)\n\\(\\mathcal{F}=\\{f_V\\}_{V\\in\\mathcal{V}}\\) is an indexed collection of measurable functions specifying the causal relations i.e., for every \\(V\\in\\mathcal{V}\\) there is a \\(U_V\\in\\mathcal{U}\\) and a function \\(f_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\), such that \\(V=f_V(\\text{pa}(V;\\mathcal{G}),U_V)\\)2 almost surely\n\nAn SCM is Markovian if all the background noises \\(\\{U_V\\}_{V\\in\\mathcal{V}}\\) are assumed to be mutually independent. In this case, the set of conditional independence statements encoded in \\(\\mathcal{G}\\) allows a Bayesian network representation that factorizes the joint distribution in terms of independent causal families (Pearl 2009):\n\\[\np(\\mathcal{V}) = \\prod_{V\\in\\mathcal{V}}p(V\\mid \\text{pa}(V;\\mathcal{G}))\n\\]\nA hard intervention on a collection of variables \\(A=(A_j)_{j\\in J}\\subset\\mathcal{V}\\) to the assigned value \\(a=(a_j)_{j\\in J}\\in\\text{supp}\\, A\\) is denoted \\(do(A=a)\\). Such intervention induces a new SCM \\(\\mathfrak{M}_{A=a}\\), where all \\(f_{A_j}\\) are replaced by constant functions that output the respective value \\(a_j\\). Its associated graph is the mutilated graph \\(\\mathcal{G}[\\overline{A}]\\) that removes all incoming arrows to \\(A\\) (Bareinboim et al. 2022).\nLet disjoint \\(A,Y\\subset\\mathcal{V}\\) denote respectively the exposure and outcome variables, the unit-level counterfactual \\(Y_{a}(u)\\) is the value \\(Y\\) takes according to \\(\\mathfrak{M}_{A=a}\\) in the individual context \\(\\mathcal{U}=u\\). Its induced population-level distribution, named the interventional distribution, can be expressed as:\n\\[\n    p(y\\mid do(A=a)) := p_{Y_{a}}(y)=\\int_{\\mathcal{U}_a[y]}\\text{d} P(u)\n\\]\nWhere \\(\\mathcal{U}_a[y]=\\{u\\in\\text{supp}\\, \\mathcal{U} : Y_{a}(u)=y \\}\\) is the inverse image of \\(y\\in\\text{supp}\\, Y\\) under \\(Y_{a}(u)\\) for a given \\(a\\in\\text{supp}\\, A\\).\nThe average treatment effect (ATE), \\(\\psi\\), and the \\(X\\)-specific conditional average treatment effect (CATE), \\(\\psi_X(\\cdot)\\), with \\(X\\subseteq\\text{nd}(A;\\mathcal{G})\\)3:, are two of the most commonly investigated causal effects/estimands. For binary \\(A\\), they correspond to difference functionals of the interventional distribution, and are defined as:\n\\[\n\\begin{aligned}\n    \\psi &:= \\Delta_a \\mathbb{E}\\left[Y\\mid do(A=a) \\right]\\\\\n    \\psi_X(x) &:= \\Delta_a \\mathbb{E}\\left[Y\\mid do(A=a),X=x \\right],\\,  x\\in\\text{supp}\\, X,\\, X\\subseteq\\text{nd}(A;\\mathcal{G})\n\\end{aligned}\n\\]\nAn interventional distribution or causal effect is said to be nonparametrically identifiable from positive \\(P(\\mathcal{V})\\) if it can be uniquely computed from it (using the conditional independence statements embedded in \\(\\mathcal{G}\\) and its mutilation). In other words, a query \\(Q\\) such as interventional distribution, ATE, or CATE, is nonparametrically identifiable from \\(P(\\mathcal{V})\\), if there exists a functional/algorithm \\(\\Psi_\\mathcal{G}:P(\\mathcal{V})\\mapsto Q\\), such that such that it returns a unique value up to some equivalent relation."
  },
  {
    "objectID": "posts/2023-09-01_UQcausality/index.html#uncertainty-modeling-and-quantification-in-structural-causal-models",
    "href": "posts/2023-09-01_UQcausality/index.html#uncertainty-modeling-and-quantification-in-structural-causal-models",
    "title": "Uncertainty modeling and quantification for causal inference",
    "section": "Uncertainty modeling and quantification in structural causal models",
    "text": "Uncertainty modeling and quantification in structural causal models\nThe two natures of uncertainty, aleatoric and epistemic (Hüllermeier and Waegeman 2021), can be associated with different parts of an SCM.\n\nAleatoric uncertainty\nIt is generally induced by randomness, i.e. \\(P(\\mathcal{U})\\) and, in turn, \\(P(\\mathcal{V})\\).\n\n\nEpistemic uncertainty\nIt is the result of working with unknowns. It can be further broken down into:\n\nModel/mechanism uncertainty: induced by unknown \\(\\mathcal{F}\\). Its analytic treatment depends on where the true mechanism lies in relation with the working models/hypotheses \\(\\mathcal{M}\\) (linear regressions, neural nets, etc.).\n\n\\(\\mathcal{M}\\)-closed world: If \\(\\mathcal{F}\\in\\mathcal{M}\\), model uncertainty can be integrated via Bayesian model-averaging.\n\\(\\mathcal{M}\\)-open/complete world: If \\(\\mathcal{F}\\notin\\mathcal{M}\\), model uncertainty can be integrated via Bayesian stacking of predictive distributions (Yao et al. 2018).\n\nStructure uncertainty: induced by unknown \\(\\mathcal{G}\\), typically under the causal sufficiency assumption, i.e., all relevant endogenous variables \\(\\mathcal{V}\\) are observed, but the graph connecting them is not (Kitson et al. 2023).\n\nHere, ‘’relevant’’ depends on the downstream task. If the goal is full structure learning, then all \\(\\mathcal{V}\\) must be observed. If the goal is downstream causal inference, then a minimal adjustment set must be observed.\n\nIdentification uncertainty: induced by latent/hidden \\(V_H\\subset\\mathcal{V}\\), where variables \\(V_H\\) are needed for identification of causal and counterfactual queries.\n\nWhen point-identification is not possible due to latent variables, such as unmeasured confounders, partial-identification (bounds) can still be informative (Chernozhukov et al. 2022)."
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#structural-causal-models-a-simple-introduction-and-example",
    "href": "posts/2023-09-15_structureUQ/index.html#structural-causal-models-a-simple-introduction-and-example",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Structural causal models: a simple introduction and example",
    "text": "Structural causal models: a simple introduction and example\nThe most complete and versatile axiomatic treatment of causal inference is given by the structural causal model (SCM) framework, by celebrated computer scientist and philosopher Dr. Judea Pearl (Pearl 2009). A brief introduction of SCM is given in a previous blog post."
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#structural-causal-models-a-more-technical-introduction",
    "href": "posts/2023-09-15_structureUQ/index.html#structural-causal-models-a-more-technical-introduction",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Structural causal models: a more technical introduction",
    "text": "Structural causal models: a more technical introduction\nAn SCM is a tuple of mathematical objects \\(\\mathcal{M}=(\\mathcal{V},\\mathcal{U},\\mathcal{G},\\mathcal{F},{P}(\\mathcal{U}))\\)1, such that:\n\n\\(\\mathcal{V}\\) is a finite set of relevant random variables (the data)\n\\(\\mathcal{U}\\) is a finite set of exogenous random variables and background noises\n\\(\\mathcal{G}\\) is a directed acyclic graph on \\(\\mathcal{V}\\)\n\\(P(\\mathcal{U})\\) is a probability measure for \\(\\mathcal{U}\\)\n\\(\\mathcal{F}=\\{f_V\\}_{V\\in\\mathcal{V}}\\) is an indexed collection of measurable functions specifying the causal relations i.e., for every \\(V\\in\\mathcal{V}\\) there is a \\(U_V\\in\\mathcal{U}\\) and a function \\(f_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\), such that \\(V=f_V(\\text{pa}(V;\\mathcal{G}),U_V)\\)2 almost surely\n\nAn SCM is Markovian if all the background noises \\(\\{U_V\\}_{V\\in\\mathcal{V}}\\) are assumed to be mutually independent. In this case, the set of conditional independence statements encoded in \\(\\mathcal{G}\\) allows a Bayesian network representation that factorizes the joint distribution in terms of independent causal families (Pearl 2009):\n\\[\np(\\mathcal{V}) = \\prod_{V\\in\\mathcal{V}}p(V\\mid \\text{pa}(V;\\mathcal{G}))\n\\]\nA hard intervention on a collection of variables \\(A=(A_j)_{j\\in J}\\subset\\mathcal{V}\\) to the assigned value \\(a=(a_j)_{j\\in J}\\in\\text{supp}\\, A\\) is denoted \\(do(A=a)\\). Such intervention induces a new SCM \\(\\mathcal{M}_{A=a}\\), where all \\(f_{A_j}\\) are replaced by constant functions that output the respective value \\(a_j\\). Its associated graph is the mutilated graph \\(\\mathcal{G}[\\overline{A}]\\) that removes all incoming arrows to \\(A\\) (Bareinboim et al. 2022).\nLet disjoint \\(A,Y\\subset\\mathcal{V}\\) denote respectively the exposure and outcome variables, the unit-level counterfactual \\(Y_{a}(u)\\) is the value \\(Y\\) takes according to \\(\\mathcal{M}_{A=a}\\) in the individual context \\(\\mathcal{U}=u\\). Its induced population-level distribution, named the interventional distribution, can be expressed as:\n\\[\n    p(y\\mid do(A=a)) := p_{Y_{a}}(y)=\\int_{\\mathcal{U}_a[y]}\\text{d} P(u)\n\\]\nWhere \\(\\mathcal{U}_a[y]=\\{u\\in\\text{supp}\\, \\mathcal{U} : Y_{a}(u)=y \\}\\) is the inverse image of \\(y\\in\\text{supp}\\, Y\\) under \\(Y_{a}(u)\\) for a given \\(a\\in\\text{supp}\\, A\\).\nThe average treatment effect (ATE), \\(\\psi\\), and the \\(X\\)-specific conditional average treatment effect (CATE), \\(\\psi_X(\\cdot)\\), with \\(X\\subseteq\\text{nd}(A;\\mathcal{G})\\)3:, are two of the most commonly investigated causal effects/estimands. For binary \\(A\\), they correspond to difference functionals of the interventional distribution, and are defined as:\n\\[\n\\begin{aligned}\n    \\psi &:= \\Delta_a \\mathbb{E}\\left[Y\\mid do(A=a) \\right]\\\\\n    \\psi_X(x) &:= \\Delta_a \\mathbb{E}\\left[Y\\mid do(A=a),X=x \\right],\\,  x\\in\\text{supp}\\, X,\\, X\\subseteq\\text{nd}(A;\\mathcal{G})\n\\end{aligned}\n\\]\nAn interventional distribution or causal effect is said to be nonparametrically identifiable from positive \\(P(\\mathcal{V})\\) if it can be uniquely computed from it (using the conditional independence statements embedded in \\(\\mathcal{G}\\) and its mutilation). In other words, a query \\(Q\\) such as interventional distribution, ATE, or CATE, is nonparametrically identifiable from \\(P(\\mathcal{V})\\), if there exists a functional/algorithm \\(\\Psi_\\mathcal{G}:P(\\mathcal{V})\\mapsto Q\\), such that such that it returns a unique value up to some equivalent relation."
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#uncertainty-modeling-and-quantification-in-structural-causal-models",
    "href": "posts/2023-09-15_structureUQ/index.html#uncertainty-modeling-and-quantification-in-structural-causal-models",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Uncertainty modeling and quantification in structural causal models",
    "text": "Uncertainty modeling and quantification in structural causal models\nThe two natures of uncertainty, aleatoric and epistemic (Hüllermeier and Waegeman 2021), can be associated with different parts of an SCM.\n\nAleatoric uncertainty\nIt is generally induced by randomness, i.e. \\(P(\\mathcal{U})\\) and, in turn, \\(P(\\mathcal{V})\\).\n\n\nEpistemic uncertainty\nIt is the result of working with unknowns. It can be further broken down into:\n\nModel/mechanism uncertainty: induced by unknown \\(\\mathcal{F}\\). Its analytic treatment depends on where the true mechanism lies in relation with the working models/hypotheses \\(\\mathcal{M}\\) (linear regressions, neural nets, etc.).\n\n\\(\\mathcal{M}\\)-closed world: If \\(\\mathcal{F}\\in\\mathcal{M}\\), model uncertainty can be integrated via Bayesian model-averaging.\n\\(\\mathcal{M}\\)-open/complete world: If \\(\\mathcal{F}\\notin\\mathcal{M}\\), model uncertainty can be integrated via Bayesian stacking of predictive distributions (Yao et al. 2018).\n\nStructure uncertainty: induced by unknown \\(\\mathcal{G}\\), typically under the causal sufficiency assumption, i.e., all relevant endogenous variables \\(\\mathcal{V}\\) are observed, but the graph connecting them is not (Kitson et al. 2023).\n\nHere, ‘’relevant’’ depends on the downstream task. If the goal is full structure learning, then all \\(\\mathcal{V}\\) must be observed. If the goal is downstream causal inference, then a minimal adjustment set must be observed.\n\nIdentification uncertainty: induced by latent/hidden \\(V_H\\subset\\mathcal{V}\\), where variables \\(V_H\\) are needed for identification of causal and counterfactual queries.\n\nWhen point-identification is not possible due to latent variables, such as unmeasured confounders, partial-identification (bounds) can still be informative (Chernozhukov et al. 2022)."
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#footnotes",
    "href": "posts/2023-09-15_structureUQ/index.html#footnotes",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome authors do not include \\(\\mathcal{G}\\) directly in \\(\\mathcal{M}\\), but say that \\(\\mathcal{G}\\) is associated with \\(\\mathcal{M}\\). This might be useful in some uncertainty-related context, such as when there are hidden variables, as many marginal graphs can be associated with the same SCM.↩︎\n\\(\\text{pa}(A;\\mathcal{G})\\) stands for the parents of node \\(A\\) in \\(\\mathcal{G}\\).↩︎\n\\(\\text{nd}(A;\\mathcal{G})\\) stands for the non-descendants of node \\(A\\) in \\(\\mathcal{G}\\).↩︎"
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#blah-blah",
    "href": "posts/2023-09-15_structureUQ/index.html#blah-blah",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Blah blah",
    "text": "Blah blah\nThe most complete and versatile axiomatic treatment of causal inference is given by the structural causal model (SCM) framework, by celebrated computer scientist and philosopher Dr. Judea Pearl (Pearl 2009). Let us give a simple example:\nConsider an experiment where we apply an external force \\(F\\) on objects with different masses \\(m\\) and then measure their induced acceleration \\(a\\). Let us put all these variables together: \\(\\mathcal{V}=\\{m,F,a\\}\\). Here, \\(m\\) and \\(F\\) are controlled (possibly randomly assigned) by the researcher, so they might be considered exogenous.\nAcceleration measurements is noisy, due to random measurement errors or residual forces applied (air current). Such noise is captured by random variable \\(U_a\\). Let us put all exogenous variables and noises together in \\(\\mathcal{U}=\\{m,F,U_a\\}\\). Observations from these variables come respectively from \\(P(m), P(F), P(U_a)\\) (independent) distributions.\nSince we measure the induced acceleration after an applied force, and we know there’s a theoretical (causal) relation between them, we can say that such acceleration is caused by the force. We can represent this graphically as \\(F\\rightarrow a\\). We also know changes in the mass being pushed induce changes in the resulting acceleration, so the \\(m\\) is also a cause of \\(a\\), or \\(m\\rightarrow a\\). The external force is applied independent of the mass, so none is cause of the other. We can represent succinctly such causal structure with a directed acyclic graph \\(\\mathcal{G}\\):\n\\[\n\\mathcal{G} :\\quad F\\rightarrow a\\leftarrow m\n\\] By Newton’s second law formula (with noise), measured acceleration can be expressed as \\(a = m^{-1}F+U_a\\). This can be shortly expressed as \\(a=f_a(m,F,U_a)\\), where \\(f_a\\) is the acceleration’s causal mechanism: a deterministic function defined as \\(f_a(x,y,z)=x^{-1}y+z\\)\nThen, the tuple of mathematical objects \\(\\mathcal{M}=(\\mathcal{V},\\mathcal{U},\\mathcal{G},f_a,{P}(\\mathcal{U}))\\) is a structural causal model (SCM) that fully describes the system, and can be leveraged to answer queries in three levels:\n\nObservational: keeping the mass constant at \\(m=2\\)kg, what is the observed curve \\(F\\) vs. \\(a\\)?\nCausal: what’s the average change in \\(a\\) if \\(F\\) changes from \\(F=1\\)N to \\(F=2\\)N with a mass of \\(1\\)kg?\nCounterfactual: what would have been the value of \\(a\\) under \\(F=2\\)N for a mass \\(m=1\\)kg that actually experienced \\(F=1\\)N and measured \\(a=1\\)?"
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#structural-causal-models",
    "href": "posts/2023-09-15_structureUQ/index.html#structural-causal-models",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Structural causal models",
    "text": "Structural causal models\nThe most complete and versatile axiomatic treatment of causal inference is given by the structural causal model (SCM) framework, developed by celebrated computer scientist and philosopher Dr. Judea Pearl (Pearl 2009). A brief introduction of SCM is given in a previous blog post."
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#structure-uncertainty",
    "href": "posts/2023-09-15_structureUQ/index.html#structure-uncertainty",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Structure uncertainty",
    "text": "Structure uncertainty\nThe most complete and versatile axiomatic treatment of causal inference is given by the structural causal model (SCM) framework, by celebrated computer scientist and philosopher Dr. Judea Pearl (Pearl 2009). Let us give a simple example:\nConsider an experiment where we apply an external force \\(F\\) on objects with different masses \\(m\\) and then measure their induced acceleration \\(a\\). Let us put all these variables together: \\(\\mathcal{V}=\\{m,F,a\\}\\). Here, \\(m\\) and \\(F\\) are controlled (possibly randomly assigned) by the researcher, so they might be considered exogenous.\nAcceleration measurements is noisy, due to random measurement errors or residual forces applied (air current). Such noise is captured by random variable \\(U_a\\). Let us put all exogenous variables and noises together in \\(\\mathcal{U}=\\{m,F,U_a\\}\\). Observations from these variables come respectively from \\(P(m), P(F), P(U_a)\\) (independent) distributions.\nSince we measure the induced acceleration after an applied force, and we know there’s a theoretical (causal) relation between them, we can say that such acceleration is caused by the force. We can represent this graphically as \\(F\\rightarrow a\\). We also know changes in the mass being pushed induce changes in the resulting acceleration, so the \\(m\\) is also a cause of \\(a\\), or \\(m\\rightarrow a\\). The external force is applied independent of the mass, so none is cause of the other. We can represent succinctly such causal structure with a directed acyclic graph \\(\\mathcal{G}\\):\n\\[\n\\mathcal{G} :\\quad F\\rightarrow a\\leftarrow m\n\\] By Newton’s second law formula (with noise), measured acceleration can be expressed as \\(a = m^{-1}F+U_a\\). This can be shortly expressed as \\(a=f_a(m,F,U_a)\\), where \\(f_a\\) is the acceleration’s causal mechanism: a deterministic function defined as \\(f_a(x,y,z)=x^{-1}y+z\\)\nThen, the tuple of mathematical objects \\(\\mathcal{M}=(\\mathcal{V},\\mathcal{U},\\mathcal{G},f_a,{P}(\\mathcal{U}))\\) is a structural causal model (SCM) that fully describes the system, and can be leveraged to answer queries in three levels:\n\nObservational: keeping the mass constant at \\(m=2\\)kg, what is the observed curve \\(F\\) vs. \\(a\\)?\nCausal: what’s the average change in \\(a\\) if \\(F\\) changes from \\(F=1\\)N to \\(F=2\\)N with a mass of \\(1\\)kg?\nCounterfactual: what would have been the value of \\(a\\) under \\(F=2\\)N for a mass \\(m=1\\)kg that actually experienced \\(F=1\\)N and measured \\(a=1\\)?"
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#structure-uncertainty-and-causal-inference",
    "href": "posts/2023-09-15_structureUQ/index.html#structure-uncertainty-and-causal-inference",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Structure uncertainty and causal inference",
    "text": "Structure uncertainty and causal inference\nConsider the case of a data-generating process (DGP) consisting of a set of pre-treatment variables \\(\\mathcal{W}\\), a binary exposure \\(A\\), and a continuous outcome \\(Y\\). We are interested in estimating the average treatment effect (ATE), given by:\n\\[\n    \\psi = \\Delta_a \\mathbb{E}\\left[Y\\mid do(A=a) \\right]\n\\] To estimate such a parameter from observational data on \\(\\mathcal{V}=\\{\\mathcal{W},A,Y\\}\\), identification must be established beforehand. A causal estimand is nonparametrically identified from the joint distribution \\(P(\\mathcal{V})\\) if it can uniquely computed as a functional of it. In other words, \\(Q\\) is identified if there exists a functional/algorithm \\(\\Psi_\\mathcal{G}:P(\\mathcal{V})\\mapsto Q\\), such that such that it returns a unique value up to some equivalent relation.\nUnder the graphical assumption of back-door admissibility of \\(\\mathcal{W}\\) in \\(\\mathcal{G}\\) (meaning that \\(\\mathcal{W}\\) blocks all paths between \\(A\\) and \\(Y\\) that start with an arrow pointing to \\(A\\)) we get conditional ignorability and thus identification of \\(\\psi\\), which now can be expressed as:\n\\[\n    \\psi = \\Psi_\\mathcal{G}[P(\\mathcal{V})] = \\mathbb{E}_\\mathcal{W}\\Delta_a \\mathbb{E}\\left[Y\\mid\\mathcal{W},A=a \\right]\n\\]\nSuch a graphical assumption validates other identification functionals, such as those given by inverse probability weighting (IPW) and doubly-robust approaches (AIPW, TMLE, DML). With finite samples, and a consistent estimator for the inner regression, a valid estimator is:\n\\[\n    \\hat{\\psi} = N^{-1}\\sum_{i=1}^N\\Delta_a \\hat{\\mathbb{E}}\\left[Y\\mid\\mathcal{W}^i,A=a \\right]\n\\]\n\n\n\n\n\n\nNote\n\n\n\nUnder ignorability, the structure \\(\\mathcal{G}\\) is not required, i.e., if \\(\\mathcal{G}\\) is unknown, \\(\\hat{\\psi}\\) can still be constructed and will be consistent. The set \\(\\mathcal{W}\\) might contain variables that are not confounders, and other unimportant variables, so \\(\\hat{\\psi}\\) might be statistically inefficient. Removing apparent unimportant variables from \\(\\mathcal{W}\\) might increase precision, but would potentially introduce confounding bias, if the eliminated variable is a weak confounding. Causal inferece is a bias-conservative endeavor, so we are more willing to sacrifice precision instead of introducing confounding bias."
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#post-treatment-variables-and-missing-outcome-data-justification-for-structure-learning",
    "href": "posts/2023-09-15_structureUQ/index.html#post-treatment-variables-and-missing-outcome-data-justification-for-structure-learning",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Post-treatment variables and missing outcome data: justification for structure learning",
    "text": "Post-treatment variables and missing outcome data: justification for structure learning\nConsider"
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#post-treatment-selection-induced-by-missing-outcome-data-justification-for-structure-learning",
    "href": "posts/2023-09-15_structureUQ/index.html#post-treatment-selection-induced-by-missing-outcome-data-justification-for-structure-learning",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Post-treatment selection induced by missing outcome data: justification for structure learning",
    "text": "Post-treatment selection induced by missing outcome data: justification for structure learning\nNow consider the inclusion of a set of post-treatment (pre-outcome) variables \\(\\mathcal{Z}_0\\) in the data. Besides, there is a unknown missingness mechanisms \\(R_Y\\) at play that dictates which observations of \\(Y\\) are selected (\\(R_Y=1\\)) and which are missing (\\(R_Y=0\\)). In this setting, the ATE is identified under the following graphical criteria:\n\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(\\mathcal{W}\\) in the substantive model: \\(Y\\perp A\\mid \\mathcal{W}\\) in \\((\\mathcal{G}\\ominus R_Y)[A\\!-\\!Y]\\)\n\\(\\mathcal{W}\\cup\\mathcal{Z}\\) \\(d\\)-separates \\(Y\\) from \\(R_Y\\) in the proper back-door graph: \\(Y\\perp R_Y\\mid \\mathcal{W},\\mathcal{Z}\\) in \\(\\mathcal{G}[A\\!-\\!Y]\\)\n\nThen, the ATE and its regression estimator, can be expressed as:\n\\[\\begin{aligned}\n\\psi_1 &= \\mathbb{E}_\\mathcal{W}\\,\\Delta_a\\mathbb{E}_{\\mathcal{Z}\\mid \\mathcal{W},A=a}\\, \\mathbb{E}[Y\\mid \\mathcal{W},\\mathcal{Z},A=a,R_Y=1]\\\\\n\\hat{\\psi}_1 &=  N^{-1}\\sum_{i=1}^N\\Delta_a\\hat{\\mathbb{E}}_{\\mathcal{Z}\\mid \\mathcal{W},A=a}\\, \\hat{\\mathbb{E}}[Y\\mid \\mathcal{W}^i,\\mathcal{Z}^i,A=a,R_Y=1]\n\\end{aligned}\n\\tag{1}\\]\nHowever, “weaker” graphical conditions can also lead to identification. For instance, if there exist \\(W\\subseteq\\mathcal{W}\\) and \\(Z_0,Z_1\\subseteq\\mathcal{Z}\\), such that:\n\n\\(W\\cup Z_0\\cup Z_1\\) \\(d\\)-separates \\(Y\\) from \\(R_Y\\) in the proper back-door graph: \\(Y\\perp R_Y\\mid W,Z_1,Z_2\\) in \\(\\mathcal{G}[A\\!-\\!Y]\\)\nAll non-causal paths between \\(A\\) and \\(Y\\) are blocked by \\(W\\) in the substantive model: \\(Y\\perp A\\mid W\\) in \\((\\mathcal{G}\\ominus R_Y)[A\\!-\\!Y]\\)\n\\(Z_0\\) does not contain forbidden nodes (mediators or their descendants) (nor descendants of A) and \\(Z_1\\) contains only forbidden nodes\n\nThen, the ATE and its regression estimator can be expressed as:\n\\[\\begin{aligned}\n\\psi_2 &= \\mathbb{E}_{W,Z_0}\\,\\Delta_a\\mathbb{E}_{Z_1\\mid W,Z_0,A=a}\\, \\mathbb{E}[Y\\mid W,Z_0,Z_1,A=a,R_Y=1]\\\\\n\\hat{\\psi}_2 &=  N^{-1}\\sum_{i=1}^N\\Delta_a\\hat{\\mathbb{E}}_{Z_1\\mid W,Z_0,A=a}\\, \\hat{\\mathbb{E}}[Y\\mid W^i,Z_0^i,Z_1^i,A=a,R_Y=1]\n\\end{aligned}\n\\tag{2}\\]\nMoreover, if in the true graph \\(\\mathcal{G}\\) the first condition is met with only \\(W\\cup Z_0\\) (\\(Z_1\\) not required), the final estimator requires only one regression model:\n\\[\\begin{aligned}\n\\psi_3 &= \\mathbb{E}_{W,Z_0}\\,\\Delta_a \\mathbb{E}[Y\\mid W,Z_0,A=a,R_Y=1]\\\\\n\\hat{\\psi}_3 &=  N^{-1}\\sum_{i=1}^N\\Delta_a\\hat{\\mathbb{E}}[Y\\mid W^i,Z_0^i,A=a,R_Y=1]\n\\end{aligned}\n\\tag{3}\\]\nIt is expected that, in this case, \\(\\hat{\\psi}_3\\) is is asymptotically more efficient than \\(\\hat{\\psi}_1\\) and \\(\\hat{\\psi}_2\\). An even more efficient estimator can leverage more information fom \\(\\mathcal{G}\\), such as conditioning on the remaining \\(pa(Y;\\mathcal{G})\\)\n\\[\n\\hat{\\psi}_4 =  N^{-1}\\sum_{i=1}^N\\Delta_a\\hat{\\mathbb{E}}[Y\\mid W^i,Z_0^i,T^i,A=a,R_Y=1]\n\\tag{4}\\]\nWhere \\(T=pa(Y;\\mathcal{G})\\,\\setminus\\ de(A;\\mathcal{G})\\,\\setminus (W\\cup Z_0\\cup A\\cup R_Y)\\)\n\n\n\n\n\n\nJustification for structure learning\n\n\n\nIn conclusion, recovery of the ATE from post-treatment selection induced by missing outcome data requires more information from the causal graph, not only to gain statistical efficiency but to inform modeling decisions, i.e., the number and type of regression models needed, the dimension of fluctuation parameter is targeted learning, and more."
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#constrained-structure-learning",
    "href": "posts/2023-09-15_structureUQ/index.html#constrained-structure-learning",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Constrained structure learning",
    "text": "Constrained structure learning\nBayesian structure learning is a type of Bayesian inverse problem that aims to infer the posterior distribution of \\(\\mathcal{G}\\) given data \\(\\boldsymbol{O}\\), i.e., \\(p(\\mathcal{G}\\mid\\boldsymbol{O})\\). Many MCMC, VI, and particle-based algorithms have been designed for this purpose, each with its sets of assumptions, strengths and weaknesses.\nIn our motivated case, structure learning should be constrain to respect the known topological partial order induced by time. This is, future variables cannot influence past variables. This implies adjacency matrix \\(\\mathcal{G}\\) can be expressed in block form as:\n\\[\n\\mathcal{G}=\\left(\\begin{array}{c|ccccc}  \n   & \\mathcal{W} & A & \\mathcal{Z} & R_Y & Y\\\\ \\hline  \n   \\mathcal{W} & 0 & C_1 & B_1 & B_1 & C_2\\\\\n   A & 0 & 0 & C_3 & C_3 & C_3\\\\\n   \\mathcal{Z} & 0 & 0 & D & B_2 & B_2\\\\\n   R_Y & 0 & 0 & 0 & 0 & 0\\\\\n   Y & 0 & 0 & 0 & 0 & 0\\\\\n\\end{array}\\right)\n\\]\nWhere \\(B_1,B_2\\) are bipartite graphs, \\(C_1,C_2,C_3\\) are bipartite graphs of one node on one side, and \\(D\\) is a DAG.\n\n\n\n\n\n\nNote\n\n\n\nWithin \\(\\mathcal{Z}\\) no time- or topological order is known a priori.\n\n\nThe problem is now equivalent to infering \\(p(B_1,B_2,C_1,C_2,C_3,D\\mid\\boldsymbol{O})\\)"
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#representation-of-bipartite-graph-and-dags",
    "href": "posts/2023-09-15_structureUQ/index.html#representation-of-bipartite-graph-and-dags",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Representation of bipartite graph and DAGs",
    "text": "Representation of bipartite graph and DAGs\nBayesian \\(\\mathcal{G}=S\\odot T\\)"
  },
  {
    "objectID": "posts/2023-09-15_structureUQ/index.html#representation-of-bipartite-graphs-and-dags",
    "href": "posts/2023-09-15_structureUQ/index.html#representation-of-bipartite-graphs-and-dags",
    "title": "Structure learning for downstream causal inference with missing outcome data",
    "section": "Representation of bipartite graphs and DAGs",
    "text": "Representation of bipartite graphs and DAGs\nLet \\(d_w=|\\mathcal{W}|\\), \\(d_z=|\\mathcal{Z}|\\) and \\(d=d_w+d_z+3\\). The (adjacency matrix) of DAG \\(\\mathcal{G}\\) lives in \\(\\{0,1\\}^{d\\times d}\\), and it can be represented by \\(\\mathcal{G}=S\\odot T\\), where \\(T\\in \\{0,1\\}^{d\\times d}\\) dictates the topological order, i.e, \\(T_{i,j}=1\\) if \\(i\\prec j\\); and \\(S\\in \\{0,1\\}^{d\\times d}\\) acts as a mask structure to disable the edge existence. \\(S\\) is needed to counter the post hoc ergo propter hoc fallacy: not because \\(j\\) follows \\(i\\), it is true that \\(i\\) causes \\(j\\). Any DAG can be represented this way (Annadani et al. 2023).\nAnnadani et al. (2023) propose a representation of \\(T\\) in term of node potentials \\(\\rho\\in\\mathbb{R}^d\\), such that \\(T_{i,j}=\\mathbb{I}(\\rho_j-\\rho_i&gt;0)\\), and leverages the equivalence \\(T=\\sigma(\\rho)\\Lambda\\sigma(\\rho)^\\top\\), with \\(\\sigma(\\rho)\\) being a permutation matrix of \\(\\rho\\), and \\(\\Lambda\\) being an upper-triangular matrix filled with ones. Such alternative formulation facilitates the computation of approximate gradients with respect to \\(\\rho\\) using techniques from the differentiable permutation literature.\nParameters of \\(S_{i,j}\\) are updated within the stochastic gradient Markov chain Monte Carlo (SG-MCMC) scheme either via variational inference (as a function of updated \\(\\rho\\)) or via own SG-MCMC. The authors point the former performs better, because coupling \\(S\\) with \\(\\rho\\) seems to be important. In other words, the embedding representation of \\(T\\) should also be informative of \\(S\\)."
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#objective",
    "href": "posts/2023-10-15_grapeSimulation/index.html#objective",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Objective",
    "text": "Objective\nThe goal is to estimate the average treatment effect (ATE) of a binary treatment on a continuous outcome, from observational data where the outcome is subject to a missingness/selection mechanism.\nFor this task, we employ:\n\nThe structural causal models (SCM) framework (Pearl 2009)\nA generated observational dataset\nA back-door admissible set of pre-treatment variables; i.e., the assumption of no latent confounding\nA missing-outcome mechanism that allows recoverability via IPW and regression adjustment\nThe targeted minimum-loss estimation (TMLE) framework (van der Laan and Rose 2011)"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#synthetic-data-from-substantive-model",
    "href": "posts/2023-10-15_grapeSimulation/index.html#synthetic-data-from-substantive-model",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Synthetic data from substantive model",
    "text": "Synthetic data from substantive model\nWe build a directed acyclic graph (DAG) \\(\\mathcal{G}\\) from domain knowledge and temporal-order constraints, involving the exposure \\(A\\), the outcome \\(Y\\), a set of confounders \\(W\\), and mediators \\(Z,M\\). We previously fit flexible nonlinear models (random forests) to learn the causal mechanisms \\({f}_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\) from real-world data on the subject. This allows us to employ learnt mechanisms to generate fake data from seeds (noises and exogenous variables) in a controlled environment, along with all necessary counterfactual variables.\nThe data consist of… XXX\n\nBy design, conditional ignorability is satisfied in the synthetic system using the full confounder set. It might not be satisfied in the system where the real-world data come from.\n\nGenerated substantive variables can be grouped in three categories:\n1. Pre-treatment / exogenous variables and the exposure:\n\n\\(W_1\\in\\mathbb{R}\\) = maternal vulnerability index: a PCA-based index summarizing mother’s diagnoses (+), level of education (-), age (-), and number of children (-)\n\\(W_2\\in\\{0,1\\}\\) = child’s sex at birth, female = 1\n\\(W_3\\in\\{0,1\\}\\) = prior prescription for ADHD medication, before grade 6\n\\(W_4\\in\\{0,1\\}\\) = diagnosis for comorbid disorders (ADHD-related, internalizing or externalizing) registered before grade 6\n\\(W_5\\in\\mathbb{R}\\) = raw score obtained at the mathematics national test for grade 5\n\\(W_6\\in\\mathbb{N}\\) = number of registrations for health services (GP and specialist) before grade 6\n\\(A\\in\\{0,1\\}\\) = prescription for ADHD medication between grades 6 and 8\n\n2. Mediator variables and the outcome:\n\n\\(Z\\in\\{0,1\\}\\) = diagnosis for comorbid disorders (ADHD-related, internalizing or externalizing) registered between grades 6 and 8\n\\(M\\in\\mathbb{N}\\) = number of registrations for health services (GP and specialist) between grades 6 and 8\n\\(Y\\in\\mathbb{R}\\) = raw score obtained at the mathematics national test for grade 8\n\n3. Counterfactual variables:\n\n\\(Z^A\\) = diagnosis for comorbid disorders (ADHD-related, internalizing or externalizing) registered between grades 6-8, had the individual taken treatment \\(A\\)\n\\(M^A\\) = number of registrations for health services (GP and specialist) between grades 6 and 8, had the individual taken treatment \\(A\\)\n\\(Y^{A}\\) = raw score obtained at the mathematics national test for grade 8, had the individual taken treatment \\(A\\)\n\\(ITE\\) = \\(Y^{1}-Y^{0}\\) = individual treatment effect"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#missingness-mechanisms-three-cases",
    "href": "posts/2023-10-15_grapeSimulation/index.html#missingness-mechanisms-three-cases",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Missingness mechanisms: three cases",
    "text": "Missingness mechanisms: three cases\nWe run the analysis under three different scenarios for the missingness mechanism of the outcome: \\(R_1\\), \\(R_2\\), and \\(R_3\\). Such mechanisms share a simple logit specification with coefficients \\(\\theta\\):\n\\[\nR = \\mathbb{I}[\\theta_1 + \\theta_2 W_2 + \\theta_3 W_3 + \\theta_4 A + \\theta_5 Z + \\theta_6\\log(1+ M) + U_R &gt; 0],\\quad U_R\\sim N(0,1)\n\\]\nDifferent configurations of \\(\\theta\\) generate missingness mechanisms of different strength.\n\nCase 1: severe selection: by setting \\(\\theta=(-0.48, 1.17, 0.85, -0.18, -0.23, -0.15)\\), we obtain around 50% of missing cases\nCase 2: medium selection: by setting \\(\\theta=(0.28, 0.97, 0.89, -0.13, -0.22, -0.12)\\), we obtain around 25% of missing cases\nCase 3: low selection: by setting \\(\\theta=(0.94, 0.70, 0.69, -0.10, -0.24, -0.10)\\), we obtain around 10% of missing cases"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#generating-the-data",
    "href": "posts/2023-10-15_grapeSimulation/index.html#generating-the-data",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Generating the data",
    "text": "Generating the data\nPackages and auxiliar functions required:\n\n\nCode\n# Load packages --------------------------------------------------------------\nlibrary(data.table)   # Processes dataframes\nlibrary(dplyr)        # Processes dataframes\nlibrary(kableExtra)   # Styles tables\nlibrary(speedglm)     # Performs fast fitting for GLM\nlibrary(nnls)         # Performs non-negative least squares\nlibrary(Rsolnp)       # Augmented Lagrange optimizer\nlibrary(sl3)          # Performs super-learning\nlibrary(tmle3)        # Performs TMLE\nlibrary(tmle3mediate) # Performs TMLE for mediation analysis\nlibrary(ggplot2)      # Plots\nlibrary(pracma)       # Performs dot product\nlibrary(estimatr)     # Performs robust standard errors\n\n\nA dataset (full.data) of \\(N=10\\,000\\) samples was generated:\n\n\nCode\n# Causal mechanisms ------------------------------------------------------------\n\n# Causal mechanism for treatment assignment\ncoef.A = c(0.00, 0.90, -0.09)\nfun.A = function(w, u){\n  dat = as.numeric(c(1, w, sign(w)*w^2))\n  lo = dot(coef.A, dat) + u\n  return(as.numeric(lo&gt;0))\n}\n\n# Causal mechanism for mediator 1\ncoef.M = c(-0.50, 1.00)\nfun.M = function(a, u){\n  dat = as.numeric(c(1, a))\n  li = dot(coef.M, dat) + u\n  return(li)\n}\n\n# Causal mechanism for mediator 2\ncoef.Z = c(4.20, 0.25, 0.30, 0.05)\nfun.Z = function(a, m, u){\n  dat = as.numeric(c(1, 2*a-1, m, (2*a-1)*m))\n  li = 0.12*(dot(coef.Z, dat) + u)^2\n  return(li)\n}\n\n# Causal mechanism for outcome\ncoef.Y = c(0.00, 1.80, 0.20, 0.75, 0.50, 2.00, 0.50, 0.80)\nfun.Y = function(w, a, m, z, u){\n  dat = as.numeric(c(1, w, w^3, 2*a-1, (2*a-1)*w, m, (2*a-1)*m, (2*a-1)*z))\n  li = dot(coef.Y, dat) + u\n  return(li)\n}\n\n# Causal mechanism for missingness mechanisms. R\ncoef.R = c(0.29, 0.54) \nfun.R = function(m, z, u){\n  dat = as.numeric(c(m, z))\n  lo = r.bias + dot(coef.R, dat) + u\n  return(as.numeric(lo&gt;0))\n}\n\n# Generate the data ------------------------------------------------------------\n\n# Number of samples\nN = 1e4\n\n# Seed\nset.seed(77)\n\n# Generate exogenous variables: independent confounders and noises, \n# plus fixed treatment assignments A.1 and A.0\nfull.data = data.table(noise.A = rnorm(N, 0, 1),\n                       noise.M = rnorm(N, 0, 1),\n                       noise.Z = rnorm(N, 0, 1),\n                       noise.Y = rnorm(N, 0, 11),\n                       noise.R1 = rnorm(N, 0, 1),\n                       noise.R2 = rnorm(N, 0, 1),\n                       noise.R3 = rnorm(N, 0, 1),\n                       W = rnorm(N, 0, 1),\n                       A.1 = 1, A.0 = 0)\n\n# Generate observations\nfull.data = full.data[, A:=mapply(fun.A, W, noise.A)] %&gt;%\n  .[, M:=mapply(fun.M, A, noise.M)] %&gt;%\n  .[, M.1:=mapply(fun.M, A.1, noise.M)] %&gt;%\n  .[, M.0:=mapply(fun.M, A.0, noise.M)] %&gt;%\n  .[, Z:=mapply(fun.Z, A, M, noise.Z)] %&gt;%\n  .[, Z.1:=mapply(fun.Z, A.1, M.1, noise.Z)] %&gt;%\n  .[, Z.0:=mapply(fun.Z, A.0, M.0, noise.Z)] %&gt;%\n  .[, Y:=mapply(fun.Y, W, A, M, Z, noise.Y)] %&gt;%\n  .[, Y.1:=mapply(fun.Y, W, A.1, M.1, Z.1, noise.Y)] %&gt;%\n  .[, Y.0:=mapply(fun.Y, W, A.0, M.0, Z.0, noise.Y)] %&gt;%\n  .[, ITE:=Y.1-Y.0] \n\n\n# Generate missingness indicator, case 1: R1\nr.bias = -1.20\nfull.data = full.data[, R1:=mapply(fun.R, M, Z, noise.R1)]\n\n# Generate missingness indicator, case 2: R2\nr.bias = -0.52\nfull.data = full.data[, R2:=mapply(fun.R, M, Z, noise.R2)]\n\n# Generate missingness indicator, case 2: R2\nr.bias = -0.43\nfull.data = full.data[, R3:=mapply(fun.R, M, Z, noise.R3)]\n\n# A glimpse of the data\nfull.data[,8:24] %&gt;% head() %&gt;%\n  kable(caption = \"Table 1: A glimpse at the generated data\")\n\n\n\nTable 1: A glimpse at the generated data\n\n\nW\nA.1\nA.0\nA\nM\nM.1\nM.0\nZ\nZ.1\nZ.0\nY\nY.1\nY.0\nITE\nR1\nR2\nR3\n\n\n\n\n0.6598987\n1\n0\n1\n-1.1125637\n-1.1125637\n-2.1125637\n2.862417\n2.862417\n2.1626663\n-9.889348\n-9.889348\n-16.45675\n6.567402\n1\n1\n1\n\n\n-0.2561566\n1\n0\n1\n1.0083732\n1.0083732\n0.0083732\n1.112559\n1.112559\n0.5776615\n19.453696\n19.453696\n14.34930\n5.104393\n0\n0\n1\n\n\n-0.3617037\n1\n0\n1\n-0.2945485\n-0.2945485\n-1.2945485\n5.422882\n5.422882\n4.3226736\n22.147969\n22.147969\n12.00778\n10.140192\n1\n1\n1\n\n\n-0.4995301\n1\n0\n1\n2.3817717\n2.3817717\n1.3817717\n3.136195\n3.136195\n2.0409439\n23.088961\n23.088961\n14.06501\n9.023953\n0\n1\n1\n\n\n-1.5825305\n1\n0\n0\n-1.1021504\n-0.1021504\n-1.1021504\n1.300471\n1.950634\n1.3004709\n21.726949\n25.643152\n21.72695\n3.916203\n1\n0\n0\n\n\n-0.7978039\n1\n0\n1\n-0.0289140\n-0.0289140\n-1.0289140\n2.196055\n2.196055\n1.4959814\n-6.400891\n-6.400891\n-11.52780\n5.126911\n1\n0\n1"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#generated-data-from-substantive-model",
    "href": "posts/2023-10-15_grapeSimulation/index.html#generated-data-from-substantive-model",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Generated data from substantive model",
    "text": "Generated data from substantive model\nWe build a directed acyclic graph (DAG) \\(\\mathcal{G}\\), involving the exposure \\(A\\), the outcome \\(Y\\), a confounder variable \\(W\\), and mediators of the effect \\(M,Z\\).\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  A ~ W,\n  M ~ A,\n  Z ~ A + M,\n  Y ~ W + A + M + Z\n) %&gt;% tidy_dagitty(layout = \"nicely\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()\n\n\n\n\n\nA fixed set of causal mechanisms \\({f}_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\) allows us to generate fake data from seeds (noises and exogenous variables) in a controlled environment, along with all necessary counterfactual variables.\nGenerated substantive variables are:\n\n\\(W\\in\\mathbb{R}\\) = confounder\n\\(A\\in\\{0,1\\}\\) = binary treatment\n\\(Y\\in\\mathbb{R}\\) = outcome\n\\(M,Z\\in\\mathbb{R}\\) = mediators of the effect of treatment on the outcome\n\nCounterfactual variables are:\n\n\\(M^A,Z^A\\) = value of mediators \\(M,Z\\) had the individual taken treatment \\(A\\)\n\\(Y^{A}\\) = value of the outcome had the individual taken treatment \\(A\\)\n\\(ITE\\) = \\(Y^{1}-Y^{0}\\) = individual treatment effect"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#missingness-mechanisms",
    "href": "posts/2023-10-15_grapeSimulation/index.html#missingness-mechanisms",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nWe run the analysis under three different scenarios for the missingness mechanism of the outcome: \\(R_1\\), \\(R_2\\), and \\(R_3\\). Such mechanisms share a simple logit specification on the mediators \\(M,Z\\):\n\\[\nR = \\mathbb{I}[\\theta_0 + \\theta_M M + \\theta_Z Z  + U_R &gt; 0],\\quad U_R\\sim N(0,1)\n\\]\nDifferent configurations of \\(\\theta\\) generate missingness mechanisms of different strength.\n\nCase 1: severe selection: by setting \\(\\theta=(-0.13, 1.05, 0.10, 0.51, -0.23, -0.17)\\), we obtain around 50% of missing cases\nCase 2: medium selection: by setting \\(\\theta=(0.60, 0.87, -0.10, 0.58, -0.17, -0.15)\\), we obtain around 25% of missing cases\nCase 3: low selection: by setting \\(\\theta=(1.24, 0.69, -0.16, 0.50, -0.16, -0.13)\\), we obtain around 10% of missing cases"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#generated-data-from-substantive-model-and-missingness-mechanism",
    "href": "posts/2023-10-15_grapeSimulation/index.html#generated-data-from-substantive-model-and-missingness-mechanism",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Generated data from substantive model and missingness mechanism",
    "text": "Generated data from substantive model and missingness mechanism\nWe build a directed acyclic graph (DAG) \\(\\mathcal{G}\\), involving the exposure \\(A\\), the outcome \\(Y\\), a confounder variable \\(W\\), mediators of the effect \\(M,Z\\), and missingness mechanism for the outcome \\(R\\)\n\n\nCode\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  A ~ W,\n  M ~ A,\n  Z ~ A + M,\n  Y ~ W + A + M + Z,\n  R ~ M + Z\n) %&gt;% tidy_dagitty(layout = \"kk\") %&gt;%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()\n\n\n\n\n\nA fixed set of causal mechanisms \\({f}_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V\\) allows us to generate fake data from seeds (noises and exogenous variables) in a controlled environment, along with all necessary counterfactual variables.\nGenerated substantive variables are:\n\n\\(W\\in\\mathbb{R}\\) = confounder\n\\(A\\in\\{0,1\\}\\) = binary treatment\n\\(Y\\in\\mathbb{R}\\) = outcome\n\\(M,Z\\in\\mathbb{R}\\) = mediators of the effect of treatment on the outcome\n\nCounterfactual variables are:\n\n\\(M^A,Z^A\\) = value of mediators \\(M,Z\\) had the individual taken treatment \\(A\\)\n\\(Y^{A}\\) = value of the outcome had the individual taken treatment \\(A\\)\n\\(ITE\\) = \\(Y^{1}-Y^{0}\\) = individual treatment effect\n\nWe employ the following nonlinear specifications for the causal mechanisms: \\[\n\\begin{aligned}\nW &= U_W \\quad & U_W\\sim N(0,1)\\\\\nA &= \\mathbb{I}[0.9W -0.09\\,\\text{sign}(W)\\,W^2 + U_A &gt; 0],\\quad & U_A\\sim N(0,1)\\\\\nM &= -0.50 + A + U_M,\\quad & U_M\\sim N(0,1)\\\\\nZ &=  0.12\\,[4.2 + 0.25\\,(2A-1) + 0.30M + 0.05\\,(2A-1)\\, M + U_Z]^2,\\quad & U_Z\\sim N(0,1)\\\\\nY &=  1.80W+ 0.20W^3 + 0.75\\,(2A-1) + 0.50\\,(2A-1)\\, W   & \\\\\n&\\qquad + 2.00M + 0.50\\,(2A-1)\\, M + 0.80\\,(2A-1)\\, Z+ U_Y,\\quad & U_Y\\sim N(0,11)\\\\\n\\end{aligned}\n\\]\n\nMissingness mechanism\nWe run the analysis under three different scenarios for the missingness mechanism of the outcome: \\(R_1\\), \\(R_2\\), and \\(R_3\\). Such mechanisms share a simple logit specification on the mediators \\(M,Z\\):\n\\[\nR = \\mathbb{I}[\\theta + 0.29 M + 0.54 Z  + U_R &gt; 0],\\quad U_R\\sim N(0,1)\n\\]\nDifferent configurations of \\(\\theta\\) generate missingness mechanisms of different strength.\n\nCase 1: severe selection: by setting \\(\\theta=-1.20\\), we obtain around 50% of missing cases\nCase 2: medium selection: by setting \\(\\theta=-0.52\\), we obtain around 25% of missing cases\nCase 3: low selection: by setting \\(\\theta=-0.43\\), we obtain around 10% of missing cases"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#estimators-to-compare",
    "href": "posts/2023-10-15_grapeSimulation/index.html#estimators-to-compare",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Estimators to compare",
    "text": "Estimators to compare\n\nTable 1: estimators to be compared in simulations\n\n\n\n\n\n\n\nEstimator\nMethod\nNote\n\n\n\n\nOracle PATE sample-based\nAverage of ITE in the whole sample\nImpossible to compute, we do not observe both counterfactuals nor missing outcome\n\n\nOracle SATE sample-based\nAverage of ITE in the selected sample\nImpossible to compute, we do not observe both counterfactuals\n\n\nStudent’s t-test\nMean difference of observed outcomes treated vs control\nIt suffers from confounding and selection biases\n\n\nTML pre-exposure\nTMLE adjusting only for pre-exposure \\(W\\)\nIt is not consistent in our DAG\n\n\nTML pre-processing\nTMLE using pre-processing tool for missing data in tmle package\nIt is not based on graphical criteria\n\n\nDoubly-inverse weighted\nOutcome difference weighted by propensity score and selection prob.\nIt is consistent in our DAG if models are correctly specified\n\n\nNested regressions\nPaper proposal 1\nIt is consistent in our DAG if models are correctly specified\n\n\nTML 2-steps\nPaper proposal 2\nIt is consistent in our DAG with multiple robusness conditions for models"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#case-1-severe-selection",
    "href": "posts/2023-10-15_grapeSimulation/index.html#case-1-severe-selection",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Case 1: severe selection",
    "text": "Case 1: severe selection\nHola"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#super-learning-procedure",
    "href": "posts/2023-10-15_grapeSimulation/index.html#super-learning-procedure",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Super-learning procedure",
    "text": "Super-learning procedure\nSome estimators are produced under a super-learning scheme. They are based on weighted stacks of a library of base estimators: i) saple mean, ii) generalized linear model (GLM), and iii) spline regression model (earth package).\n\n\nCode\n# Learning algorithms employed to learn treatment/outcome mechanisms -----------\n\n# Mean model\nlrnr_me = make_learner(Lrnr_mean) \n# GLM\nlrnr_lm = make_learner(Lrnr_glm_fast)\n# Spline regression\nlrnr_sp = make_learner(Lrnr_earth)  \n\n# Meta-learners: to stack together predictions from the learners ---------------\n\n# Combine continuous predictions with non-negative least squares\nmeta_C = make_learner(Lrnr_nnls)   \n\n# Combine binary predictions with logit likelihood (augmented Lagrange optimizer)\nmeta_B = make_learner(Lrnr_solnp,                     \n  loss_function = loss_loglik_binomial,              \n  learner_function = metalearner_logistic_binomial)   \n\n# Super-learners: learners + meta-learners together ----------------------------\n\n# Continuous super-learning\nsuper_C = Lrnr_sl$new(learners = list(lrnr_me, lrnr_lm, lrnr_sp), \n                      metalearner = meta_C)\n# Binary super-learning\nsuper_B = Lrnr_sl$new(learners = list(lrnr_me, lrnr_lm, lrnr_sp), \n                      metalearner = meta_B)\n# Super-learners put together\nsuper_list = list(A = super_B,\n                  Y = super_C)"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#case-1-severe-missingnessselection",
    "href": "posts/2023-10-15_grapeSimulation/index.html#case-1-severe-missingnessselection",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Case 1: severe missingness/selection",
    "text": "Case 1: severe missingness/selection\n\n\nCode\n#-------------------------------------------------------------------------------\n# Estimator A: Oracle PATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data$ITE)\npate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator B: Oracle SATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data[R1==1,ITE])\nsate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator 1: Unadjusted t-test -----------------------------------------------\n\nunadj.mod = lm_robust(Y~A,full.data[R1==1,])\n\n# Save estimate and CI\nunadj.est = unname(c(unadj.mod$conf.low[2],\n                     unadj.mod$conf.high[2],\n                     unadj.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 2: TMLE with only pre-treatment variables --------------------------\n\ntmle.pret = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = 'W', A = 'A', Y = 'Y'),   # Variables involved                         \n  data = full.data[R1==1,],                      # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.ptest = c(tmle.pret$summary$lower,\n               tmle.pret$summary$upper,\n               tmle.pret$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 3: TMLE with pre-processing tool for missingness -------------------\n\ntemp.dt = copy(full.data)\ntemp.dt$Y = ifelse(temp.dt$R1==0,NA,temp.dt$Y)\n\nprocessed = process_missing(temp.dt[,c('W','A','Y')],                    # data\n                            node_list = list(W = 'W',   # confounders\n                                             A = 'A',   # exposure\n                                             Y = 'Y'),  # outcome\n                            complete_nodes = c('W','A'),\n                            impute_nodes = \"Y\",\n                            max_p_missing = 0.9)  \n\ntmle.proc = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = c('W','delta_Y'),\n                   A = 'A', Y = 'Y'),            # Variables involved                         \n  data = processed$data,                         # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.proc = c(tmle.proc$summary$lower,\n               tmle.proc$summary$upper,\n               tmle.proc$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 4: Doubly-inverse weighted estimator -------------------------------\n\n# Model for treatment assignment\ntrain.A = make_sl3_Task(\n    data = full.data, outcome = 'A', covariates = 'W')\n\nA_fit = super_B$train(task = train.A)\nA_pre = A_fit$predict(task = train.A)\n\n# Model for missingness mechanism\ntrain.R = make_sl3_Task(\n    data = full.data, outcome = 'R1', covariates = c('W','A','M','Z'))\n\nR_fit = super_B$train(task = train.R)\nR_pre = R_fit$predict(task = train.R)\n\n# LM with doubly inverse weights\nfull.data = full.data[, DIW := (1/R_pre)*((A/A_pre)+(1-A)/(1-A_pre))]\n\ndiw.mod = lm_robust(Y~A,full.data[R1==1,], weights = DIW)\n\n# Save estimate and CI\ndiw.est = unname(c(diw.mod$conf.low[2],\n            diw.mod$conf.high[2],\n            diw.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 5: Nested regressions, T-learner -----------------------------------\n\n# Model for Q1\ntrain.Q1 = make_sl3_Task(\n    data = full.data[R1==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n\nQ1_fit = super_C$train(task = train.Q1)\n\n# Prediction task for Q1\npred.Q1 = make_sl3_Task(\n  data = full.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n\nfull.data$Q1 = Q1_fit$predict(task = pred.Q1)\n\n# Model for Q2.1 \ntrain.Q2.1 = make_sl3_Task(\n    data = full.data[A==1,], outcome = 'Q1', covariates = c('W'))\n\nQ2.1_fit = super_C$train(task = train.Q2.1)\n\npred.Q2.1 = make_sl3_Task(\n    data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2.1_fit$predict(task = pred.Q2.1)\n\n# Model for Q2.0\ntrain.Q2.0 = make_sl3_Task(\n    data = full.data[A==0,], outcome = 'Q1', covariates = c('W'))\n\nQ2.0_fit = super_C$train(task = train.Q2.0)\n\npred.Q2.0 = make_sl3_Task(\n    data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.0 = Q2.0_fit$predict(task = pred.Q2.0)\n\n# Predicted difference Q2.1 - Q2.0\n\nfull.data$delta = full.data$Q2.1-full.data$Q2.0\n\n# Bootstrap procedure to compute the standard deviation\n\nbsamples = c()\nfor(j in 1:30){\n  \n  # Boostraped data\n  ind = sample(1:N,N,replace=T)\n  bs.data = copy(full.data[ind,])\n  \n  # Model for Q1\n  train.bs.Q1 = make_sl3_Task(\n      data = bs.data[R1==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.Q1_fit = super_C$train(task = train.bs.Q1)\n  \n  # Prediction task for Q1\n  pred.bs.Q1 = make_sl3_Task(\n    data = bs.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.data$Q1 = bs.Q1_fit$predict(task = pred.bs.Q1)\n  \n  # Model for Q2.1.\n  train.bs.Q2.1 = make_sl3_Task(\n      data = bs.data[A==1,], outcome = 'Q1', covariates = c('W'))\n  \n  bs.Q2.1_fit = super_C$train(task = train.bs.Q2.1)\n  \n  pred.bs.Q2.1 = make_sl3_Task(\n      data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.1 = bs.Q2.1_fit$predict(task = pred.bs.Q2.1)\n  \n  # Model for Q2.0\n  train.bs.Q2.0 = make_sl3_Task(\n      data = bs.data[A==0,], outcome = 'Q1', covariates = c('W'))\n  \n  bsQ2.0_fit = super_C$train(task = train.bs.Q2.0)\n  \n  pred.bs.Q2.0 = make_sl3_Task(\n      data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.0 = Q2.0_fit$predict(task = pred.bs.Q2.0)\n  \n  # Add predicted difference Q2.1 - Q2.0 to boostrap vessel\n  bsamples = c(bsamples, mean(bs.data$Q2.1-bs.data$Q2.0))\n}\n\n# Save estimate and CI\nnesreg.T = c(mean(full.data$delta)-qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta)+qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta))\n\n#-------------------------------------------------------------------------------\n# Estimator 6: 2-step TMLE  ----------------------------------------------------\n\n# STEP 1 ------------------\n\n# Define clever variable\nfull.data = full.data[, clever.H1 := (1/R_pre)*((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.1 = lm(Y ~ -1 + offset(Q1) + clever.H1, data=full.data[R1==1,])\n\n# Auxiliary data frame for prediction\ntemp.dt = copy(full.data)\n\n# Using estimated fluctuation parameter, update Q1.1\ntemp.dt$A = 1\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.1 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.1 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.1,\n                                                 clever.H1=(1/R_pre)*(1/A_pre)))\n\n# Using estimated fluctuation parameter, update Q1.0\ntemp.dt$A = 0\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.0 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.0 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.0,\n                                                 clever.H1=(1/R_pre)*(-1/(1-A_pre))))\n\n# Learn Q2.1 from up.Q1.1, using A=1 cases\ntemp.dt = copy(full.data[A==1,])\n\ntrain.Q2 = make_sl3_Task(\n    data = temp.dt, outcome = 'up.Q1.1', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n    data = full.data, outcome = 'up.Q1.1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2_fit$predict(task = pred.Q2)\n\n# Learn Q2.0 from up.Q1.0, using A=0 cases\ntemp.dt = copy(full.data[A==0,])\n\ntrain.Q2 = make_sl3_Task(\n    data = temp.dt, outcome = 'up.Q1.0', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n    data = full.data, outcome = 'up.Q1.0', covariates = c('W'))\n\nfull.data$Q2.0 = Q2_fit$predict(task = pred.Q2)\n\n# STEP 2 ------------------\n\n# Compute the observed values for up.Q1 and Q2\n# Define clever variable\nfull.data = full.data[, up.Q1.A := A*up.Q1.1 + (1-A)*up.Q1.0 ] %&gt;% \n  .[, Q2.A := A*Q2.1 + (1-A)*Q2.0 ] %&gt;% \n  .[, clever.H2 := ((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.2 = lm(up.Q1.A ~ -1 + offset(Q2.A) + clever.H2, data=full.data)\n\n# Using estimated fluctuation parameter, update Q2.1\nfull.data$up.Q2.1 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.1,\n                                                 clever.H2=1/A_pre))\n\n# Using estimated fluctuation parameter, update Q2.0\nfull.data$up.Q2.0 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.0,\n                                                 clever.H2=-1/(1-A_pre)))\n\n# Compute the updated difference Q2.1 - Q2.0\n# Compute the observed value for up.Q2\n# Compute the value of the efficient influence function\nfull.data = full.data[, delta.up.Q2 := up.Q2.1-up.Q2.0] %&gt;%\n  .[, up.Q2.A := A*up.Q2.1 + (1-A)*up.Q2.0] %&gt;%\n  .[, EIF := delta.up.Q2 + clever.H2*(up.Q1.A - up.Q2.A) + clever.H1*(Y - up.Q1.A)*R1]\n\n# Using the EIF, compute the asymptotic error\nasymp.sd = sd(full.data$EIF)/sqrt(N)\n\n# Save estimate and CI\ntmle.2step = c(mean(full.data$delta.up.Q2)-qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2)+qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2))\n\n\n#-------------------------------------------------------------------------------\n# Visualization of results -----------------------------------------------------\n\n# PUT ALL TOGETHER\nestimators = data.table(rbind(pate,sate,unadj.est,tmle.ptest,tmle.proc,\n                              diw.est,nesreg.T,tmle.2step))\ncolnames(estimators) = c('lower','upper','point.est')\nestimators$type = c('Oracle PATE','Oracle SATE',\"Student's t-test\",'TML PreExp','TML PreProc',\n                    'DIW','Nested Reg','TML 2-Step')\n\nestimators$type = factor(estimators$type,levels = rev(estimators$type))\n\n# Estimate and CI plot\nggplot(estimators, aes(y=type, x=point.est, group=type)) +\n      geom_point(position=position_dodge(0.78)) +\n      geom_errorbar(aes(xmin=lower, xmax=upper, color=type),\n                    width=0.5, position=position_dodge(0.78)) + \n  guides(color=FALSE) + labs(x='Estimate',y='Estimator') +\n  theme_linedraw() + xlim(c(3.3,11.1)) + geom_vline(xintercept = pate[3], linetype=\"dashed\")"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#case-2-moderate-missingnessselection",
    "href": "posts/2023-10-15_grapeSimulation/index.html#case-2-moderate-missingnessselection",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Case 2: moderate missingness/selection",
    "text": "Case 2: moderate missingness/selection\n\n\nCode\n#-------------------------------------------------------------------------------\n# Estimator A: Oracle PATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data$ITE)\npate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator B: Oracle SATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data[R2==1,ITE])\nsate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator 1: Unadjusted t-test -----------------------------------------------\n\nunadj.mod = lm_robust(Y~A,full.data[R2==1,])\n\n# Save estimate and CI\nunadj.est = unname(c(unadj.mod$conf.low[2],\n                     unadj.mod$conf.high[2],\n                     unadj.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 2: TMLE with only pre-treatment variables --------------------------\n\ntmle.pret = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = 'W', A = 'A', Y = 'Y'),   # Variables involved                         \n  data = full.data[R2==1,],                      # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.ptest = c(tmle.pret$summary$lower,\n               tmle.pret$summary$upper,\n               tmle.pret$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 3: TMLE with pre-processing tool for missingness -------------------\n\ntemp.dt = copy(full.data)\ntemp.dt$Y = ifelse(temp.dt$R2==0,NA,temp.dt$Y)\n\nprocessed = process_missing(temp.dt[,c('W','A','Y')],                    # data\n                            node_list = list(W = 'W',   # confounders\n                                             A = 'A',   # exposure\n                                             Y = 'Y'),  # outcome\n                            complete_nodes = c('W','A'),\n                            impute_nodes = \"Y\",\n                            max_p_missing = 0.9)  \n\ntmle.proc = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = c('W','delta_Y'),\n                   A = 'A', Y = 'Y'),            # Variables involved                         \n  data = processed$data,                         # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.proc = c(tmle.proc$summary$lower,\n              tmle.proc$summary$upper,\n              tmle.proc$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 4: Doubly-inverse weighted estimator -------------------------------\n\n# Model for treatment assignment\ntrain.A = make_sl3_Task(\n  data = full.data, outcome = 'A', covariates = 'W')\n\nA_fit = super_B$train(task = train.A)\nA_pre = A_fit$predict(task = train.A)\n\n# Model for missingness mechanism\ntrain.R = make_sl3_Task(\n  data = full.data, outcome = 'R2', covariates = c('W','A','M','Z'))\n\nR_fit = super_B$train(task = train.R)\nR_pre = R_fit$predict(task = train.R)\n\n# LM with doubly inverse weights\nfull.data = full.data[, DIW := (1/R_pre)*((A/A_pre)+(1-A)/(1-A_pre))]\n\ndiw.mod = lm_robust(Y~A,full.data[R2==1,], weights = DIW)\n\n# Save estimate and CI\ndiw.est = unname(c(diw.mod$conf.low[2],\n                   diw.mod$conf.high[2],\n                   diw.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 5: Nested regressions, T-learner -----------------------------------\n\n# Model for Q1\ntrain.Q1 = make_sl3_Task(\n  data = full.data[R2==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n\nQ1_fit = super_C$train(task = train.Q1)\n\n# Prediction task for Q1\npred.Q1 = make_sl3_Task(\n  data = full.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n\nfull.data$Q1 = Q1_fit$predict(task = pred.Q1)\n\n# Model for Q2.1 \ntrain.Q2.1 = make_sl3_Task(\n  data = full.data[A==1,], outcome = 'Q1', covariates = c('W'))\n\nQ2.1_fit = super_C$train(task = train.Q2.1)\n\npred.Q2.1 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2.1_fit$predict(task = pred.Q2.1)\n\n# Model for Q2.0\ntrain.Q2.0 = make_sl3_Task(\n  data = full.data[A==0,], outcome = 'Q1', covariates = c('W'))\n\nQ2.0_fit = super_C$train(task = train.Q2.0)\n\npred.Q2.0 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.0 = Q2.0_fit$predict(task = pred.Q2.0)\n\n# Predicted difference Q2.1 - Q2.0\n\nfull.data$delta = full.data$Q2.1-full.data$Q2.0\n\n# Bootstrap procedure to compute the standard deviation\n\nbsamples = c()\nfor(j in 1:30){\n  \n  # Boostraped data\n  ind = sample(1:N,N,replace=T)\n  bs.data = copy(full.data[ind,])\n  \n  # Model for Q1\n  train.bs.Q1 = make_sl3_Task(\n    data = bs.data[R2==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.Q1_fit = super_C$train(task = train.bs.Q1)\n  \n  # Prediction task for Q1\n  pred.bs.Q1 = make_sl3_Task(\n    data = bs.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.data$Q1 = bs.Q1_fit$predict(task = pred.bs.Q1)\n  \n  # Model for Q2.1.\n  train.bs.Q2.1 = make_sl3_Task(\n    data = bs.data[A==1,], outcome = 'Q1', covariates = c('W'))\n  \n  bs.Q2.1_fit = super_C$train(task = train.bs.Q2.1)\n  \n  pred.bs.Q2.1 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.1 = bs.Q2.1_fit$predict(task = pred.bs.Q2.1)\n  \n  # Model for Q2.0\n  train.bs.Q2.0 = make_sl3_Task(\n    data = bs.data[A==0,], outcome = 'Q1', covariates = c('W'))\n  \n  bsQ2.0_fit = super_C$train(task = train.bs.Q2.0)\n  \n  pred.bs.Q2.0 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.0 = Q2.0_fit$predict(task = pred.bs.Q2.0)\n  \n  # Add predicted difference Q2.1 - Q2.0 to boostrap vessel\n  bsamples = c(bsamples, mean(bs.data$Q2.1-bs.data$Q2.0))\n}\n\n# Save estimate and CI\nnesreg.T = c(mean(full.data$delta)-qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta)+qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta))\n\n#-------------------------------------------------------------------------------\n# Estimator 6: 2-step TMLE  ----------------------------------------------------\n\n# STEP 1 ------------------\n\n# Define clever variable\nfull.data = full.data[, clever.H1 := (1/R_pre)*((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.1 = lm(Y ~ -1 + offset(Q1) + clever.H1, data=full.data[R2==1,])\n\n# Auxiliary data frame for prediction\ntemp.dt = copy(full.data)\n\n# Using estimated fluctuation parameter, update Q1.1\ntemp.dt$A = 1\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.1 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.1 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.1,\n                                                 clever.H1=(1/R_pre)*(1/A_pre)))\n\n# Using estimated fluctuation parameter, update Q1.0\ntemp.dt$A = 0\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.0 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.0 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.0,\n                                                 clever.H1=(1/R_pre)*(-1/(1-A_pre))))\n\n# Learn Q2.1 from up.Q1.1, using A=1 cases\ntemp.dt = copy(full.data[A==1,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.1', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2_fit$predict(task = pred.Q2)\n\n# Learn Q2.0 from up.Q1.0, using A=0 cases\ntemp.dt = copy(full.data[A==0,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.0', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.0', covariates = c('W'))\n\nfull.data$Q2.0 = Q2_fit$predict(task = pred.Q2)\n\n# STEP 2 ------------------\n\n# Compute the observed values for up.Q1 and Q2\n# Define clever variable\nfull.data = full.data[, up.Q1.A := A*up.Q1.1 + (1-A)*up.Q1.0 ] %&gt;% \n  .[, Q2.A := A*Q2.1 + (1-A)*Q2.0 ] %&gt;% \n  .[, clever.H2 := ((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.2 = lm(up.Q1.A ~ -1 + offset(Q2.A) + clever.H2, data=full.data)\n\n# Using estimated fluctuation parameter, update Q2.1\nfull.data$up.Q2.1 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.1,\n                                                 clever.H2=1/A_pre))\n\n# Using estimated fluctuation parameter, update Q2.0\nfull.data$up.Q2.0 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.0,\n                                                 clever.H2=-1/(1-A_pre)))\n\n# Compute the updated difference Q2.1 - Q2.0\n# Compute the observed value for up.Q2\n# Compute the value of the efficient influence function\nfull.data = full.data[, delta.up.Q2 := up.Q2.1-up.Q2.0] %&gt;%\n  .[, up.Q2.A := A*up.Q2.1 + (1-A)*up.Q2.0] %&gt;%\n  .[, EIF := delta.up.Q2 + clever.H2*(up.Q1.A - up.Q2.A) + clever.H1*(Y - up.Q1.A)*R2]\n\n# Using the EIF, compute the asymptotic error\nasymp.sd = sd(full.data$EIF)/sqrt(N)\n\n# Save estimate and CI\ntmle.2step = c(mean(full.data$delta.up.Q2)-qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2)+qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2))\n\n\n#-------------------------------------------------------------------------------\n# Visualization of results -----------------------------------------------------\n\n# PUT ALL TOGETHER\nestimators = data.table(rbind(pate,sate,unadj.est,tmle.ptest,tmle.proc,\n                              diw.est,nesreg.T,tmle.2step))\ncolnames(estimators) = c('lower','upper','point.est')\nestimators$type = c('Oracle PATE','Oracle SATE',\"Student's t-test\",'TML PreExp','TML PreProc',\n                    'DIW','Nested Reg','TML 2-Step')\n\nestimators$type = factor(estimators$type,levels = rev(estimators$type))\n\n# Estimate and CI plot\nggplot(estimators, aes(y=type, x=point.est, group=type)) +\n  geom_point(position=position_dodge(0.78)) +\n  geom_errorbar(aes(xmin=lower, xmax=upper, color=type),\n                width=0.5, position=position_dodge(0.78)) + \n  guides(color=FALSE) + labs(x='Estimate',y='Estimator') + \n  theme_linedraw() + xlim(c(3.3,11.1)) + geom_vline(xintercept = pate[3], linetype=\"dashed\")"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#case-3-low-missingnessselection-with-misspecification-of-q_1",
    "href": "posts/2023-10-15_grapeSimulation/index.html#case-3-low-missingnessselection-with-misspecification-of-q_1",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Case 3: low missingness/selection with misspecification of \\(Q_1\\)",
    "text": "Case 3: low missingness/selection with misspecification of \\(Q_1\\)\n\n\nCode\n#-------------------------------------------------------------------------------\n# Estimator A: Oracle PATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data$ITE)\npate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator B: Oracle SATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data[R3==1,ITE])\nsate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator 1: Unadjusted t-test -----------------------------------------------\n\nunadj.mod = lm_robust(Y~A,full.data[R3==1,])\n\n# Save estimate and CI\nunadj.est = unname(c(unadj.mod$conf.low[2],\n                     unadj.mod$conf.high[2],\n                     unadj.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 2: TMLE with only pre-treatment variables --------------------------\n\ntmle.pret = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = 'W', A = 'A', Y = 'Y'),   # Variables involved                         \n  data = full.data[R3==1,],                      # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.ptest = c(tmle.pret$summary$lower,\n               tmle.pret$summary$upper,\n               tmle.pret$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 3: TMLE with pre-processing tool for missingness -------------------\n\ntemp.dt = copy(full.data)\ntemp.dt$Y = ifelse(temp.dt$R3==0,NA,temp.dt$Y)\n\nprocessed = process_missing(temp.dt[,c('W','A','Y')],                    # data\n                            node_list = list(W = 'W',   # confounders\n                                             A = 'A',   # exposure\n                                             Y = 'Y'),  # outcome\n                            complete_nodes = c('W','A'),\n                            impute_nodes = \"Y\",\n                            max_p_missing = 0.9)  \n\ntmle.proc = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = c('W','delta_Y'),\n                   A = 'A', Y = 'Y'),            # Variables involved                         \n  data = processed$data,                         # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.proc = c(tmle.proc$summary$lower,\n              tmle.proc$summary$upper,\n              tmle.proc$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 4: Doubly-inverse weighted estimator -------------------------------\n\n# Model for treatment assignment\ntrain.A = make_sl3_Task(\n  data = full.data, outcome = 'A', covariates = 'W')\n\nA_fit = super_B$train(task = train.A)\nA_pre = A_fit$predict(task = train.A)\n\n# Model for missingness mechanism\ntrain.R = make_sl3_Task(\n  data = full.data, outcome = 'R3', covariates = c('W','A','M','Z'))\n\nR_fit = super_B$train(task = train.R)\nR_pre = R_fit$predict(task = train.R)\n\n# LM with doubly inverse weights\nfull.data = full.data[, DIW := (1/R_pre)*((A/A_pre)+(1-A)/(1-A_pre))]\n\ndiw.mod = lm_robust(Y~A,full.data[R3==1,], weights = DIW)\n\n# Save estimate and CI\ndiw.est = unname(c(diw.mod$conf.low[2],\n                   diw.mod$conf.high[2],\n                   diw.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 5: Nested regressions, T-learner -----------------------------------\n\n# Model for Q1: MISSPECIFIED\ntrain.Q1 = lm(Y ~ W+A+M+Z, data = full.data[R3==1,])\n\nfull.data$Q1 = predict(train.Q1, newdata = full.data)\n\n# Model for Q2.1 \ntrain.Q2.1 = make_sl3_Task(\n  data = full.data[A==1,], outcome = 'Q1', covariates = c('W'))\n\nQ2.1_fit = super_C$train(task = train.Q2.1)\n\npred.Q2.1 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2.1_fit$predict(task = pred.Q2.1)\n\n# Model for Q2.0\ntrain.Q2.0 = make_sl3_Task(\n  data = full.data[A==0,], outcome = 'Q1', covariates = c('W'))\n\nQ2.0_fit = super_C$train(task = train.Q2.0)\n\npred.Q2.0 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.0 = Q2.0_fit$predict(task = pred.Q2.0)\n\n# Predicted difference Q2.1 - Q2.0\n\nfull.data$delta = full.data$Q2.1-full.data$Q2.0\n\n# Bootstrap procedure to compute the standard deviation\n\nbsamples = c()\nfor(j in 1:60){\n  \n  # Boostraped data\n  ind = sample(1:N,N,replace=T)\n  bs.data = copy(full.data[ind,])\n  \n  # Model for Q1: MISSPECIFIED\n  train.bs.Q1 = lm(Y ~ W+A+M+Z, data = bs.data[R3==1,])\n\n  bs.data$Q1 = predict(train.bs.Q1, newdata = bs.data)\n  \n  # Model for Q2.1.\n  train.bs.Q2.1 = make_sl3_Task(\n    data = bs.data[A==1,], outcome = 'Q1', covariates = c('W'))\n  \n  bs.Q2.1_fit = super_C$train(task = train.bs.Q2.1)\n  \n  pred.bs.Q2.1 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.1 = bs.Q2.1_fit$predict(task = pred.bs.Q2.1)\n  \n  # Model for Q2.0\n  train.bs.Q2.0 = make_sl3_Task(\n    data = bs.data[A==0,], outcome = 'Q1', covariates = c('W'))\n  \n  bsQ2.0_fit = super_C$train(task = train.bs.Q2.0)\n  \n  pred.bs.Q2.0 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.0 = Q2.0_fit$predict(task = pred.bs.Q2.0)\n  \n  # Add predicted difference Q2.1 - Q2.0 to boostrap vessel\n  bsamples = c(bsamples, mean(bs.data$Q2.1-bs.data$Q2.0))\n}\n\n# Save estimate and CI\nnesreg.T = c(mean(full.data$delta)-qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta)+qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta))\n\n#-------------------------------------------------------------------------------\n# Estimator 6: 2-step TMLE  ----------------------------------------------------\n\n# STEP 1 ------------------\n\n# Define clever variable\nfull.data = full.data[, clever.H1 := (1/R_pre)*((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.1 = lm(Y ~ -1 + offset(Q1) + clever.H1, data=full.data[R3==1,])\n\n# Auxiliary data frame for prediction\ntemp.dt = copy(full.data)\n\n# Using estimated fluctuation parameter, update Q1.1\ntemp.dt$A = 1\nfull.data$Q1.1 = predict(train.Q1, newdata = temp.dt)\n\nfull.data$up.Q1.1 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.1,\n                                                 clever.H1=(1/R_pre)*(1/A_pre)))\n\n# Using estimated fluctuation parameter, update Q1.0\ntemp.dt$A = 0\nfull.data$Q1.0 = predict(train.Q1, newdata = temp.dt)\n\nfull.data$up.Q1.0 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.0,\n                                                 clever.H1=(1/R_pre)*(-1/(1-A_pre))))\n\n# Learn Q2.1 from up.Q1.1, using A=1 cases\ntemp.dt = copy(full.data[A==1,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.1', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2_fit$predict(task = pred.Q2)\n\n# Learn Q2.0 from up.Q1.0, using A=0 cases\ntemp.dt = copy(full.data[A==0,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.0', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.0', covariates = c('W'))\n\nfull.data$Q2.0 = Q2_fit$predict(task = pred.Q2)\n\n# STEP 2 ------------------\n\n# Compute the observed values for up.Q1 and Q2\n# Define clever variable\nfull.data = full.data[, up.Q1.A := A*up.Q1.1 + (1-A)*up.Q1.0 ] %&gt;% \n  .[, Q2.A := A*Q2.1 + (1-A)*Q2.0 ] %&gt;% \n  .[, clever.H2 := ((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.2 = lm(up.Q1.A ~ -1 + offset(Q2.A) + clever.H2, data=full.data)\n\n# Using estimated fluctuation parameter, update Q2.1\nfull.data$up.Q2.1 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.1,\n                                                 clever.H2=1/A_pre))\n\n# Using estimated fluctuation parameter, update Q2.0\nfull.data$up.Q2.0 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.0,\n                                                 clever.H2=-1/(1-A_pre)))\n\n# Compute the updated difference Q2.1 - Q2.0\n# Compute the observed value for up.Q2\n# Compute the value of the efficient influence function\nfull.data = full.data[, delta.up.Q2 := up.Q2.1-up.Q2.0] %&gt;%\n  .[, up.Q2.A := A*up.Q2.1 + (1-A)*up.Q2.0] %&gt;%\n  .[, EIF := delta.up.Q2 + clever.H2*(up.Q1.A - up.Q2.A) + clever.H1*(Y - up.Q1.A)*R3]\n\n# Using the EIF, compute the asymptotic error\nasymp.sd = sd(full.data$EIF)/sqrt(N)\n\n# Save estimate and CI\ntmle.2step = c(mean(full.data$delta.up.Q2)-qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2)+qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2))\n\n\n#-------------------------------------------------------------------------------\n# Visualization of results -----------------------------------------------------\n\n# PUT ALL TOGETHER\nestimators = data.table(rbind(pate,sate,unadj.est,tmle.ptest,tmle.proc,\n                              diw.est,nesreg.T,tmle.2step))\ncolnames(estimators) = c('lower','upper','point.est')\nestimators$type = c('Oracle PATE','Oracle SATE',\"Student's t-test\",'TML PreExp','TML PreProc',\n                    'DIW','Nested Reg','TML 2-Step')\n\nestimators$type = factor(estimators$type,levels = rev(estimators$type))\n\n# Estimate and CI plot\nggplot(estimators, aes(y=type, x=point.est, group=type)) +\n  geom_point(position=position_dodge(0.78)) +\n  geom_errorbar(aes(xmin=lower, xmax=upper, color=type),\n                width=0.5, position=position_dodge(0.78)) + \n  guides(color=FALSE) + labs(x='Estimate',y='Estimator') + \n  theme_linedraw() + xlim(c(3.3,11.1)) + geom_vline(xintercept = pate[3], linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\nCode\n#j=8\n#glue('lower whisker={estimators[j,1]}, median={estimators[j,3]}, upper whisker={estimators[j,2]}')"
  },
  {
    "objectID": "posts/2023-10-15_grapeSimulation/index.html#conclusions",
    "href": "posts/2023-10-15_grapeSimulation/index.html#conclusions",
    "title": "Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data",
    "section": "Conclusions",
    "text": "Conclusions\n\nNested regression estimator provides the best alternative under the assumption of correct models specifications. It can consistently recover the ATE from confounding and selection bias (under conditional ignorability and recoverability conditions), and it presents narrower confidence interval relative to the DIW and TML 2-steps estimators, which are also consistent in this scenario. Yet, it requires a bootstrap procedure to compute standard errors, which might be costly in complex models/datasets, and it fails to be consistent when one the outcome models are not correctly specified.\nTML 2-step estimator is less efficient than nested regression estimator, producing wider confidence intervals, but it is more efficient than the DIW estimator. Moreover, it remains consistent even when the outcome models are not correctly specified, provided the treatment assignment and missingness mechanisms are both correct."
  }
]