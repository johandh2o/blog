{
  "hash": "77e2cf4ccd182eaa3ea0726bb8ac5c01",
  "result": {
    "markdown": "---\ntitle: \"Simulation task: recovering causal effects from post-treatment selection induced by missing outcome data\"\ndescription: \"A simulation exercise on missing data, selection bias, causal inference and TMLE\"\ndate: \"2023-10-15\"\ncategories: [selection bias, mediation, regression, IPW, doubly-robust, TMLE]\nfontsize: 11pt\nformat: \n  html:\n    fig-width: 7.5\n    fig-height: 4\n    fig-align: center\n    code-fold: true\n    toc: true\nbibliography: references.bib\nimage: logo.png\n---\n\n\n------------------------------------------------------------------------\n\n## Objective\n\nThe goal is to estimate the **average treatment effect (ATE)** of a binary treatment on a continuous outcome, from **observational data** where **the outcome is subject to a missingness/selection mechanism**.\n\nFor this task, we employ:\n\n-   The structural causal models (SCM) framework [@PearlCausality]\n-   A generated observational dataset\n-   A back-door admissible set of pre-treatment variables; i.e., the assumption of no latent confounding\n-   A missing-outcome mechanism that allows recoverability via IPW and regression adjustment\n-   The targeted minimum-loss estimation (TMLE) framework [@TMLEbook1]\n\n## Generated data from substantive model and missingness mechanism\n\nWe build a directed acyclic graph (DAG) $\\mathcal{G}$, involving the exposure $A$, the outcome $Y$, a confounder variable $W$, mediators of the effect $M,Z$, and missingness mechanism for the outcome $R$ \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# DAG visualization\nlibrary(ggplot2)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndagify(\n  A ~ W,\n  M ~ A,\n  Z ~ A + M,\n  Y ~ W + A + M + Z,\n  R ~ M + Z\n) %>% tidy_dagitty(layout = \"kk\") %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(color='white',size=0.5) +\n  geom_dag_edges() +\n  geom_dag_text(color='black') +\n  theme_dag()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=480}\n:::\n:::\n\n\n\n\nA fixed set of causal mechanisms ${f}_V:\\text{supp}\\, \\text{pa}(V;\\mathcal{G})\\times \\text{supp}\\, U_V\\rightarrow\\text{supp}\\, V$ allows us to generate fake data from seeds (noises and exogenous variables) in a controlled environment, along with all necessary counterfactual variables.\n\nGenerated substantive variables are:\n\n-   $W\\in\\mathbb{R}$ = confounder\n-   $A\\in\\{0,1\\}$ = binary treatment\n-   $Y\\in\\mathbb{R}$ = outcome\n-   $M,Z\\in\\mathbb{R}$ = mediators of the effect of treatment on the outcome\n\nCounterfactual variables are:\n\n-   $M^A,Z^A$ = value of mediators $M,Z$ *had the individual taken treatment* $A$\n-   $Y^{A}$ = value of the outcome *had the individual taken treatment* $A$\n-   $ITE$ = $Y^{1}-Y^{0}$ = individual treatment effect\n\n\nWe employ the following nonlinear specifications for the causal mechanisms:\n$$\n\\begin{aligned}\nW &= U_W \\quad & U_W\\sim N(0,1)\\\\\nA &= \\mathbb{I}[0.9W -0.09\\,\\text{sign}(W)\\,W^2 + U_A > 0],\\quad & U_A\\sim N(0,1)\\\\\nM &= -0.50 + A + U_M,\\quad & U_M\\sim N(0,1)\\\\\nZ &=  0.12\\,[4.2 + 0.25\\,(2A-1) + 0.30M + 0.05\\,(2A-1)\\, M + U_Z]^2,\\quad & U_Z\\sim N(0,1)\\\\\nY &=  1.80W+ 0.20W^3 + 0.75\\,(2A-1) + 0.50\\,(2A-1)\\, W   & \\\\\n &\\qquad + 2.00M + 0.50\\,(2A-1)\\, M + 0.80\\,(2A-1)\\, Z+ U_Y,\\quad & U_Y\\sim N(0,11)\\\\\n\\end{aligned}\n$$\n\n### Missingness mechanism\n\n\nWe run the analysis under three different scenarios for the missingness mechanism of the outcome, the final one with two scenarios of misspecification on $Q_1$ and $(Q_2,R_3)$. Such $R$-mechanisms share a simple logit specification on the mediators $M,Z$:\n\n$$\nR = \\mathbb{I}[\\theta + 0.29 M + 0.54 Z  + U_R > 0],\\quad U_R\\sim N(0,1)\n$$\n\nDifferent configurations of $\\theta$ generate missingness mechanisms of different strength. \n\n- **Case 1: severe selection**: by setting $\\theta=-1.20$, we obtain around 50\\% of missing cases\n- **Case 2: medium selection**: by setting $\\theta=-0.35$, we obtain around 25\\% of missing cases\n- **Case 3: low selection**: by setting $\\theta=0.40$, we obtain around 10\\% of missing cases\n\n## Generating the data\n\nPackages and auxiliary functions required:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages --------------------------------------------------------------\nlibrary(data.table)   # Processes dataframes\nlibrary(dplyr)        # Processes dataframes\nlibrary(kableExtra)   # Styles tables\nlibrary(speedglm)     # Performs fast fitting for GLM\nlibrary(nnls)         # Performs non-negative least squares\nlibrary(Rsolnp)       # Augmented Lagrange optimizer\nlibrary(sl3)          # Performs super-learning\nlibrary(tmle3)        # Performs TMLE\nlibrary(tmle3mediate) # Performs TMLE for mediation analysis\nlibrary(ggplot2)      # Plots\nlibrary(pracma)       # Performs dot product\nlibrary(estimatr)     # Performs robust standard errors\n```\n:::\n\n\nA dataset (`full.data`) of $N=10\\,000$ samples was generated:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Causal mechanisms ------------------------------------------------------------\n\n# Causal mechanism for treatment assignment\ncoef.A = c(0.00, 0.90, -0.09)\nfun.A = function(w, u){\n  dat = as.numeric(c(1, w, sign(w)*w^2))\n  lo = dot(coef.A, dat) + u\n  return(as.numeric(lo>0))\n}\n\n# Causal mechanism for mediator 1\ncoef.M = c(-0.50, 1.00)\nfun.M = function(a, u){\n  dat = as.numeric(c(1, a))\n  li = dot(coef.M, dat) + u\n  return(li)\n}\n\n# Causal mechanism for mediator 2\ncoef.Z = c(4.20, 0.25, 0.30, 0.05)\nfun.Z = function(a, m, u){\n  dat = as.numeric(c(1, 2*a-1, m, (2*a-1)*m))\n  li = 0.12*(dot(coef.Z, dat) + u)^2\n  return(li)\n}\n\n# Causal mechanism for outcome\ncoef.Y = c(0.00, 1.80, 0.20, 0.75, 0.50, 2.00, 0.50, 0.80)\nfun.Y = function(w, a, m, z, u){\n  dat = as.numeric(c(1, w, w^3, 2*a-1, (2*a-1)*w, m, (2*a-1)*m, (2*a-1)*z))\n  li = dot(coef.Y, dat) + u\n  return(li)\n}\n\n# Causal mechanism for missingness mechanisms. R\ncoef.R = c(0.29, 0.54) \nfun.R = function(m, z, u){\n  dat = as.numeric(c(m, z))\n  lo = r.bias + dot(coef.R, dat) + u\n  return(as.numeric(lo>0))\n}\n\n# Generate the data ------------------------------------------------------------\n\n# Number of samples\nN = 1e4\n\n# Seed \nset.seed(77)\n\n# Generate exogenous variables: independent confounders and noises, \n# plus fixed treatment assignments A.1 and A.0\nfull.data = data.table(noise.A = rnorm(N, 0, 1),\n                       noise.M = rnorm(N, 0, 1),\n                       noise.Z = rnorm(N, 0, 1),\n                       noise.Y = rnorm(N, 0, 11),\n                       noise.R1 = rnorm(N, 0, 1),\n                       noise.R2 = rnorm(N, 0, 1),\n                       noise.R3 = rnorm(N, 0, 1),\n                       W = rnorm(N, 0, 1),\n                       A.1 = 1, A.0 = 0)\n\n# Generate observations\nfull.data = full.data[, A:=mapply(fun.A, W, noise.A)] %>%\n  .[, M:=mapply(fun.M, A, noise.M)] %>%\n  .[, M.1:=mapply(fun.M, A.1, noise.M)] %>%\n  .[, M.0:=mapply(fun.M, A.0, noise.M)] %>%\n  .[, Z:=mapply(fun.Z, A, M, noise.Z)] %>%\n  .[, Z.1:=mapply(fun.Z, A.1, M.1, noise.Z)] %>%\n  .[, Z.0:=mapply(fun.Z, A.0, M.0, noise.Z)] %>%\n  .[, Y:=mapply(fun.Y, W, A, M, Z, noise.Y)] %>%\n  .[, Y.1:=mapply(fun.Y, W, A.1, M.1, Z.1, noise.Y)] %>%\n  .[, Y.0:=mapply(fun.Y, W, A.0, M.0, Z.0, noise.Y)] %>%\n  .[, ITE:=Y.1-Y.0] \n\n\n# Generate missingness indicator, case 1: R1\nr.bias = -1.20\nfull.data = full.data[, R1:=mapply(fun.R, M, Z, noise.R1)]\n\n# Generate missingness indicator, case 2: R2\nr.bias = -0.35\nfull.data = full.data[, R2:=mapply(fun.R, M, Z, noise.R2)]\n\n# Generate missingness indicator, case 2: R3 y R4\nr.bias = 0.40\nfull.data = full.data[, R3:=mapply(fun.R, M, Z, noise.R3)]\n\n# A glimpse of the data\nfull.data[,8:24] %>% head() %>%\n  kable(caption = \"Table 1: A glimpse at the generated data\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n<caption>Table 1: A glimpse at the generated data</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> W </th>\n   <th style=\"text-align:right;\"> A.1 </th>\n   <th style=\"text-align:right;\"> A.0 </th>\n   <th style=\"text-align:right;\"> A </th>\n   <th style=\"text-align:right;\"> M </th>\n   <th style=\"text-align:right;\"> M.1 </th>\n   <th style=\"text-align:right;\"> M.0 </th>\n   <th style=\"text-align:right;\"> Z </th>\n   <th style=\"text-align:right;\"> Z.1 </th>\n   <th style=\"text-align:right;\"> Z.0 </th>\n   <th style=\"text-align:right;\"> Y </th>\n   <th style=\"text-align:right;\"> Y.1 </th>\n   <th style=\"text-align:right;\"> Y.0 </th>\n   <th style=\"text-align:right;\"> ITE </th>\n   <th style=\"text-align:right;\"> R1 </th>\n   <th style=\"text-align:right;\"> R2 </th>\n   <th style=\"text-align:right;\"> R3 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.6598987 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -1.1125637 </td>\n   <td style=\"text-align:right;\"> -1.1125637 </td>\n   <td style=\"text-align:right;\"> -2.1125637 </td>\n   <td style=\"text-align:right;\"> 2.862417 </td>\n   <td style=\"text-align:right;\"> 2.862417 </td>\n   <td style=\"text-align:right;\"> 2.1626663 </td>\n   <td style=\"text-align:right;\"> -9.889348 </td>\n   <td style=\"text-align:right;\"> -9.889348 </td>\n   <td style=\"text-align:right;\"> -16.45675 </td>\n   <td style=\"text-align:right;\"> 6.567402 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.2561566 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1.0083732 </td>\n   <td style=\"text-align:right;\"> 1.0083732 </td>\n   <td style=\"text-align:right;\"> 0.0083732 </td>\n   <td style=\"text-align:right;\"> 1.112559 </td>\n   <td style=\"text-align:right;\"> 1.112559 </td>\n   <td style=\"text-align:right;\"> 0.5776615 </td>\n   <td style=\"text-align:right;\"> 19.453696 </td>\n   <td style=\"text-align:right;\"> 19.453696 </td>\n   <td style=\"text-align:right;\"> 14.34930 </td>\n   <td style=\"text-align:right;\"> 5.104393 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.3617037 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -0.2945485 </td>\n   <td style=\"text-align:right;\"> -0.2945485 </td>\n   <td style=\"text-align:right;\"> -1.2945485 </td>\n   <td style=\"text-align:right;\"> 5.422882 </td>\n   <td style=\"text-align:right;\"> 5.422882 </td>\n   <td style=\"text-align:right;\"> 4.3226736 </td>\n   <td style=\"text-align:right;\"> 22.147969 </td>\n   <td style=\"text-align:right;\"> 22.147969 </td>\n   <td style=\"text-align:right;\"> 12.00778 </td>\n   <td style=\"text-align:right;\"> 10.140192 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.4995301 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 2.3817717 </td>\n   <td style=\"text-align:right;\"> 2.3817717 </td>\n   <td style=\"text-align:right;\"> 1.3817717 </td>\n   <td style=\"text-align:right;\"> 3.136195 </td>\n   <td style=\"text-align:right;\"> 3.136195 </td>\n   <td style=\"text-align:right;\"> 2.0409439 </td>\n   <td style=\"text-align:right;\"> 23.088961 </td>\n   <td style=\"text-align:right;\"> 23.088961 </td>\n   <td style=\"text-align:right;\"> 14.06501 </td>\n   <td style=\"text-align:right;\"> 9.023953 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -1.5825305 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> -1.1021504 </td>\n   <td style=\"text-align:right;\"> -0.1021504 </td>\n   <td style=\"text-align:right;\"> -1.1021504 </td>\n   <td style=\"text-align:right;\"> 1.300471 </td>\n   <td style=\"text-align:right;\"> 1.950634 </td>\n   <td style=\"text-align:right;\"> 1.3004709 </td>\n   <td style=\"text-align:right;\"> 21.726949 </td>\n   <td style=\"text-align:right;\"> 25.643152 </td>\n   <td style=\"text-align:right;\"> 21.72695 </td>\n   <td style=\"text-align:right;\"> 3.916203 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> -0.7978039 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> -0.0289140 </td>\n   <td style=\"text-align:right;\"> -0.0289140 </td>\n   <td style=\"text-align:right;\"> -1.0289140 </td>\n   <td style=\"text-align:right;\"> 2.196055 </td>\n   <td style=\"text-align:right;\"> 2.196055 </td>\n   <td style=\"text-align:right;\"> 1.4959814 </td>\n   <td style=\"text-align:right;\"> -6.400891 </td>\n   <td style=\"text-align:right;\"> -6.400891 </td>\n   <td style=\"text-align:right;\"> -11.52780 </td>\n   <td style=\"text-align:right;\"> 5.126911 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Estimators to compare\n\n| Estimator | Method | Note | \n|---------------|---------------|---------------|\n| Oracle PATE sample-based | Average of ITE in the whole sample | Impossible to compute, we do not observe both counterfactuals nor missing outcome | \n| Oracle SATE sample-based | Average of ITE in the selected sample | Impossible to compute, we do not observe both counterfactuals |\n| Student's t-test | Mean difference of observed outcomes treated vs control | It suffers from confounding and selection biases|\n| TML pre-exposure | TMLE adjusting only for pre-exposure $W$ | It is not consistent in our DAG |\n| TML pre-processing | TMLE using pre-processing tool for missing data in `tmle` package | It is not based on graphical criteria |\n| Doubly-inverse weighted | Outcome difference weighted by propensity score and selection prob. | It is consistent in our DAG if models are correctly specified |\n| [Nested regressions]{style=\"color:blue;\"} | [Paper proposal 1]{style=\"color:blue;\"} | [It is consistent in our DAG if models are correctly specified]{style=\"color:blue;\"} |\n| [TML 2-steps]{style=\"color:blue;\"} | [Paper proposal 2]{style=\"color:blue;\"} | [It is consistent in our DAG with multiple robusness conditions for models]{style=\"color:blue;\"} |\n\n: Table 1: estimators to be compared in simulations {.striped .hover} {tbl-colwidths=\"\\[25,35,40\\]\"}\n\n\n## Super-learning procedure\n\nSome estimators are produced under a super-learning scheme. They are based on weighted stacks of a library of base estimators: i) *saple mean*, ii) *generalized linear model* (GLM), and iii) *spline regression model* (`earth` package).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Learning algorithms employed to learn treatment/outcome mechanisms -----------\n\n# Mean model\nlrnr_me = make_learner(Lrnr_mean) \n# GLM\nlrnr_lm = make_learner(Lrnr_glm_fast)\n# Spline regression\nlrnr_sp = make_learner(Lrnr_earth)  \n\n# Meta-learners: to stack together predictions from the learners ---------------\n\n# Combine continuous predictions with non-negative least squares\nmeta_C = make_learner(Lrnr_nnls)   \n\n# Combine binary predictions with logit likelihood (augmented Lagrange optimizer)\nmeta_B = make_learner(Lrnr_solnp,                     \n  loss_function = loss_loglik_binomial,              \n  learner_function = metalearner_logistic_binomial)   \n\n# Super-learners: learners + meta-learners together ----------------------------\n\n# Continuous super-learning\nsuper_C = Lrnr_sl$new(learners = list(lrnr_me, lrnr_lm, lrnr_sp), \n                      metalearner = meta_C)\n# Binary super-learning\nsuper_B = Lrnr_sl$new(learners = list(lrnr_me, lrnr_lm, lrnr_sp), \n                      metalearner = meta_B)\n# Super-learners put together\nsuper_list = list(A = super_B,\n                  Y = super_C)              \n```\n:::\n\n\n\n## Case 1: severe missingness/selection\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#-------------------------------------------------------------------------------\n# Estimator A: Oracle PATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data$ITE)\npate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator B: Oracle SATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data[R1==1,ITE])\nsate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator 1: Unadjusted t-test -----------------------------------------------\n\nunadj.mod = lm_robust(Y~A,full.data[R1==1,])\n\n# Save estimate and CI\nunadj.est = unname(c(unadj.mod$conf.low[2],\n                     unadj.mod$conf.high[2],\n                     unadj.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 2: TMLE with only pre-treatment variables --------------------------\n\ntmle.pret = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = 'W', A = 'A', Y = 'Y'),   # Variables involved                         \n  data = full.data[R1==1,],                      # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.ptest = c(tmle.pret$summary$lower,\n               tmle.pret$summary$upper,\n               tmle.pret$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 3: TMLE with pre-processing tool for missingness -------------------\n\ntemp.dt = copy(full.data)\ntemp.dt$Y = ifelse(temp.dt$R1==0,NA,temp.dt$Y)\n\nprocessed = process_missing(temp.dt[,c('W','A','Y')],                    # data\n                            node_list = list(W = 'W',   # confounders\n                                             A = 'A',   # exposure\n                                             Y = 'Y'),  # outcome\n                            complete_nodes = c('W','A'),\n                            impute_nodes = \"Y\",\n                            max_p_missing = 0.9)  \n\ntmle.proc = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = c('W','delta_Y'),\n                   A = 'A', Y = 'Y'),            # Variables involved                         \n  data = processed$data,                         # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.proc = c(tmle.proc$summary$lower,\n               tmle.proc$summary$upper,\n               tmle.proc$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 4: Doubly-inverse weighted estimator -------------------------------\n\n# Model for treatment assignment\ntrain.A = make_sl3_Task(\n    data = full.data, outcome = 'A', covariates = 'W')\n\nA_fit = super_B$train(task = train.A)\nA_pre = A_fit$predict(task = train.A)\n\n# Model for missingness mechanism\ntrain.R = make_sl3_Task(\n    data = full.data, outcome = 'R1', covariates = c('W','A','M','Z'))\n\nR_fit = super_B$train(task = train.R)\nR_pre = R_fit$predict(task = train.R)\n\n# LM with doubly inverse weights\nfull.data = full.data[, DIW := (1/R_pre)*((A/A_pre)+(1-A)/(1-A_pre))]\n\ndiw.mod = lm_robust(Y~A,full.data[R1==1,], weights = DIW)\n\n# Save estimate and CI\ndiw.est = unname(c(diw.mod$conf.low[2],\n            diw.mod$conf.high[2],\n            diw.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 5: Nested regressions, T-learner -----------------------------------\n\n# Model for Q1\ntrain.Q1 = make_sl3_Task(\n    data = full.data[R1==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n\nQ1_fit = super_C$train(task = train.Q1)\n\n# Prediction task for Q1\npred.Q1 = make_sl3_Task(\n  data = full.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n\nfull.data$Q1 = Q1_fit$predict(task = pred.Q1)\n\n# Model for Q2.1 \ntrain.Q2.1 = make_sl3_Task(\n    data = full.data[A==1,], outcome = 'Q1', covariates = c('W'))\n\nQ2.1_fit = super_C$train(task = train.Q2.1)\n\npred.Q2.1 = make_sl3_Task(\n    data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2.1_fit$predict(task = pred.Q2.1)\n\n# Model for Q2.0\ntrain.Q2.0 = make_sl3_Task(\n    data = full.data[A==0,], outcome = 'Q1', covariates = c('W'))\n\nQ2.0_fit = super_C$train(task = train.Q2.0)\n\npred.Q2.0 = make_sl3_Task(\n    data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.0 = Q2.0_fit$predict(task = pred.Q2.0)\n\n# Predicted difference Q2.1 - Q2.0\n\nfull.data$delta = full.data$Q2.1-full.data$Q2.0\n\n# Bootstrap procedure to compute the standard deviation\n\nbsamples = c()\nfor(j in 1:30){\n  \n  # Boostraped data\n  ind = sample(1:N,N,replace=T)\n  bs.data = copy(full.data[ind,])\n  \n  # Model for Q1\n  train.bs.Q1 = make_sl3_Task(\n      data = bs.data[R1==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.Q1_fit = super_C$train(task = train.bs.Q1)\n  \n  # Prediction task for Q1\n  pred.bs.Q1 = make_sl3_Task(\n    data = bs.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.data$Q1 = bs.Q1_fit$predict(task = pred.bs.Q1)\n  \n  # Model for Q2.1.\n  train.bs.Q2.1 = make_sl3_Task(\n      data = bs.data[A==1,], outcome = 'Q1', covariates = c('W'))\n  \n  bs.Q2.1_fit = super_C$train(task = train.bs.Q2.1)\n  \n  pred.bs.Q2.1 = make_sl3_Task(\n      data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.1 = bs.Q2.1_fit$predict(task = pred.bs.Q2.1)\n  \n  # Model for Q2.0\n  train.bs.Q2.0 = make_sl3_Task(\n      data = bs.data[A==0,], outcome = 'Q1', covariates = c('W'))\n  \n  bsQ2.0_fit = super_C$train(task = train.bs.Q2.0)\n  \n  pred.bs.Q2.0 = make_sl3_Task(\n      data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.0 = Q2.0_fit$predict(task = pred.bs.Q2.0)\n  \n  # Add predicted difference Q2.1 - Q2.0 to boostrap vessel\n  bsamples = c(bsamples, mean(bs.data$Q2.1-bs.data$Q2.0))\n}\n\n# Save estimate and CI\nnesreg.T = c(mean(full.data$delta)-qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta)+qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta))\n\n#-------------------------------------------------------------------------------\n# Estimator 6: 2-step TMLE  ----------------------------------------------------\n\n# STEP 1 ------------------\n\n# Define clever variable\nfull.data = full.data[, clever.H1 := (1/R_pre)*((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.1 = lm(Y ~ -1 + offset(Q1) + clever.H1, data=full.data[R1==1,])\n\n# Auxiliary data frame for prediction\ntemp.dt = copy(full.data)\n\n# Using estimated fluctuation parameter, update Q1.1\ntemp.dt$A = 1\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.1 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.1 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.1,\n                                                 clever.H1=(1/R_pre)*(1/A_pre)))\n\n# Using estimated fluctuation parameter, update Q1.0\ntemp.dt$A = 0\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.0 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.0 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.0,\n                                                 clever.H1=(1/R_pre)*(-1/(1-A_pre))))\n\n# Learn Q2.1 from up.Q1.1, using A=1 cases\ntemp.dt = copy(full.data[A==1,])\n\ntrain.Q2 = make_sl3_Task(\n    data = temp.dt, outcome = 'up.Q1.1', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n    data = full.data, outcome = 'up.Q1.1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2_fit$predict(task = pred.Q2)\n\n# Learn Q2.0 from up.Q1.0, using A=0 cases\ntemp.dt = copy(full.data[A==0,])\n\ntrain.Q2 = make_sl3_Task(\n    data = temp.dt, outcome = 'up.Q1.0', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n    data = full.data, outcome = 'up.Q1.0', covariates = c('W'))\n\nfull.data$Q2.0 = Q2_fit$predict(task = pred.Q2)\n\n# STEP 2 ------------------\n\n# Compute the observed values for up.Q1 and Q2\n# Define clever variable\nfull.data = full.data[, up.Q1.A := A*up.Q1.1 + (1-A)*up.Q1.0 ] %>% \n  .[, Q2.A := A*Q2.1 + (1-A)*Q2.0 ] %>% \n  .[, clever.H2 := ((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.2 = lm(up.Q1.A ~ -1 + offset(Q2.A) + clever.H2, data=full.data)\n\n# Using estimated fluctuation parameter, update Q2.1\nfull.data$up.Q2.1 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.1,\n                                                 clever.H2=1/A_pre))\n\n# Using estimated fluctuation parameter, update Q2.0\nfull.data$up.Q2.0 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.0,\n                                                 clever.H2=-1/(1-A_pre)))\n\n# Compute the updated difference Q2.1 - Q2.0\n# Compute the observed value for up.Q2\n# Compute the value of the efficient influence function\nfull.data = full.data[, delta.up.Q2 := up.Q2.1-up.Q2.0] %>%\n  .[, up.Q2.A := A*up.Q2.1 + (1-A)*up.Q2.0] %>%\n  .[, EIF := delta.up.Q2 + clever.H2*(up.Q1.A - up.Q2.A) + clever.H1*(Y - up.Q1.A)*R1]\n\n# Using the EIF, compute the asymptotic error\nasymp.sd = sd(full.data$EIF)/sqrt(N)\n\n# Save estimate and CI\ntmle.2step = c(mean(full.data$delta.up.Q2)-qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2)+qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2))\n\n\n#-------------------------------------------------------------------------------\n# Visualization of results -----------------------------------------------------\n\n# PUT ALL TOGETHER\nestimators = data.table(rbind(pate,sate,unadj.est,tmle.ptest,tmle.proc,\n                              diw.est,nesreg.T,tmle.2step))\ncolnames(estimators) = c('lower','upper','point.est')\nestimators$type = c('Oracle PATE','Oracle SATE',\"Student's t-test\",'TML PreExp','TML PreProc',\n                    'DIW','Nested Reg','TML 2-Step')\n\nestimators$type = factor(estimators$type,levels = rev(estimators$type))\n\n# Estimate and CI plot \nggplot(estimators, aes(y=type, x=point.est, group=type)) +\n      geom_point(position=position_dodge(0.78)) +\n      geom_errorbar(aes(xmin=lower, xmax=upper, color=type),\n                    width=0.5, position=position_dodge(0.78)) + \n  guides(color=FALSE) + labs(x='Estimate',y='Estimator') +\n  theme_linedraw() + xlim(c(3.3,11.1)) + geom_vline(xintercept = pate[3], linetype=\"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Case 2: moderate missingness/selection\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#-------------------------------------------------------------------------------\n# Estimator A: Oracle PATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data$ITE)\npate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator B: Oracle SATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data[R2==1,ITE])\nsate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator 1: Unadjusted t-test -----------------------------------------------\n\nunadj.mod = lm_robust(Y~A,full.data[R2==1,])\n\n# Save estimate and CI\nunadj.est = unname(c(unadj.mod$conf.low[2],\n                     unadj.mod$conf.high[2],\n                     unadj.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 2: TMLE with only pre-treatment variables --------------------------\n\ntmle.pret = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = 'W', A = 'A', Y = 'Y'),   # Variables involved                         \n  data = full.data[R2==1,],                      # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.ptest = c(tmle.pret$summary$lower,\n               tmle.pret$summary$upper,\n               tmle.pret$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 3: TMLE with pre-processing tool for missingness -------------------\n\ntemp.dt = copy(full.data)\ntemp.dt$Y = ifelse(temp.dt$R2==0,NA,temp.dt$Y)\n\nprocessed = process_missing(temp.dt[,c('W','A','Y')],                    # data\n                            node_list = list(W = 'W',   # confounders\n                                             A = 'A',   # exposure\n                                             Y = 'Y'),  # outcome\n                            complete_nodes = c('W','A'),\n                            impute_nodes = \"Y\",\n                            max_p_missing = 0.9)  \n\ntmle.proc = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = c('W','delta_Y'),\n                   A = 'A', Y = 'Y'),            # Variables involved                         \n  data = processed$data,                         # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.proc = c(tmle.proc$summary$lower,\n              tmle.proc$summary$upper,\n              tmle.proc$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 4: Doubly-inverse weighted estimator -------------------------------\n\n# Model for treatment assignment\ntrain.A = make_sl3_Task(\n  data = full.data, outcome = 'A', covariates = 'W')\n\nA_fit = super_B$train(task = train.A)\nA_pre = A_fit$predict(task = train.A)\n\n# Model for missingness mechanism\ntrain.R = make_sl3_Task(\n  data = full.data, outcome = 'R2', covariates = c('W','A','M','Z'))\n\nR_fit = super_B$train(task = train.R)\nR_pre = R_fit$predict(task = train.R)\n\n# LM with doubly inverse weights\nfull.data = full.data[, DIW := (1/R_pre)*((A/A_pre)+(1-A)/(1-A_pre))]\n\ndiw.mod = lm_robust(Y~A,full.data[R2==1,], weights = DIW)\n\n# Save estimate and CI\ndiw.est = unname(c(diw.mod$conf.low[2],\n                   diw.mod$conf.high[2],\n                   diw.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 5: Nested regressions, T-learner -----------------------------------\n\n# Model for Q1\ntrain.Q1 = make_sl3_Task(\n  data = full.data[R2==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n\nQ1_fit = super_C$train(task = train.Q1)\n\n# Prediction task for Q1\npred.Q1 = make_sl3_Task(\n  data = full.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n\nfull.data$Q1 = Q1_fit$predict(task = pred.Q1)\n\n# Model for Q2.1 \ntrain.Q2.1 = make_sl3_Task(\n  data = full.data[A==1,], outcome = 'Q1', covariates = c('W'))\n\nQ2.1_fit = super_C$train(task = train.Q2.1)\n\npred.Q2.1 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2.1_fit$predict(task = pred.Q2.1)\n\n# Model for Q2.0\ntrain.Q2.0 = make_sl3_Task(\n  data = full.data[A==0,], outcome = 'Q1', covariates = c('W'))\n\nQ2.0_fit = super_C$train(task = train.Q2.0)\n\npred.Q2.0 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.0 = Q2.0_fit$predict(task = pred.Q2.0)\n\n# Predicted difference Q2.1 - Q2.0\n\nfull.data$delta = full.data$Q2.1-full.data$Q2.0\n\n# Bootstrap procedure to compute the standard deviation\n\nbsamples = c()\nfor(j in 1:30){\n  \n  # Boostraped data\n  ind = sample(1:N,N,replace=T)\n  bs.data = copy(full.data[ind,])\n  \n  # Model for Q1\n  train.bs.Q1 = make_sl3_Task(\n    data = bs.data[R2==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.Q1_fit = super_C$train(task = train.bs.Q1)\n  \n  # Prediction task for Q1\n  pred.bs.Q1 = make_sl3_Task(\n    data = bs.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.data$Q1 = bs.Q1_fit$predict(task = pred.bs.Q1)\n  \n  # Model for Q2.1.\n  train.bs.Q2.1 = make_sl3_Task(\n    data = bs.data[A==1,], outcome = 'Q1', covariates = c('W'))\n  \n  bs.Q2.1_fit = super_C$train(task = train.bs.Q2.1)\n  \n  pred.bs.Q2.1 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.1 = bs.Q2.1_fit$predict(task = pred.bs.Q2.1)\n  \n  # Model for Q2.0\n  train.bs.Q2.0 = make_sl3_Task(\n    data = bs.data[A==0,], outcome = 'Q1', covariates = c('W'))\n  \n  bsQ2.0_fit = super_C$train(task = train.bs.Q2.0)\n  \n  pred.bs.Q2.0 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.0 = Q2.0_fit$predict(task = pred.bs.Q2.0)\n  \n  # Add predicted difference Q2.1 - Q2.0 to boostrap vessel\n  bsamples = c(bsamples, mean(bs.data$Q2.1-bs.data$Q2.0))\n}\n\n# Save estimate and CI\nnesreg.T = c(mean(full.data$delta)-qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta)+qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta))\n\n#-------------------------------------------------------------------------------\n# Estimator 6: 2-step TMLE  ----------------------------------------------------\n\n# STEP 1 ------------------\n\n# Define clever variable\nfull.data = full.data[, clever.H1 := (1/R_pre)*((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.1 = lm(Y ~ -1 + offset(Q1) + clever.H1, data=full.data[R2==1,])\n\n# Auxiliary data frame for prediction\ntemp.dt = copy(full.data)\n\n# Using estimated fluctuation parameter, update Q1.1\ntemp.dt$A = 1\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.1 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.1 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.1,\n                                                 clever.H1=(1/R_pre)*(1/A_pre)))\n\n# Using estimated fluctuation parameter, update Q1.0\ntemp.dt$A = 0\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.0 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.0 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.0,\n                                                 clever.H1=(1/R_pre)*(-1/(1-A_pre))))\n\n# Learn Q2.1 from up.Q1.1, using A=1 cases\ntemp.dt = copy(full.data[A==1,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.1', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2_fit$predict(task = pred.Q2)\n\n# Learn Q2.0 from up.Q1.0, using A=0 cases\ntemp.dt = copy(full.data[A==0,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.0', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.0', covariates = c('W'))\n\nfull.data$Q2.0 = Q2_fit$predict(task = pred.Q2)\n\n# STEP 2 ------------------\n\n# Compute the observed values for up.Q1 and Q2\n# Define clever variable\nfull.data = full.data[, up.Q1.A := A*up.Q1.1 + (1-A)*up.Q1.0 ] %>% \n  .[, Q2.A := A*Q2.1 + (1-A)*Q2.0 ] %>% \n  .[, clever.H2 := ((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.2 = lm(up.Q1.A ~ -1 + offset(Q2.A) + clever.H2, data=full.data)\n\n# Using estimated fluctuation parameter, update Q2.1\nfull.data$up.Q2.1 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.1,\n                                                 clever.H2=1/A_pre))\n\n# Using estimated fluctuation parameter, update Q2.0\nfull.data$up.Q2.0 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.0,\n                                                 clever.H2=-1/(1-A_pre)))\n\n# Compute the updated difference Q2.1 - Q2.0\n# Compute the observed value for up.Q2\n# Compute the value of the efficient influence function\nfull.data = full.data[, delta.up.Q2 := up.Q2.1-up.Q2.0] %>%\n  .[, up.Q2.A := A*up.Q2.1 + (1-A)*up.Q2.0] %>%\n  .[, EIF := delta.up.Q2 + clever.H2*(up.Q1.A - up.Q2.A) + clever.H1*(Y - up.Q1.A)*R2]\n\n# Using the EIF, compute the asymptotic error\nasymp.sd = sd(full.data$EIF)/sqrt(N)\n\n# Save estimate and CI\ntmle.2step = c(mean(full.data$delta.up.Q2)-qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2)+qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2))\n\n\n#-------------------------------------------------------------------------------\n# Visualization of results -----------------------------------------------------\n\n# PUT ALL TOGETHER\nestimators = data.table(rbind(pate,sate,unadj.est,tmle.ptest,tmle.proc,\n                              diw.est,nesreg.T,tmle.2step))\ncolnames(estimators) = c('lower','upper','point.est')\nestimators$type = c('Oracle PATE','Oracle SATE',\"Student's t-test\",'TML PreExp','TML PreProc',\n                    'DIW','Nested Reg','TML 2-Step')\n\nestimators$type = factor(estimators$type,levels = rev(estimators$type))\n\n# Estimate and CI plot\nggplot(estimators, aes(y=type, x=point.est, group=type)) +\n  geom_point(position=position_dodge(0.78)) +\n  geom_errorbar(aes(xmin=lower, xmax=upper, color=type),\n                width=0.5, position=position_dodge(0.78)) + \n  guides(color=FALSE) + labs(x='Estimate',y='Estimator') + \n  theme_linedraw() + xlim(c(3.3,11.1)) + geom_vline(xintercept = pate[3], linetype=\"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Case 3A: low missingness/selection with misspecification of $Q_1$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#-------------------------------------------------------------------------------\n# Estimator A: Oracle PATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data$ITE)\npate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator B: Oracle SATE -----------------------------------------------------\n\noracle.ttest = t.test(full.data[R3==1,ITE])\nsate = unname(c(oracle.ttest$conf.int[1],\n                oracle.ttest$conf.int[2],\n                oracle.ttest$estimate))\n\n#-------------------------------------------------------------------------------\n# Estimator 1: Unadjusted t-test -----------------------------------------------\n\nunadj.mod = lm_robust(Y~A,full.data[R3==1,])\n\n# Save estimate and CI\nunadj.est = unname(c(unadj.mod$conf.low[2],\n                     unadj.mod$conf.high[2],\n                     unadj.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 2: TMLE with only pre-treatment variables --------------------------\n\ntmle.pret = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = 'W', A = 'A', Y = 'Y'),   # Variables involved                         \n  data = full.data[R3==1,],                      # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.ptest = c(tmle.pret$summary$lower,\n               tmle.pret$summary$upper,\n               tmle.pret$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 3: TMLE with pre-processing tool for missingness -------------------\n\ntemp.dt = copy(full.data)\ntemp.dt$Y = ifelse(temp.dt$R3==0,NA,temp.dt$Y)\n\nprocessed = process_missing(temp.dt[,c('W','A','Y')],                    # data\n                            node_list = list(W = 'W',   # confounders\n                                             A = 'A',   # exposure\n                                             Y = 'Y'),  # outcome\n                            complete_nodes = c('W','A'),\n                            impute_nodes = \"Y\",\n                            max_p_missing = 0.9)  \n\ntmle.proc = tmle3(\n  tmle_spec = tmle_ATE(1,0),                     # Targeting the ATE\n  node_list = list(W = c('W','delta_Y'),\n                   A = 'A', Y = 'Y'),            # Variables involved                         \n  data = processed$data,                         # Data                                  \n  learner_list = super_list)                     # Super-learners  \n\n# Save estimate and CI\ntmle.proc = c(tmle.proc$summary$lower,\n              tmle.proc$summary$upper,\n              tmle.proc$summary$tmle_est)\n\n#-------------------------------------------------------------------------------\n# Estimator 4: Doubly-inverse weighted estimator -------------------------------\n\n# Model for treatment assignment\ntrain.A = make_sl3_Task(\n  data = full.data, outcome = 'A', covariates = 'W')\n\nA_fit = super_B$train(task = train.A)\nA_pre = A_fit$predict(task = train.A)\n\n# Model for missingness mechanism\ntrain.R = make_sl3_Task(\n  data = full.data, outcome = 'R3', covariates = c('W','A','M','Z'))\n\nR_fit = super_B$train(task = train.R)\nR_pre = R_fit$predict(task = train.R)\n\n# LM with doubly inverse weights\nfull.data = full.data[, DIW := (1/R_pre)*((A/A_pre)+(1-A)/(1-A_pre))]\n\ndiw.mod = lm_robust(Y~A,full.data[R3==1,], weights = DIW)\n\n# Save estimate and CI\ndiw.est = unname(c(diw.mod$conf.low[2],\n                   diw.mod$conf.high[2],\n                   diw.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 5: Nested regressions, T-learner -----------------------------------\n\n# Model for Q1: MISSPECIFIED\ntrain.Q1 = lm(Y ~ I(W^2)+A+M+I(log(Z)), data = full.data[R3==1,])\n\nfull.data$Q1 = predict(train.Q1, newdata = full.data)\n\n# Model for Q2.1 \ntrain.Q2.1 = make_sl3_Task(\n  data = full.data[A==1,], outcome = 'Q1', covariates = c('W'))\n\nQ2.1_fit = super_C$train(task = train.Q2.1)\n\npred.Q2.1 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2.1_fit$predict(task = pred.Q2.1)\n\n# Model for Q2.0\ntrain.Q2.0 = make_sl3_Task(\n  data = full.data[A==0,], outcome = 'Q1', covariates = c('W'))\n\nQ2.0_fit = super_C$train(task = train.Q2.0)\n\npred.Q2.0 = make_sl3_Task(\n  data = full.data, outcome = 'Q1', covariates = c('W'))\n\nfull.data$Q2.0 = Q2.0_fit$predict(task = pred.Q2.0)\n\n# Predicted difference Q2.1 - Q2.0\n\nfull.data$delta = full.data$Q2.1-full.data$Q2.0\n\n# Bootstrap procedure to compute the standard deviation\n\nbsamples = c()\nfor(j in 1:30){\n  \n  # Boostraped data\n  ind = sample(1:N,N,replace=T)\n  bs.data = copy(full.data[ind,])\n  \n  # Model for Q1: MISSPECIFIED\n  train.bs.Q1 = lm(Y ~ W+A+M+Z, data = bs.data[R3==1,])\n\n  bs.data$Q1 = predict(train.bs.Q1, newdata = bs.data)\n  \n  # Model for Q2.1.\n  train.bs.Q2.1 = make_sl3_Task(\n    data = bs.data[A==1,], outcome = 'Q1', covariates = c('W'))\n  \n  bs.Q2.1_fit = super_C$train(task = train.bs.Q2.1)\n  \n  pred.bs.Q2.1 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.1 = bs.Q2.1_fit$predict(task = pred.bs.Q2.1)\n  \n  # Model for Q2.0\n  train.bs.Q2.0 = make_sl3_Task(\n    data = bs.data[A==0,], outcome = 'Q1', covariates = c('W'))\n  \n  bsQ2.0_fit = super_C$train(task = train.bs.Q2.0)\n  \n  pred.bs.Q2.0 = make_sl3_Task(\n    data = bs.data, outcome = 'Q1', covariates = c('W'))\n  \n  bs.data$Q2.0 = Q2.0_fit$predict(task = pred.bs.Q2.0)\n  \n  # Add predicted difference Q2.1 - Q2.0 to boostrap vessel\n  bsamples = c(bsamples, mean(bs.data$Q2.1-bs.data$Q2.0))\n}\n\n# Save estimate and CI\nnesreg.T = c(mean(full.data$delta)-qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta)+qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta))\n\n#-------------------------------------------------------------------------------\n# Estimator 6: 2-step TMLE  ----------------------------------------------------\n\n# STEP 1 ------------------\n\n# Define clever variable\nfull.data = full.data[, clever.H1 := (1/R_pre)*((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.1 = lm(Y ~ -1 + offset(Q1) + clever.H1, data=full.data[R3==1,])\n\n# Auxiliary data frame for prediction\ntemp.dt = copy(full.data)\n\n# Using estimated fluctuation parameter, update Q1.1\ntemp.dt$A = 1\nfull.data$Q1.1 = predict(train.Q1, newdata = temp.dt)\n\nfull.data$up.Q1.1 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.1,\n                                                 clever.H1=(1/R_pre)*(1/A_pre)))\n\n# Using estimated fluctuation parameter, update Q1.0\ntemp.dt$A = 0\nfull.data$Q1.0 = predict(train.Q1, newdata = temp.dt)\n\nfull.data$up.Q1.0 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.0,\n                                                 clever.H1=(1/R_pre)*(-1/(1-A_pre))))\n\n# Learn Q2.1 from up.Q1.1, using A=1 cases\ntemp.dt = copy(full.data[A==1,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.1', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.1', covariates = c('W'))\n\nfull.data$Q2.1 = Q2_fit$predict(task = pred.Q2)\n\n# Learn Q2.0 from up.Q1.0, using A=0 cases\ntemp.dt = copy(full.data[A==0,])\n\ntrain.Q2 = make_sl3_Task(\n  data = temp.dt, outcome = 'up.Q1.0', covariates = c('W'))\n\nQ2_fit = super_C$train(task = train.Q2)\n\npred.Q2 = make_sl3_Task(\n  data = full.data, outcome = 'up.Q1.0', covariates = c('W'))\n\nfull.data$Q2.0 = Q2_fit$predict(task = pred.Q2)\n\n# STEP 2 ------------------\n\n# Compute the observed values for up.Q1 and Q2\n# Define clever variable\nfull.data = full.data[, up.Q1.A := A*up.Q1.1 + (1-A)*up.Q1.0 ] %>% \n  .[, Q2.A := A*Q2.1 + (1-A)*Q2.0 ] %>% \n  .[, clever.H2 := ((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.2 = lm(up.Q1.A ~ -1 + offset(Q2.A) + clever.H2, data=full.data)\n\n# Using estimated fluctuation parameter, update Q2.1\nfull.data$up.Q2.1 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.1,\n                                                 clever.H2=1/A_pre))\n\n# Using estimated fluctuation parameter, update Q2.0\nfull.data$up.Q2.0 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.0,\n                                                 clever.H2=-1/(1-A_pre)))\n\n# Compute the updated difference Q2.1 - Q2.0\n# Compute the observed value for up.Q2\n# Compute the value of the efficient influence function\nfull.data = full.data[, delta.up.Q2 := up.Q2.1-up.Q2.0] %>%\n  .[, up.Q2.A := A*up.Q2.1 + (1-A)*up.Q2.0] %>%\n  .[, EIF := delta.up.Q2 + clever.H2*(up.Q1.A - up.Q2.A) + clever.H1*(Y - up.Q1.A)*R3]\n\n# Using the EIF, compute the asymptotic error\nasymp.sd = sd(full.data$EIF)/sqrt(N)\n\n# Save estimate and CI\ntmle.2step = c(mean(full.data$delta.up.Q2)-qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2)+qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2))\n\n\n#-------------------------------------------------------------------------------\n# Visualization of results -----------------------------------------------------\n\n# PUT ALL TOGETHER\nestimators = data.table(rbind(pate,sate,unadj.est,tmle.ptest,tmle.proc,\n                              diw.est,nesreg.T,tmle.2step))\ncolnames(estimators) = c('lower','upper','point.est')\nestimators$type = c('Oracle PATE','Oracle SATE',\"Student's t-test\",'TML PreExp','TML PreProc',\n                    'DIW','Nested Reg','TML 2-Step')\n\nestimators$type = factor(estimators$type,levels = rev(estimators$type))\n\n# Estimate and CI plot\nggplot(estimators, aes(y=type, x=point.est, group=type)) +\n  geom_point(position=position_dodge(0.78)) +\n  geom_errorbar(aes(xmin=lower, xmax=upper, color=type),\n                width=0.5, position=position_dodge(0.78)) + \n  guides(color=FALSE) + labs(x='Estimate',y='Estimator') + \n  theme_linedraw() + xlim(c(3.3,11.1)) + geom_vline(xintercept = pate[3], linetype=\"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n## Case 3B: low missingness/selection with misspecification of $Q_2$ and $r$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#-------------------------------------------------------------------------------\n# Misspecified prob of selection -----------------------------------------------\n\n# Model for missingness mechanism\nR_mis = predict(glm(R3 ~ W+A+poly(M,2)+I(log(Z)),\n                    full.data, family=binomial('logit')), type='response')\n\n#-------------------------------------------------------------------------------\n# Estimator 4: Doubly-inverse weighted estimator -------------------------------\n\n# LM with doubly inverse weights\nfull.data = full.data[, DIW := (1/R_mis)*((A/A_pre)+(1-A)/(1-A_pre))]\n\ndiw.mod = lm_robust(Y~A,full.data[R3==1,], weights = DIW)\n\n# Save estimate and CI\ndiw.est = unname(c(diw.mod$conf.low[2],\n                   diw.mod$conf.high[2],\n                   diw.mod$coefficients[2]))\n\n#-------------------------------------------------------------------------------\n# Estimator 5: Nested regressions, T-learner -----------------------------------\n\n# Model for Q1\ntrain.Q1 = make_sl3_Task(\n  data = full.data[R3==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n\nQ1_fit = super_C$train(task = train.Q1)\n\n# Prediction task for Q1\npred.Q1 = make_sl3_Task(\n  data = full.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n\nfull.data$Q1 = Q1_fit$predict(task = pred.Q1)\n\n# Model for Q2 misspecified\ntrain.Q2 = lm(Q1 ~ A+I(log(4+W))+A:I(W^2), full.data)\n\nfull.data$Q2.1 = predict(train.Q2, newdata=data.frame(W=full.data$W,A=1))\n\nfull.data$Q2.0 = predict(train.Q2, newdata=data.frame(W=full.data$W,A=0))\n\n# Predicted difference Q2.1 - Q2.0\n\nfull.data$delta = full.data$Q2.1-full.data$Q2.0\n\n# Bootstrap procedure to compute the standard deviation\n\nbsamples = c()\nfor(j in 1:30){\n  \n  # Boostraped data\n  ind = sample(1:N,N,replace=T)\n  bs.data = copy(full.data[ind,])\n  \n  # Model for Q1\n  train.bs.Q1 = make_sl3_Task(\n    data = bs.data[R3==1,], outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.Q1_fit = super_C$train(task = train.bs.Q1)\n  \n  # Prediction task for Q1\n  pred.bs.Q1 = make_sl3_Task(\n    data = bs.data, outcome = 'Y', covariates = c('W','A','M','Z'))\n  \n  bs.data$Q1 = bs.Q1_fit$predict(task = pred.bs.Q1)\n  \n  # Model for Q2\n  train.bs.Q2 = lm(Q1 ~ A+I(log(4+W))+A:I(W^2), bs.data)\n\n  full.data$Q2.1 = predict(train.bs.Q2, newdata=data.frame(W=bs.data$W,A=1))\n\n  full.data$Q2.0 = predict(train.bs.Q2, newdata=data.frame(W=bs.data$W,A=0))\n  \n  # Add predicted difference Q2.1 - Q2.0 to boostrap vessel\n  bsamples = c(bsamples, mean(bs.data$Q2.1-bs.data$Q2.0))\n}\n\n# Save estimate and CI\nnesreg.T = c(mean(full.data$delta)-qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta)+qnorm(0.975)*sd(bsamples),\n             mean(full.data$delta))\n\n#-------------------------------------------------------------------------------\n# Estimator 6: 2-step TMLE  ----------------------------------------------------\n\n# STEP 1 ------------------\n\n# Define clever variable\nfull.data = full.data[, clever.H1 := (1/R_mis)*((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.1 = lm(Y ~ -1 + offset(Q1) + clever.H1, data=full.data[R3==1,])\n\n# Auxiliary data frame for prediction\ntemp.dt = copy(full.data)\n\n# Using estimated fluctuation parameter, update Q1.1\ntemp.dt$A = 1\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.1 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.1 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.1,\n                                                 clever.H1=(1/R_mis)*(1/A_pre)))\n\n# Using estimated fluctuation parameter, update Q1.0\ntemp.dt$A = 0\npred.Q1 = make_sl3_Task(\n  data = temp.dt, outcome = 'Y', covariates = c('W','A','M','Z'))\nfull.data$Q1.0 = Q1_fit$predict(task = pred.Q1)\n\nfull.data$up.Q1.0 = predict(fluct.model.1, \n                            newdata = data.frame(Q1=full.data$Q1.0,\n                                                 clever.H1=(1/R_mis)*(-1/(1-A_pre))))\n\nfull.data = full.data[, up.Q1.A := A*up.Q1.1 + (1-A)*up.Q1.0 ]\n\n# Learn Q2.1,0 from up.Q1.1,0\ntrain.Q2 = lm(up.Q1.A ~ A+I(log(4+W))+A:I(W^2), full.data)\n\nfull.data$Q2.1 = predict(train.Q2, newdata=data.frame(W=full.data$W,A=1))\n\nfull.data$Q2.0 = predict(train.Q2, newdata=data.frame(W=full.data$W,A=0))\n\n# STEP 2 ------------------\n\n# Compute the observed values for up.Q1 and Q2\n# Define clever variable\nfull.data = full.data[, Q2.A := A*Q2.1 + (1-A)*Q2.0 ] %>% \n  .[, clever.H2 := ((A/A_pre)-(1-A)/(1-A_pre))] \n\n# Define fluctuation model\nfluct.model.2 = lm(up.Q1.A ~ -1 + offset(Q2.A) + clever.H2, data=full.data)\n\n# Using estimated fluctuation parameter, update Q2.1\nfull.data$up.Q2.1 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.1,\n                                                 clever.H2=1/A_pre))\n\n# Using estimated fluctuation parameter, update Q2.0\nfull.data$up.Q2.0 = predict(fluct.model.2, \n                            newdata = data.frame(Q2.A=full.data$Q2.0,\n                                                 clever.H2=-1/(1-A_pre)))\n\n# Compute the updated difference Q2.1 - Q2.0\n# Compute the observed value for up.Q2\n# Compute the value of the efficient influence function\nfull.data = full.data[, delta.up.Q2 := up.Q2.1-up.Q2.0] %>%\n  .[, up.Q2.A := A*up.Q2.1 + (1-A)*up.Q2.0] %>%\n  .[, EIF := delta.up.Q2 + clever.H2*(up.Q1.A - up.Q2.A) + clever.H1*(Y - up.Q1.A)*R3]\n\n# Using the EIF, compute the asymptotic error\nasymp.sd = sd(full.data$EIF)/sqrt(N)\n\n# Save estimate and CI\ntmle.2step = c(mean(full.data$delta.up.Q2)-qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2)+qnorm(0.975)*asymp.sd,\n               mean(full.data$delta.up.Q2))\n\n\n#-------------------------------------------------------------------------------\n# Visualization of results -----------------------------------------------------\n\n# PUT ALL TOGETHER\nestimators = data.table(rbind(pate,sate,unadj.est,tmle.ptest,tmle.proc,\n                              diw.est,nesreg.T,tmle.2step))\ncolnames(estimators) = c('lower','upper','point.est')\nestimators$type = c('Oracle PATE','Oracle SATE',\"Student's t-test\",'TML PreExp','TML PreProc',\n                    'DIW','Nested Reg','TML 2-Step')\n\nestimators$type = factor(estimators$type,levels = rev(estimators$type))\n\n# Estimate and CI plot\nggplot(estimators, aes(y=type, x=point.est, group=type)) +\n  geom_point(position=position_dodge(0.78)) +\n  geom_errorbar(aes(xmin=lower, xmax=upper, color=type),\n                width=0.5, position=position_dodge(0.78)) + \n  guides(color=FALSE) + labs(x='Estimate',y='Estimator') + \n  theme_linedraw() + xlim(c(3.3,11.1)) + geom_vline(xintercept = pate[3], linetype=\"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=768}\n:::\n\n```{.r .cell-code}\n#j=8\n#glue('lower whisker={estimators[j,1]}, median={estimators[j,3]}, upper whisker={estimators[j,2]}')\n```\n:::\n\n\n## Conclusions\n\n- **Nested regression estimator** provides the best alternative under the assumption of correct models specifications. It can consistently recover the ATE from confounding and selection bias (under conditional ignorability and recoverability conditions), and it presents narrower confidence interval relative to the DIW and TML 2-steps estimators, which are also consistent in this scenario. Yet, it requires a bootstrap procedure to compute standard errors, which might be costly in complex models/datasets, and **it fails to be consistent when one the outcome models are not correctly specified**.\n\n- **TML 2-step estimator** is less efficient than nested regression estimator, producing wider confidence intervals, but it is more efficient than the DIW estimator. Moreover, it remains consistent even when the outcome models are not correctly specified, provided the treatment assignment and missingness mechanisms are both correct.\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}